[
  {
    "objectID": "tidytuesday-exercise/tidytuesday-exercise.html",
    "href": "tidytuesday-exercise/tidytuesday-exercise.html",
    "title": "Tidy Tuesday Exercise",
    "section": "",
    "text": "Placeholder file for the future Tidy Tuesday exercise."
  },
  {
    "objectID": "starter-analysis-exercise/results/readme.html",
    "href": "starter-analysis-exercise/results/readme.html",
    "title": "Erick Mollinedo MADA portfolio",
    "section": "",
    "text": "This folder contains results produced by the code, such as figures and tables.\nDepending on the size and type of your project, you can either place it all in a single folder or create sub-folders. For instance you could create a folder for figures, another for tables. Or you could create a sub-folder for dataset 1, another for dataset 2. Or you could have a subfolder for exploratory analysis, another for final analysis. The options are endless, choose whatever makes sense for your project. For this template, there is just a a single folder, but having sub-folders is often a good idea."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "",
    "text": "The structure below is one possible setup for a data analysis project (including the course project). For a manuscript, adjust as needed. You don’t need to have exactly these sections, but the content covering those sections should be addressed.\nThis uses MS Word as output format. See here for more information. You can switch to other formats, like html or pdf. See the Quarto documentation for other formats.\n\n\nWarning: package 'here' was built under R version 4.2.3"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#general-background-information",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.1 General Background Information",
    "text": "3.1 General Background Information\nProvide enough background on your topic that others can understand the why and how of your analysis"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#description-of-data-and-data-source",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.2 Description of data and data source",
    "text": "3.2 Description of data and data source\nDescribe what the data is, what it contains, where it is from, etc. Eventually this might be part of a methods section."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#questionshypotheses-to-be-addressed",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "3.3 Questions/Hypotheses to be addressed",
    "text": "3.3 Questions/Hypotheses to be addressed\nState the research questions you plan to answer with this analysis.\nTo cite other work (important everywhere, but likely happens first in introduction), make sure your references are in the bibtex file specified in the YAML header above (here dataanalysis_template_references.bib) and have the right bibtex key. Then you can include like this:\nExamples of reproducible research projects can for instance be found in (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, Shen, & Handel, 2020)"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-aquisition",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.1 Data aquisition",
    "text": "4.1 Data aquisition\nAs applicable, explain where and how you got the data. If you directly import the data from an online source, you can combine this section with the next."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#data-import-and-cleaning",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.2 Data import and cleaning",
    "text": "4.2 Data import and cleaning\nWrite code that reads in the file and cleans it so it’s ready for analysis. Since this will be fairly long code for most datasets, it might be a good idea to have it in one or several R scripts. If that is the case, explain here briefly what kind of cleaning/processing you do, and provide more details and well documented code somewhere (e.g. as supplement in a paper). All materials, including files that contain code, should be commented well so everyone can follow along."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "4.3 Statistical analysis",
    "text": "4.3 Statistical analysis\nExplain anything related to your statistical analyses."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#exploratorydescriptive-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.1 Exploratory/Descriptive analysis",
    "text": "5.1 Exploratory/Descriptive analysis\nUse a combination of text/tables/figures to explore and describe your data. Show the most important descriptive results here. Additional ones should go in the supplement. Even more can be in the R and Quarto files that are part of your project.\nTable 1 shows a summary of the data.\nNote the loading of the data providing a relative path using the ../../ notation. (Two dots means a folder up). You never want to specify an absolute path like C:\\ahandel\\myproject\\results\\ because if you share this with someone, it won’t work for them since they don’t have that path. You can also use the here R package to create paths. See examples of that below. I recommend the here package, but I’m showing the other approach here just in case you encounter it.\n\n\n\n\nTable 1: Data summary table.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_type\nskim_variable\nn_missing\ncomplete_rate\ncharacter.min\ncharacter.max\ncharacter.empty\ncharacter.n_unique\ncharacter.whitespace\nfactor.ordered\nfactor.n_unique\nfactor.top_counts\nnumeric.mean\nnumeric.sd\nnumeric.p0\nnumeric.p25\nnumeric.p50\nnumeric.p75\nnumeric.p100\nnumeric.hist\n\n\n\n\ncharacter\nOccupation\n0\n1\n5\n11\n0\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nfactor\nGender\n0\n1\nNA\nNA\nNA\nNA\nNA\nFALSE\n3\nM: 4, F: 3, O: 2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nnumeric\nHeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n165.66667\n15.976545\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nnumeric\nWeight\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n70.11111\n21.245261\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nnumeric\nAge\n0\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n33.88889\n5.925463\n25\n30\n34\n37\n45\n▅▅▇▂▂"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#basic-statistical-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.2 Basic statistical analysis",
    "text": "5.2 Basic statistical analysis\nTo get some further insight into your data, if reasonable you could compute simple statistics (e.g. simple models with 1 predictor) to look for associations between your outcome(s) and each individual predictor variable. Though note that unless you pre-specified the outcome and main exposure, any “p&lt;0.05 means statistical significance” interpretation is not valid.\nFigure 1 shows a scatterplot figure produced by one of the R scripts.\n\n\n\n\n\nFigure 1: Height and weight stratified by gender."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#full-analysis",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "5.3 Full analysis",
    "text": "5.3 Full analysis\nUse one or several suitable statistical/machine learning methods to analyze your data and to produce meaningful figures, tables, etc. This might again be code that is best placed in one or several separate R scripts that need to be well documented. You want the code to produce figures and data ready for display as tables, and save those. Then you load them here.\nExample Table 2 shows a summary of a linear model fit.\n\n\n\n\nTable 2: Linear model fit table.\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n149.2726967\n23.3823360\n6.3839942\n0.0013962\n\n\nWeight\n0.2623972\n0.3512436\n0.7470519\n0.4886517\n\n\nGenderM\n-2.1244913\n15.5488953\n-0.1366329\n0.8966520\n\n\nGenderO\n-4.7644739\n19.0114155\n-0.2506112\n0.8120871"
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#summary-and-interpretation",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.1 Summary and Interpretation",
    "text": "6.1 Summary and Interpretation\nSummarize what you did, what you found and what it means."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#strengths-and-limitations",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.2 Strengths and Limitations",
    "text": "6.2 Strengths and Limitations\nDiscuss what you perceive as strengths and limitations of your analysis."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#conclusions",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.3 Conclusions",
    "text": "6.3 Conclusions\nWhat are the main take-home messages?\nInclude citations in your Rmd file using bibtex, the list of references will automatically be placed at the end\nThis paper (Leek & Peng, 2015) discusses types of analyses.\nThese papers (McKay, Ebell, Billings, et al., 2020; McKay, Ebell, Dale, et al., 2020) are good examples of papers published using a fully reproducible setup similar to the one shown in this template.\nNote that this cited reference will show up at the end of the document, the reference formatting is determined by the CSL file specified in the YAML header. Many more style files for almost any journal are available. You also specify the location of your bibtex reference file in the YAML. You can call your reference file anything you like, I just used the generic word references.bib but giving it a more descriptive name is probably better."
  },
  {
    "objectID": "starter-analysis-exercise/products/report/starter-analysis-report.html#class-2-exercise",
    "href": "starter-analysis-exercise/products/report/starter-analysis-report.html#class-2-exercise",
    "title": "Manuscript/Report Template for a Data Analysis Project",
    "section": "6.4 Class 2: Exercise",
    "text": "6.4 Class 2: Exercise\nFor this exercise I plotted height of the individuals by their occupation.\n\n\n\n\n\nFigure 2: Boxplot of individuals height by occupation\n\n\n\n\nBased on Figure 2, we can see that individuals that work on agriculture have the greatest height and there is not much variation in range. However, individuals that work on industry are the ones that have a lot of variation and most of them are in the 25th percentile. We cannot infer much about individuals that work for government, or occupation or in other professions, since there is just one data point for these. However, it seems that the greatest height are at individuals that work doing household chores.\n\n\n\n\n\nFigure 3: Basic plot of Age by Weight\n\n\n\n\nFigure 3 shows that there appears to be an association between age and weight. Based on the linear model, there could be an increase of weight by age, since biologically we cannot say that age increases by weight.\n\n\n\n\nTable 3: Linear model fit table including age and occupation\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n97.420904\n45.06027\n2.1620134\n0.1193492\n\n\nAge\n2.063559\n1.24932\n1.6517461\n0.1971540\n\n\nOccupationgovernment\n-9.836158\n18.39892\n-0.5346052\n0.6300061\n\n\nOccupationhousehold\n33.990113\n22.22791\n1.5291637\n0.2236991\n\n\nOccupationindustry\n-11.333333\n12.79494\n-0.8857665\n0.4409819\n\n\nOccupationother\n-5.327684\n19.28257\n-0.2762954\n0.8002623\n\n\n\n\n\n\nIn Table 3 we can observe a model fit to predict Height as a function of age and occupation. The model explains the interactions, however none of the occupations and/or age are statistically significant based on the p-value."
  },
  {
    "objectID": "starter-analysis-exercise/data/readme.html",
    "href": "starter-analysis-exercise/data/readme.html",
    "title": "Erick Mollinedo MADA portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all data at various stages.\nThis data is being loaded/manipulated/changed/saved with code from the code folders.\nYou should place the raw data in the raw_data folder and not edit it. Ever!\nIdeally, load the raw data into R and do all changes there with code, so everything is automatically reproducible and documented.\nSometimes, you need to edit the files in the format you got. For instance, Excel files are sometimes so poorly formatted that it’s close to impossible to read them into R, or the persons you got the data from used color to code some information, which of course won’t import into R. In those cases, you might have to make modifications in a software other than R. If you need to make edits in whatever format you got the data (e.g. Excel), make a copy and place those copies in a separate folder, AND ONLY EDIT THOSE COPIES. Also, write down somewhere the edits you made.\nAdd as many sub-folders as suitable. If you only have a single processing step, one sub-folder for processed data is enough. If you have multiple stages of cleaning and processing, additional sub-folders might be useful. Adjust based on the complexity of your project.\nI suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data:\nhttp://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata"
  },
  {
    "objectID": "starter-analysis-exercise/code/readme.html",
    "href": "starter-analysis-exercise/code/readme.html",
    "title": "Erick Mollinedo MADA portfolio",
    "section": "",
    "text": "Place your various R or Quarto files in the appropriate folders.\nYou can either have fewer large scripts, or multiple scripts that do only specific actions. Those can be R or Quarto files. In either case, document the scripts and what goes on in them so well that someone else (including future you) can easily figure out what is happening.\nThe scripts should load the appropriate data (e.g. raw or processed), perform actions, and save results (e.g. processed data, figures, computed values) in the appropriate folders. Document somewhere what inputs each script takes and where output is placed.\nIf scripts need to be run in a specific order, document this. Either as comments in the script, or in a separate text file such as this readme file. Ideally of course in both locations.\nDepending on your specific project, you might want to have further folders/sub-folders."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile2.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\n\nWarning: package 'readxl' was built under R version 4.2.3\n\nlibrary(dplyr) #for data processing/cleaning\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.2.3\n\nlibrary(here) #to set paths\n\nWarning: package 'here' was built under R version 4.2.3\n\n\nhere() starts at C:/Users/malik/Documents/1. UGA Classes/15. Malika Spring 2024/MADASpring_24/erickmollinedo-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata2.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 5 × 3\n  `Variable Name` `Variable Definition`                         `Allowed Values`\n  &lt;chr&gt;           &lt;chr&gt;                                         &lt;chr&gt;           \n1 Height          height in centimeters                         numeric value &gt;…\n2 Weight          weight in kilograms                           numeric value &gt;…\n3 Gender          identified gender (male/female/other)         M/F/O/NA        \n4 Age             age in years                                  numeric value &gt;…\n5 Occupation      Occupation, as closest as the pre-defined ca… agriculture/edu…\n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 5\n$ Height     &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"15…\n$ Weight     &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender     &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\"…\n$ Age        &lt;dbl&gt; 35, 34, 28, 45, 27, 25, 37, 33, 29, 43, 45, 28, 38, 30\n$ Occupation &lt;chr&gt; \"agriculture\", \"agriculture\", \"education\", \"industry\", \"edu…\n\nsummary(rawdata)\n\n    Height              Weight          Gender               Age       \n Length:14          Min.   :  45.0   Length:14          Min.   :25.00  \n Class :character   1st Qu.:  55.0   Class :character   1st Qu.:28.25  \n Mode  :character   Median :  70.0   Mode  :character   Median :33.50  \n                    Mean   : 602.7                      Mean   :34.07  \n                    3rd Qu.:  90.0                      3rd Qu.:37.75  \n                    Max.   :7000.0                      Max.   :45.00  \n                    NA's   :1                                          \n  Occupation       \n Length:14         \n Class :character  \n Mode  :character  \n                   \n                   \n                   \n                   \n\nhead(rawdata)\n\n# A tibble: 6 × 5\n  Height Weight Gender   Age Occupation \n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;      \n1 180        80 M         35 agriculture\n2 175        70 O         34 agriculture\n3 sixty      60 F         28 education  \n4 178        76 F         45 industry   \n5 192        90 NA        27 education  \n6 6          55 F         25 household  \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n3\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nOccupation\n0\n1\n5\n11\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55.00\n70.0\n90.00\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n34.07\n6.75\n25\n28.25\n33.5\n37.75\n45\n▇▃▅▂▅\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nOccupation\n0\n1\n5\n11\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n34.54\n6.79\n25\n29.00\n34\n38\n45\n▇▃▆▂▆\n\n\n\n\nhist(d1$Height)\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nOccupation\n0\n1\n5\n11\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\nAge\n0\n1.00\n34.54\n6.79\n25\n29.00\n34\n38\n45\n▇▃▆▂▆\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\nOccupation\n0\n1\n5\n11\n0\n6\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n32.82\n5.83\n25\n28.5\n33\n36\n45\n▇▃▆▂▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nOccupation\n0\n1\n5\n11\n0\n6\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\nAge\n0\n1\n32.82\n5.83\n25\n28.5\n33\n36\n45\n▇▃▆▂▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nOccupation\n0\n1\n5\n11\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nAge\n0\n1\n33.89\n5.93\n25\n30\n34\n37\n45\n▅▅▇▂▂\n\n\n\n\n\n\nd5&lt;-d4%&gt;% dplyr::mutate(Age = as.numeric(Age))\nskimr::skim(d5)\n\n\nData summary\n\n\nName\nd5\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nfactor\n1\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nOccupation\n0\n1\n5\n11\n0\n5\n0\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nAge\n0\n1\n33.89\n5.93\n25\n30\n34\n37\n45\n▅▅▇▂▂\n\n\n\n\nhist(d5$Age)\n\n\n\n\n\nd5$Occupation &lt;- as.factor(d5$Occupation)\nskimr::skim(d5)\n\n\nData summary\n\n\nName\nd5\n\n\nNumber of rows\n9\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n2\n\n\nnumeric\n3\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\nOccupation\n0\n1\nFALSE\n5\nagr: 3, ind: 3, gov: 1, hou: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\nAge\n0\n1\n33.89\n5.93\n25\n30\n34\n37\n45\n▅▅▇▂▂\n\n\n\n\nsummary(d5)\n\n     Height          Weight       Gender      Age              Occupation\n Min.   :133.0   Min.   : 45.00   F:3    Min.   :25.00   agriculture:3   \n 1st Qu.:156.0   1st Qu.: 55.00   M:4    1st Qu.:30.00   government :1   \n Median :166.0   Median : 70.00   O:2    Median :34.00   household  :1   \n Mean   :165.7   Mean   : 70.11          Mean   :33.89   industry   :3   \n 3rd Qu.:178.0   3rd Qu.: 80.00          3rd Qu.:37.00   other      :1   \n Max.   :183.0   Max.   :110.00          Max.   :45.00                   \n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d5\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/readme.html",
    "href": "starter-analysis-exercise/code/eda-code/readme.html",
    "title": "Erick Mollinedo MADA portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory data analysis (EDA) on the processed/cleaned data. The code produces a few tables and figures, which are saved in the appropriate results sub-folder."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda.html",
    "href": "starter-analysis-exercise/code/eda-code/eda.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nWarning: package 'here' was built under R version 4.2.3\n\n\nhere() starts at C:/Users/malik/Documents/1. UGA Classes/15. Malika Spring 2024/MADASpring_24/erickmollinedo-MADA-portfolio\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.2.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  factor                   2     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique\n1 Gender                0             1 FALSE          3\n2 Occupation            0             1 FALSE          5\n  top_counts                    \n1 M: 4, F: 3, O: 2              \n2 agr: 3, ind: 3, gov: 1, hou: 1\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean    sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0  133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2   45  55  70  80  110 ▇▂▃▂▂\n3 Age                   0             1  33.9  5.93  25  30  34  37   45 ▅▅▇▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html",
    "href": "presentation-exercise/presentation-exercise.html",
    "title": "Presentation Exercise",
    "section": "",
    "text": "For this exercise I chose the third graph from the “How Americans view Biden’s response to the coronavirus crisis” article from FiveThirtyEight (https://projects.fivethirtyeight.com/coronavirus-polls/). The graph is a time trend on how americans were worried about they or someone beloved to getting infected with COVID-19, from February 2020 to April 2021. To recreate the graph I used the RTutor AI online tool, where after many attempts I was given a product that worked but still, I had to make some small tweaks to make the graph more alike the original one.\nThis is the original graph from the article:\n[]\nPackages I used for this exercise:\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'purrr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3\n\n\nWarning: package 'forcats' was built under R version 4.2.3\n\n\nWarning: package 'lubridate' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(scales)\n\nWarning: package 'scales' was built under R version 4.2.3\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(here)\n\nWarning: package 'here' was built under R version 4.2.3\n\n\nhere() starts at C:/Users/malik/Documents/1. UGA Classes/15. Malika Spring 2024/MADASpring_24/erickmollinedo-MADA-portfolio\n\nlibrary(readr)\nlibrary(lubridate)\n\nLoading the dataset and assign it to the covid_poll object:\n\n#Using the `read_csv()` and `here()` functions to load the dataset\ncovid_poll &lt;- read_csv(here(\"presentation-exercise\", \"data\", \"covid_concern_toplines.csv\"))\n\nRows: 1496 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): subject, modeldate, party, timestamp\ndbl (4): very_estimate, somewhat_estimate, not_very_estimate, not_at_all_est...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFirst I transformed the modeldate variable to date type using the lubridate package. I assigned it to the covid_infect dataframe. I also filtered only the responses needed for the graph, from the subject variable.\n\n# Transform 'modeldate' to date type and filter the data\ncovid_infect &lt;- covid_poll %&gt;%\n  mutate(modeldate = mdy(modeldate)) %&gt;% #Mutate the `modeldate` variable to month/day/year\n  filter(modeldate &lt; as.Date(\"2021-04-22\") & subject == 'concern-infected') #Transform the `modeldate` variable as date type and filter the 'concern-infected' value from the `subject` variable.\n\nNow I decided to produce weekly averages, instead of using the daily datapoints, I assigned this to the covid_weekly dataframe\n\n# Calculate weekly averages of the estimates\ncovid_weekly &lt;- covid_infect %&gt;%\n  group_by(week = floor_date(modeldate, \"week\")) %&gt;% #Group by weeks\n  summarize( #Produce the summaries for each response variable (4 code lines below)\n    very_estimate = mean(very_estimate, na.rm = TRUE),\n    somewhat_estimate = mean(somewhat_estimate, na.rm = TRUE),\n    not_very_estimate = mean(not_very_estimate, na.rm = TRUE),\n    not_at_all_estimate = mean(not_at_all_estimate, na.rm = TRUE),\n    .groups = 'drop')\n\nThen I created the covid_longer dataframe using the pivot_longer() function, so the data is more easy to be used for the final graph.\n\n# Pivot data to long format for plotting\ncovid_longer &lt;- covid_weekly %&gt;%\n  pivot_longer(cols = c(very_estimate, somewhat_estimate, not_very_estimate, not_at_all_estimate), #Use `pivot_longer()` to mutate the dataframe\n               names_to = \"estimate_type\",\n               values_to = \"estimate\") %&gt;%\n  mutate(estimate_label = recode(estimate_type, #Recode the values of the response variables to characters more legible (4 code lines below)\n                                 \"very_estimate\" = \"Very\",\n                                 \"somewhat_estimate\" = \"Somewhat\",\n                                 \"not_very_estimate\" = \"Not Very\",\n                                 \"not_at_all_estimate\" = \"Not at all\"))\n\nAnd finally creating the plot using ggplot(). Each code chunk is detailed below.\n\n#Creating the time trend graph\nggplot(covid_longer, aes(x = week, y = estimate, group = estimate_label, color = estimate_label)) + #Selecting the x and y variables and grouping by `estimate_label`\n  geom_line() + #Select a line graph\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%m/%d\", #Selecting the x-scale breaks in 1-month intervals and selecting as month/day format\n               limits = as.Date(c(\"2020-02-01\", \"2021-04-01\"))) + #Selecting the start and end date limits\n  scale_y_continuous(limits = c(0, 75), breaks = c(0, 25, 50)) + #Selecting the y-axis limits, and set the breaks\n  scale_color_manual(values = c(\"Very\" = \"red\", \"Somewhat\" = \"orange\", \n                                \"Not Very\" = \"mediumpurple1\", \"Not at all\" = \"purple\")) + #Setting the colors\n  labs(title = \"How worried are Americans about infection?\", #Writing the title\n    subtitle = paste(\"How concerned Americans say they are that they, someone in their family or someone else they know will\", \"\\n\", \"become infected with the coronavirus\"))+ #Writing the subtitle, and using \"\\n\" to sepparate it in two lines\n  theme_minimal() + #setting the theme\n  theme(legend.position = \"bottom\", #The position of the legend for level of concern\n    plot.title = element_text(hjust = 0.5, size = 10), #Position and size of the title\n    plot.subtitle = element_text(hjust = 0.5, size = 8), #Position and size of the subtitle\n    axis.title.x = element_blank(), #Removing the x-axis label\n    axis.title.y = element_blank())+ #Removing the y-axis label\n  geom_vline(xintercept = as.Date(\"2020-02-29\"), linetype = \"dashed\") + #Setting a dashed line on a specific date with text below, the following 4 lines of code are for another 4 specific lines\n  geom_vline(xintercept = as.Date(\"2020-05-28\"), linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2020-10-02\"), linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2020-11-07\"), linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2021-01-20\"), linetype = \"dashed\") +\n  geom_text(aes(x = as.Date(\"2020-02-29\"), y = 50, label = paste(\"First U.S.\", \"\\n\", \"death reported\")), angle = 0, vjust = 0, fontface = \"italic\", color = \"black\") + #Setting the text for the first dashed line, breaking it into two text parts so they fit inside the plot, also setting the angle at 0 and in italic font. (The following lines of code are for the other 4 texts)\n  geom_text(aes(x = as.Date(\"2020-05-28\"), y = 50, label = paste(\"U.S. deaths\", \"\\n\", \"surpass 100,000\")), angle = 0, vjust = 0, fontface = \"italic\", color = \"black\") +\n  geom_text(aes(x = as.Date(\"2020-09-02\"), y = 60, label = paste(\"Trump diagnosed\", \"\\n\", \"with COVID-19\")), angle = 0, vjust = 0, fontface = \"italic\", color = \"black\") +\n  geom_text(aes(x = as.Date(\"2020-11-07\"), y = 48, label = paste(\"Biden declared\", \"\\n\", \"election winner\")), angle = 0, vjust = 0, fontface = \"italic\", color = \"black\") +\n  geom_text(aes(x = as.Date(\"2021-01-20\"), y = 50, label = paste(\"Biden sworn\", \"\\n\", \"into office\")), angle = 0, vjust = 0, fontface = \"italic\", color = \"black\")\n\nWarning: Removed 12 rows containing missing values (`geom_line()`).\n\n\n\n\n\nNote: The code has been updated to reflect changes from suggestions from others. Also, the original graph is interactive, which in my case I did not capture. Here is the original graph again for comparison:\n[]"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#data-visualization",
    "href": "presentation-exercise/presentation-exercise.html#data-visualization",
    "title": "Presentation Exercise",
    "section": "",
    "text": "For this exercise I chose the third graph from the “How Americans view Biden’s response to the coronavirus crisis” article from FiveThirtyEight (https://projects.fivethirtyeight.com/coronavirus-polls/). The graph is a time trend on how americans were worried about they or someone beloved to getting infected with COVID-19, from February 2020 to April 2021. To recreate the graph I used the RTutor AI online tool, where after many attempts I was given a product that worked but still, I had to make some small tweaks to make the graph more alike the original one.\nThis is the original graph from the article:\n[]\nPackages I used for this exercise:\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.3\n\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nWarning: package 'tibble' was built under R version 4.2.3\n\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\n\nWarning: package 'readr' was built under R version 4.2.3\n\n\nWarning: package 'purrr' was built under R version 4.2.3\n\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\nWarning: package 'stringr' was built under R version 4.2.3\n\n\nWarning: package 'forcats' was built under R version 4.2.3\n\n\nWarning: package 'lubridate' was built under R version 4.2.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(scales)\n\nWarning: package 'scales' was built under R version 4.2.3\n\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nlibrary(here)\n\nWarning: package 'here' was built under R version 4.2.3\n\n\nhere() starts at C:/Users/malik/Documents/1. UGA Classes/15. Malika Spring 2024/MADASpring_24/erickmollinedo-MADA-portfolio\n\nlibrary(readr)\nlibrary(lubridate)\n\nLoading the dataset and assign it to the covid_poll object:\n\n#Using the `read_csv()` and `here()` functions to load the dataset\ncovid_poll &lt;- read_csv(here(\"presentation-exercise\", \"data\", \"covid_concern_toplines.csv\"))\n\nRows: 1496 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (4): subject, modeldate, party, timestamp\ndbl (4): very_estimate, somewhat_estimate, not_very_estimate, not_at_all_est...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nFirst I transformed the modeldate variable to date type using the lubridate package. I assigned it to the covid_infect dataframe. I also filtered only the responses needed for the graph, from the subject variable.\n\n# Transform 'modeldate' to date type and filter the data\ncovid_infect &lt;- covid_poll %&gt;%\n  mutate(modeldate = mdy(modeldate)) %&gt;% #Mutate the `modeldate` variable to month/day/year\n  filter(modeldate &lt; as.Date(\"2021-04-22\") & subject == 'concern-infected') #Transform the `modeldate` variable as date type and filter the 'concern-infected' value from the `subject` variable.\n\nNow I decided to produce weekly averages, instead of using the daily datapoints, I assigned this to the covid_weekly dataframe\n\n# Calculate weekly averages of the estimates\ncovid_weekly &lt;- covid_infect %&gt;%\n  group_by(week = floor_date(modeldate, \"week\")) %&gt;% #Group by weeks\n  summarize( #Produce the summaries for each response variable (4 code lines below)\n    very_estimate = mean(very_estimate, na.rm = TRUE),\n    somewhat_estimate = mean(somewhat_estimate, na.rm = TRUE),\n    not_very_estimate = mean(not_very_estimate, na.rm = TRUE),\n    not_at_all_estimate = mean(not_at_all_estimate, na.rm = TRUE),\n    .groups = 'drop')\n\nThen I created the covid_longer dataframe using the pivot_longer() function, so the data is more easy to be used for the final graph.\n\n# Pivot data to long format for plotting\ncovid_longer &lt;- covid_weekly %&gt;%\n  pivot_longer(cols = c(very_estimate, somewhat_estimate, not_very_estimate, not_at_all_estimate), #Use `pivot_longer()` to mutate the dataframe\n               names_to = \"estimate_type\",\n               values_to = \"estimate\") %&gt;%\n  mutate(estimate_label = recode(estimate_type, #Recode the values of the response variables to characters more legible (4 code lines below)\n                                 \"very_estimate\" = \"Very\",\n                                 \"somewhat_estimate\" = \"Somewhat\",\n                                 \"not_very_estimate\" = \"Not Very\",\n                                 \"not_at_all_estimate\" = \"Not at all\"))\n\nAnd finally creating the plot using ggplot(). Each code chunk is detailed below.\n\n#Creating the time trend graph\nggplot(covid_longer, aes(x = week, y = estimate, group = estimate_label, color = estimate_label)) + #Selecting the x and y variables and grouping by `estimate_label`\n  geom_line() + #Select a line graph\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%m/%d\", #Selecting the x-scale breaks in 1-month intervals and selecting as month/day format\n               limits = as.Date(c(\"2020-02-01\", \"2021-04-01\"))) + #Selecting the start and end date limits\n  scale_y_continuous(limits = c(0, 75), breaks = c(0, 25, 50)) + #Selecting the y-axis limits, and set the breaks\n  scale_color_manual(values = c(\"Very\" = \"red\", \"Somewhat\" = \"orange\", \n                                \"Not Very\" = \"mediumpurple1\", \"Not at all\" = \"purple\")) + #Setting the colors\n  labs(title = \"How worried are Americans about infection?\", #Writing the title\n    subtitle = paste(\"How concerned Americans say they are that they, someone in their family or someone else they know will\", \"\\n\", \"become infected with the coronavirus\"))+ #Writing the subtitle, and using \"\\n\" to sepparate it in two lines\n  theme_minimal() + #setting the theme\n  theme(legend.position = \"bottom\", #The position of the legend for level of concern\n    plot.title = element_text(hjust = 0.5, size = 10), #Position and size of the title\n    plot.subtitle = element_text(hjust = 0.5, size = 8), #Position and size of the subtitle\n    axis.title.x = element_blank(), #Removing the x-axis label\n    axis.title.y = element_blank())+ #Removing the y-axis label\n  geom_vline(xintercept = as.Date(\"2020-02-29\"), linetype = \"dashed\") + #Setting a dashed line on a specific date with text below, the following 4 lines of code are for another 4 specific lines\n  geom_vline(xintercept = as.Date(\"2020-05-28\"), linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2020-10-02\"), linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2020-11-07\"), linetype = \"dashed\") +\n  geom_vline(xintercept = as.Date(\"2021-01-20\"), linetype = \"dashed\") +\n  geom_text(aes(x = as.Date(\"2020-02-29\"), y = 50, label = paste(\"First U.S.\", \"\\n\", \"death reported\")), angle = 0, vjust = 0, fontface = \"italic\", color = \"black\") + #Setting the text for the first dashed line, breaking it into two text parts so they fit inside the plot, also setting the angle at 0 and in italic font. (The following lines of code are for the other 4 texts)\n  geom_text(aes(x = as.Date(\"2020-05-28\"), y = 50, label = paste(\"U.S. deaths\", \"\\n\", \"surpass 100,000\")), angle = 0, vjust = 0, fontface = \"italic\", color = \"black\") +\n  geom_text(aes(x = as.Date(\"2020-09-02\"), y = 60, label = paste(\"Trump diagnosed\", \"\\n\", \"with COVID-19\")), angle = 0, vjust = 0, fontface = \"italic\", color = \"black\") +\n  geom_text(aes(x = as.Date(\"2020-11-07\"), y = 48, label = paste(\"Biden declared\", \"\\n\", \"election winner\")), angle = 0, vjust = 0, fontface = \"italic\", color = \"black\") +\n  geom_text(aes(x = as.Date(\"2021-01-20\"), y = 50, label = paste(\"Biden sworn\", \"\\n\", \"into office\")), angle = 0, vjust = 0, fontface = \"italic\", color = \"black\")\n\nWarning: Removed 12 rows containing missing values (`geom_line()`).\n\n\n\n\n\nNote: The code has been updated to reflect changes from suggestions from others. Also, the original graph is interactive, which in my case I did not capture. Here is the original graph again for comparison:\n[]"
  },
  {
    "objectID": "presentation-exercise/presentation-exercise.html#presentation-of-results",
    "href": "presentation-exercise/presentation-exercise.html#presentation-of-results",
    "title": "Presentation Exercise",
    "section": "Presentation of results",
    "text": "Presentation of results\nTo create a table I used the same dataset as above.\nI loaded an extra package for this part:\n\nlibrary(gt)\n\nWarning: package 'gt' was built under R version 4.2.3\n\n\nFirst I created a new object called covid_summary, that will be used to create the table.\n\n# Create the dataframe used as base for the table\ncovid_summary &lt;- covid_infect %&gt;%\n  mutate(month = floor_date(as.Date(modeldate, format = \"%Y-%m-%d\"), \"month\")) %&gt;% #Make the date in consistent format and mutate the `start_date` variable so it's named as `month`\n  group_by(month) %&gt;% #Group by month of the year\n  summarise( #Create the average percent by month, and round it to two decimal places (applies to the following 4 lines of code)\n    avg_very_estimate = round(mean(very_estimate, na.rm = TRUE), 2),\n    avg_somewhat_estimate = round(mean(somewhat_estimate, na.rm = TRUE), 2),\n    avg_not_very_estimate = round(mean(not_very_estimate, na.rm = TRUE), 2),\n    avg_not_at_all_estimate = round(mean(not_at_all_estimate, na.rm = TRUE), 2)\n  ) %&gt;%\n  mutate(across(starts_with(\"avg_\"), ~ as.numeric(format(., nsmall = 2)))) %&gt;% #Change variables to numeric\n  rename(Month = \"month\", #Rename the variables to be more legible\n         Very = \"avg_very_estimate\",\n         Somewhat = \"avg_somewhat_estimate\",\n         `Not very` = \"avg_not_very_estimate\",\n         `Not at all` = \"avg_not_at_all_estimate\")\n\nAnd now creating the table using the gt package, and apply some style edits.\n\n#Create a professional table using the `gt` package.\ncovid_summary %&gt;% gt() %&gt;% #Create the base table\n   tab_header(\n    title = \"How worried are Americans about COVID-19 infection?\") %&gt;% #Attach a title to the table\n  tab_spanner(label = \"Concern Percentage\", #Create a subtitle or header for columns 2 to 5\n    columns = vars(Very, Somewhat, `Not very`, `Not at all`)) %&gt;% #Select the columns or variables\n  tab_style(style = cell_text(weight = \"bold\"), locations = cells_column_labels(columns=c(\"Month\", \"Very\", \"Somewhat\", \"Not very\", \"Not at all\"))) %&gt;% #Setting the column labels in bold\n  tab_style(style = cell_text(weight = \"bold\"), locations = cells_title()) #Setting the title in bold\n\nWarning: Since gt v0.3.0, `columns = vars(...)` has been deprecated.\n• Please use `columns = c(...)` instead.\n\n\n\n\n\n\n  \n    \n      How worried are Americans about COVID-19 infection?\n    \n    \n    \n      Month\n      \n        Concern Percentage\n      \n    \n    \n      Very\n      Somewhat\n      Not very\n      Not at all\n    \n  \n  \n    2020-02-01\n17.48\n24.24\n36.61\n20.67\n    2020-03-01\n24.47\n33.54\n26.97\n13.87\n    2020-04-01\n33.96\n37.27\n18.85\n8.90\n    2020-05-01\n32.06\n36.47\n19.80\n11.03\n    2020-06-01\n29.07\n35.64\n21.00\n13.22\n    2020-07-01\n33.79\n34.08\n18.17\n11.91\n    2020-08-01\n35.02\n33.75\n17.79\n11.30\n    2020-09-01\n32.77\n33.77\n19.80\n12.12\n    2020-10-01\n31.33\n34.89\n19.24\n11.98\n    2020-11-01\n32.63\n34.49\n18.47\n12.32\n    2020-12-01\n35.36\n34.14\n17.72\n11.30\n    2021-01-01\n35.36\n33.02\n17.33\n11.65\n    2021-02-01\n36.17\n30.77\n17.68\n11.64\n    2021-03-01\n30.80\n33.52\n20.34\n13.20\n    2021-04-01\n27.17\n32.90\n25.60\n15.28\n  \n  \n  \n\n\n\n\nThe product is a simple table, but we can see more clear that overall, the majority of people were very or somewhat concerned of a COVID-19 infection from February 2020 to April 2021."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html",
    "href": "fitting-exercise/fitting-exercise.html",
    "title": "Fitting exercise",
    "section": "",
    "text": "These are the packages I used for this exercise\n\nlibrary(here)\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gtsummary)\nlibrary(GGally)\n\nLoading the dataset, assigned it to the mavoglurant dataframe.\n\nmavoglurant &lt;- read_csv(here(\"fitting-exercise\", \"Mavoglurant_A2121_nmpk.csv\"))\n\nRows: 2678 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (17): ID, CMT, EVID, EVI2, MDV, DV, LNDV, AMT, TIME, DOSE, OCC, RATE, AG...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nFirst, I created a plot showing the concentration of Mavoglurant DV over TIME, by DOSE. In the first attempt, the dose was plotted as a numeric variable so I mutated DOSE to be a categorical variable.\n\n#Make `DOSE` a categorical variable using as.factor().\nmavoglurant &lt;- mavoglurant %&gt;%\n  mutate(DOSE = as.factor(DOSE))\n\n#Create the plot of concentration by time, categorized by dose using ggplot().\nggplot(mavoglurant, aes(x = TIME, y = DV, group= ID)) +\n  geom_line() + #Do a line plot\n  facet_wrap(~ DOSE) + #Group by DOSE\n  labs(x = \"Time\", y = \"Mavoglurant concentration\", color = \"Dose\")\n\n\n\n\n\n\n\n\nNow, keeping just one of the observations for individuals that have two OCC observations.\n\nmavoglurant &lt;- mavoglurant %&gt;% filter(OCC == 1)\n\nNow, removing observations where TIME is equal to 0 and create a new dataframe mavoglurant_sum where it summarizes the concentrations from DV by each subject. Then, I created the mavoglurant_zero dataframe that contains only the observations where TIME is equal to 0. An finally I joined both new dataframes into the mavoglurant_new df.\n\n# Exclude observations where 'TIME' = 0 and then compute the sum of 'DV' for each subject or 'ID', to create the `mavoglurant_sum` dataframe.\nmavoglurant_sum &lt;- mavoglurant %&gt;%\n  filter(TIME != 0) %&gt;% #Remove observations where time= 0\n  group_by(ID) %&gt;% #Group by subject\n  summarize(Y = sum(DV)) #The sum variable is called `Y`\n\n#Create a dataframe with observations where TIME= 0.\nmavoglurant_zero &lt;- mavoglurant %&gt;% \n  filter(TIME == 0) %&gt;% \n  group_by(ID)\n\n#Join the previous dataframes using left_join()\nmavoglurant_new &lt;- inner_join(mavoglurant_sum, mavoglurant_zero, by = \"ID\")\n\nFinally, I filtered out unnecessary variables for this exercise and RACE, and SEX were converted to factor type variables.\n\n#Mutate SEX and RACE to factory type variables and then only keep Y, DOSE, AGE, SEX, RACE, WT and HT.\nmavoglurant_new &lt;- mavoglurant_new %&gt;% \n  mutate(RACE = as.factor(RACE), SEX = as.factor(SEX)) %&gt;% \n  select(c(Y, DOSE, AGE, SEX, RACE, WT, HT))\n\n#Check the structure of the new dataframe\nstr(mavoglurant_new)\n\ntibble [120 × 7] (S3: tbl_df/tbl/data.frame)\n $ Y   : num [1:120] 2691 2639 2150 1789 3126 ...\n $ DOSE: Factor w/ 3 levels \"25\",\"37.5\",\"50\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AGE : num [1:120] 42 24 31 46 41 27 23 20 23 28 ...\n $ SEX : Factor w/ 2 levels \"1\",\"2\": 1 1 1 2 2 1 1 1 1 1 ...\n $ RACE: Factor w/ 4 levels \"1\",\"2\",\"7\",\"88\": 2 2 1 1 2 2 1 4 2 1 ...\n $ WT  : num [1:120] 94.3 80.4 71.8 77.4 64.3 ...\n $ HT  : num [1:120] 1.77 1.76 1.81 1.65 1.56 ...\n\n\n\n\n\nThe following plots and tables summarize the data observed from the mavoglurant_new dataframe.\nFirst, a Boxplot that shows the dependent variable (Y) across the three different doses.\n\n#Using ggplot() to create a boxplot of the predicted variable Y and the DOSE\nggplot(mavoglurant_new, aes(x= DOSE, y= Y))+\n  geom_boxplot(fill= \"aquamarine3\")+\n  theme_classic()+\n  labs(x= \"Dose\", y= \"Mavoglurant concentration\")\n\n\n\n\n\n\n\n\nBased on the previous plot, it can be observed that at higher dose, the concentration of mavoglurant (predicted variable) increases. It is also seen that the range of concentrations is higher at the higher dose (50).\nNow some plots that show the distribution of the dependent variable (Y) and the numeric independent variables AGE, WT and HT.\n\n#Histogram of the dependent variable (Y)\nggplot(mavoglurant_new, aes(x= Y))+\n  geom_histogram(fill= \"aquamarine3\", color= \"red\")+\n  labs(x= \"Mavoglurant concentration\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n#Histogram of AGE\nggplot(mavoglurant_new, aes(x= AGE))+\n  geom_histogram(fill= \"darkgoldenrod1\", color= \"red\")+\n  labs(x= \"Age\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n#Histogram of WT\nggplot(mavoglurant_new, aes(x= WT))+\n  geom_histogram(fill= \"darkgoldenrod1\", color= \"red\")+\n  labs(x= \"Weight\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n#Histogram of HT\nggplot(mavoglurant_new, aes(x= HT))+\n  geom_histogram(fill= \"darkgoldenrod1\", color= \"red\")+\n  labs(x= \"Height\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nIn the previous plots in can be seen that the dependent (Y) variable and the Weight, follow a normal distribution. Height is observed that is skewed to the right, so this variable could not be following a normal distribution. On the other hand, it is observed that Age follows a bi-modal distribution. This is providing an insight about maybe first applying a regression model to this dataset.\nThe following table summarizes the previous variables, categorized by SEX (1 or 2). Here, it is shown the mean (sd), median (IQR) and the range.\n\n#Creating a summary table using the tbl_summary() function from `gtsummary`\nsumtable &lt;- mavoglurant_new %&gt;% select(Y, AGE, HT, WT, SEX) %&gt;% \n  tbl_summary(by= SEX, \n              type = all_continuous() ~ \"continuous2\",\n              statistic = all_continuous() ~ c(\"{mean} ({sd})\", \"{median} ({p25}, {p75})\", \"{min}, {max}\")) %&gt;% \n  bold_labels()\n\n#Visualize the table\nsumtable\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n1, N = 104\n2, N = 16\n\n\n\n\nY\n\n\n\n\n\n\n    Mean (SD)\n2,478 (959)\n2,236 (983)\n\n\n    Median (IQR)\n2,398 (1,727, 3,072)\n2,060 (1,491, 2,698)\n\n\n    Range\n826, 5,607\n1,044, 4,835\n\n\nAGE\n\n\n\n\n\n\n    Mean (SD)\n32 (9)\n41 (7)\n\n\n    Median (IQR)\n30 (25, 39)\n42 (38, 45)\n\n\n    Range\n18, 49\n28, 50\n\n\nHT\n\n\n\n\n\n\n    Mean (SD)\n1.78 (0.07)\n1.63 (0.06)\n\n\n    Median (IQR)\n1.78 (1.73, 1.82)\n1.63 (1.58, 1.66)\n\n\n    Range\n1.59, 1.93\n1.52, 1.75\n\n\nWT\n\n\n\n\n\n\n    Mean (SD)\n84 (12)\n73 (11)\n\n\n    Median (IQR)\n83 (75, 92)\n70 (64, 81)\n\n\n    Range\n57, 115\n58, 90\n\n\n\n\n\n\n\nAnd here, showing barplots for the categorical variables SEX and RACE.\n\n#Creating a bar plot that shows the counts for each race category by sex.\nggplot(mavoglurant_new, aes(x= RACE, fill= SEX))+\n  geom_bar(position = \"dodge\")+\n  theme_classic()+\n  labs(x= \"Race\")\n\n\n\n\n\n\n\n\nIt is observed on the previous plot that there are more subjects of sex 1, than 2 for the 1, 2 and 88 race categories. Meanwhile for the race category 7, it seems that there is the same amount of subjects by sex category. It is a shame that the correct labels for these categories are not known for sure.\nAnd finally, exploring correlations between all the variables, visualizing by a plot:\n\n#Creating a correlation plot using the ggpairs() function from the GGally package.\nggpairs(mavoglurant_new, columns = c(1, 3, 6, 7), progress = F)\n\n\n\n\n\n\n\n\nBased on this plot it is observed that the highest correlation is between the variables Height and Weight (0.6), and the linear plots in the middle confirm the distribution of each one of the variables.\n\n\n\n\n\nFirst, I fitted a linear model using the continuous outcome (Y) and DOSE as the predictor.\n\n# Define the model specification for linear regression\nlinear_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;% #Specify the linear model to fit the model\n  set_mode(\"regression\") #Setting the mode as a regression model\n\n# Define the formula\nformula1 &lt;- Y ~ DOSE\n\n# Fit the model\nlm_simple &lt;- linear_model %&gt;%\n  fit(formula1, data = mavoglurant_new) #Calling the formula and the dataframe to compute the linear model\n\n# Output the model summary\nsummary(lm_simple$fit)\n\n\nCall:\nstats::lm(formula = Y ~ DOSE, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1290.1  -445.6   -90.9   352.2  2367.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1782.67      87.85  20.292  &lt; 2e-16 ***\nDOSE37.5      681.24     213.69   3.188  0.00184 ** \nDOSE50       1456.20     130.43  11.165  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 674.8 on 117 degrees of freedom\nMultiple R-squared:  0.5159,    Adjusted R-squared:  0.5076 \nF-statistic: 62.33 on 2 and 117 DF,  p-value: &lt; 2.2e-16\n\n\nBased on the model it can be inferred that the outcome increases by around 681.24 units with the dose 37.5 and increases by 1456.20 with the dose 50, all compared with the dose 25. It is also observed that the differences are statistically significant, given the p-values are less than 0.001.\nNow, fitting a linear model using the continuous outcome (Y) and using the rest of the variables as predictors.\n\n#The model specification has already been set in the previous code chunk, so there is no need to set it again.\n\n# Define the formula\nformula2 &lt;- Y ~ AGE + WT + HT + DOSE + SEX + RACE\n\n# Fit the model\nlm_multi &lt;- linear_model %&gt;%\n  fit(formula2, data = mavoglurant_new)\n\n# Output the model summary\nsummary(lm_multi$fit)\n\n\nCall:\nstats::lm(formula = Y ~ AGE + WT + HT + DOSE + SEX + RACE, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1496.97  -362.81   -71.26   285.84  2421.48 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4890.923   1822.710   2.683 0.008415 ** \nAGE            3.521      7.895   0.446 0.656517    \nWT           -23.281      6.440  -3.615 0.000454 ***\nHT          -741.050   1108.100  -0.669 0.505051    \nDOSE37.5     663.683    200.448   3.311 0.001258 ** \nDOSE50      1499.048    122.462  12.241  &lt; 2e-16 ***\nSEX2        -360.048    217.775  -1.653 0.101121    \nRACE2        148.883    129.821   1.147 0.253936    \nRACE7       -420.950    451.163  -0.933 0.352846    \nRACE88       -65.300    246.961  -0.264 0.791954    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 616.6 on 110 degrees of freedom\nMultiple R-squared:   0.62, Adjusted R-squared:  0.5889 \nF-statistic: 19.94 on 9 and 110 DF,  p-value: &lt; 2.2e-16\n\n\nFor the interpretation of this model I will focus only on the statistically significant predictors (p-value &lt; 0.001). Besides dose 37.5 with an increase of the outcome by a factor of ~664 and dose 50 with an increase by a factor of ~1500, Weight is also another variable associated with a decrease of the outcome by a factor of ~23.\nIn summary, it can be observed that the coefficients slightly changed between both models, however the second model seems a better fit. To evaluate which model is best, I computed the root mean square error (RMSE) and R-squared as metrics. First for the linear model using one predictor, and then using multiple predictors.\n\n#ONE VARIABLE AS PREDICTOR\n#Create a prediction from the dataframe\nlmsimple_pred &lt;- predict(lm_simple, new_data = mavoglurant_new %&gt;% select(-Y))\n\n#Match predicted with observed\nlmsimple_pred &lt;- bind_cols(lmsimple_pred, mavoglurant_new %&gt;% select(Y))\n\n#Estimate the metrics\nlmsimple_metrics &lt;- metric_set(rmse, rsq)\nlmsimple_metrics(lmsimple_pred, truth = Y, estimate = .pred)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     666.   \n2 rsq     standard       0.516\n\n#MULTIPLE VARIABLES AS PREDICTORS\n#Create a prediction from the dataframe\nlmmulti_pred &lt;- predict(lm_multi, new_data = mavoglurant_new %&gt;% select(-Y))\n\n#Match predicted with observed\nlmmulti_pred &lt;- bind_cols(lmmulti_pred, mavoglurant_new %&gt;% select(Y))\n\n#Estimate the metrics\nlmmulti_metrics &lt;- metric_set(rmse, rsq)\nlmmulti_metrics(lmmulti_pred, truth = Y, estimate = .pred)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     590.   \n2 rsq     standard       0.620\n\n\nWe can observe that the RMSE is lower (590.3) in the model that inputs all the variables as predictors compared to the linear model that uses Dose as a predictor (RMSE= 666.3). We also observe that the R2 is slightly higher in the second model (0.62) compared to the first model (0.52). In this case we can conclude that the second model (linear model with multiple predictors) is a better fit to this dataset.\n\n\n\nNow, I fitted a logistic model to the outcome SEX, and using DOSE as a predictor. I also evaluated the Accuracy and ROC-AUC of this model in the following steps.\n\n# Define the model specification\nlogistic_spec &lt;- logistic_reg() %&gt;%  #Defining as logistic\n  set_engine(\"glm\") %&gt;% #...From the GLM family\n  set_mode(\"classification\") #Classification, since it involves categorical variables\n\n# Create the recipe\nrecipe &lt;- recipe(SEX ~ DOSE, data = mavoglurant_new) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes())\n\n# Split the data into training and testing sets\nset.seed(123) #For reproducibility\ndata_split &lt;- initial_split(mavoglurant_new, prop = 0.75)\ntrain_data &lt;- training(data_split) #Create a training data to apply the model\ntest_data &lt;- testing(data_split) #Create a test data to apply the model evaluation\n\n# Fit the model\nlogistic_fit &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(logistic_spec) %&gt;%\n  fit(data = train_data)\n\n# Make predictions on the test set to determine the ROC-AUC of the model\npredictions &lt;- predict(logistic_fit, test_data, type = \"prob\")\n\n#Make predictions on the test set to determine the Accuracy of the model\npredictions2 &lt;- logistic_fit %&gt;% predict(new_data = test_data)\n\n# Bind the predictions to the testing set\nresults &lt;- bind_cols(test_data, predictions) #ROC-AUC\nresults2 &lt;- bind_cols(test_data, predictions2) #Accuracy\n\n# Calculate ROC-AUC\nroc_auc &lt;- roc_auc(results, truth = SEX, .pred_1)\n\n# Calculate Accuracy\naccuracy &lt;- accuracy(results2, truth = SEX, estimate = .pred_class)\n\n# Output the model and the metrics\nlog1 &lt;- glm(formula = SEX ~ DOSE, family = binomial(link = \"logit\"), \n    data = train_data)\nsummary(log1)\n\n\nCall:\nglm(formula = SEX ~ DOSE, family = binomial(link = \"logit\"), \n    data = train_data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.3581     0.3737  -3.634 0.000279 ***\nDOSE37.5      0.2595     0.8980   0.289 0.772583    \nDOSE50       -1.0986     0.7082  -1.551 0.120851    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 77.801  on 89  degrees of freedom\nResidual deviance: 74.572  on 87  degrees of freedom\nAIC: 80.572\n\nNumber of Fisher Scoring iterations: 5\n\nlist(Accuracy = accuracy, ROC_AUC = roc_auc)\n\n$Accuracy\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.933\n\n$ROC_AUC\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.393\n\n\nAnd finally, fitting a logistic model to the outcome SEX, using all of the variables as predictors. I also computed the ROC-AUC and Accuracy of this model.\n\n# The model has been defined before 'logistic_spec', so there is no need to define it again.\n\n# Create the recipe of this model\nrecipe2 &lt;- recipe(SEX ~ Y + AGE + WT + HT + DOSE + RACE, data = mavoglurant_new) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_predictors())\n\n# Split the data into training and testing sets\nset.seed(123) #For reproducibility\ndata_split2 &lt;- initial_split(mavoglurant_new, prop = 0.75)\ntrain_data2 &lt;- training(data_split2) #Create a training data to apply the model\ntest_data2 &lt;- testing(data_split2) #Create a test data to apply the model evaluation\n\n# Fit the model\nlogistic_fit2 &lt;- workflow() %&gt;%\n  add_recipe(recipe2) %&gt;%\n  add_model(logistic_spec) %&gt;%\n  fit(data = train_data)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n# Make predictions on the test set to determine the ROC-AUC of the model\npredictions_auc &lt;- predict(logistic_fit2, test_data2, type = \"prob\")\n\n#Make predictions on the test set to determine the Accuracy of the model\npredictions_acc &lt;- logistic_fit %&gt;% predict(new_data = test_data2)\n\n# Bind the predictions to the testing set\nresults_auc2 &lt;- bind_cols(test_data2, predictions_auc) #ROC-AUC\nresults_acc2 &lt;- bind_cols(test_data2, predictions_acc) #Accuracy\n\n# Calculate ROC-AUC\nroc_auc2 &lt;- roc_auc(results_auc2, truth = SEX, .pred_1)\n\n# Calculate Accuracy\naccuracy2 &lt;- accuracy(results_acc2, truth = SEX, estimate = .pred_class)\n\n# Output the metrics using list()\nlog2 &lt;- glm(formula = SEX ~ Y + AGE + WT + HT + DOSE + RACE, family = binomial(link = \"logit\"), \n    data = train_data2)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nsummary(log2)\n\n\nCall:\nglm(formula = SEX ~ Y + AGE + WT + HT + DOSE + RACE, family = binomial(link = \"logit\"), \n    data = train_data2)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) 119.467600  47.800737   2.499   0.0124 *\nY            -0.002105   0.002031  -1.036   0.3000  \nAGE           0.319506   0.174943   1.826   0.0678 .\nWT           -0.190608   0.130000  -1.466   0.1426  \nHT          -64.948813  26.352756  -2.465   0.0137 *\nDOSE37.5     -7.347424   8.122569  -0.905   0.3657  \nDOSE50       -3.665713   5.095677  -0.719   0.4719  \nRACE2        -6.722388   4.881126  -1.377   0.1684  \nRACE7        -3.259653  17.820091  -0.183   0.8549  \nRACE88       -5.593682  12.167640  -0.460   0.6457  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 77.801  on 89  degrees of freedom\nResidual deviance: 12.013  on 80  degrees of freedom\nAIC: 32.013\n\nNumber of Fisher Scoring iterations: 10\n\nlist(Accuracy = accuracy2, ROC_AUC = roc_auc2)\n\n$Accuracy\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.933\n\n$ROC_AUC\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.964\n\n\nBased on the previous logistic models, it is observed that appears there is no association between the dose of mavoglurant and sex. However, when observing the second logistic model, it appears there is a statistically significant association between height and sex (p-value &lt; 0.05). While looking at the accuracy from both models, we can see that both have the same accuracy (93%), however, the ROC-AUC value is pretty low for the model that uses only Dose as a predictor (0.39), meanwhile, the model that uses dose and all the other variables as predictors has a better value (0.96), which reflects better sensitivity and specificity."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#mavoglurant-modeling-exercise",
    "href": "fitting-exercise/fitting-exercise.html#mavoglurant-modeling-exercise",
    "title": "Fitting exercise",
    "section": "",
    "text": "These are the packages I used for this exercise\n\nlibrary(here)\n\nhere() starts at C:/Users/molli/OneDrive/Documentos/UGA/Spring 2024/MADA/erickmollinedo-MADA-portfolio\n\nlibrary(readr)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ purrr     1.0.2\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──\n✔ broom        1.0.5      ✔ rsample      1.2.0 \n✔ dials        1.2.1      ✔ tune         1.1.2 \n✔ infer        1.0.5      ✔ workflows    1.1.4 \n✔ modeldata    1.3.0      ✔ workflowsets 1.0.1 \n✔ parsnip      1.2.0      ✔ yardstick    1.3.0 \n✔ recipes      1.0.10     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages\n\nlibrary(gtsummary)\n\n\nAttaching package: 'gtsummary'\n\nThe following objects are masked from 'package:recipes':\n\n    all_double, all_factor, all_integer, all_logical, all_numeric\n\nlibrary(GGally)\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nLoading the dataset, assigned it to the mavoglurant dataframe.\n\nmavoglurant &lt;- read_csv(here(\"fitting-exercise\", \"Mavoglurant_A2121_nmpk.csv\"))\n\nRows: 2678 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (17): ID, CMT, EVID, EVI2, MDV, DV, LNDV, AMT, TIME, DOSE, OCC, RATE, AG...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nFirst, I created a plot showing the concentration of Mavoglurant DV over TIME, by DOSE. In the first attempt, the dose was plotted as a numeric variable so I mutated DOSE to be a categorical variable.\n\n#Make `DOSE` a categorical variable using as.factor().\nmavoglurant &lt;- mavoglurant %&gt;%\n  mutate(DOSE = as.factor(DOSE))\n\n#Create the plot of concentration by time, categorized by dose using ggplot().\nggplot(mavoglurant, aes(x = TIME, y = DV, group= ID)) +\n  geom_line() + #Do a line plot\n  facet_wrap(~ DOSE) + #Group by DOSE\n  labs(x = \"Time\", y = \"Mavoglurant concentration\", color = \"Dose\")\n\n\n\n\n\n\n\n\nNow, keeping just one of the observations for individuals that have two OCC observations.\n\nmavoglurant &lt;- mavoglurant %&gt;% filter(OCC == 1)\n\nNow, removing observations where TIME is equal to 0 and create a new dataframe mavoglurant_sum where it summarizes the concentrations from DV by each subject. Then, I created the mavoglurant_zero dataframe that contains only the observations where TIME is equal to 0. An finally I joined both new dataframes into the mavoglurant_new df.\n\n# Exclude observations where 'TIME' = 0 and then compute the sum of 'DV' for each subject or 'ID', to create the `mavoglurant_sum` dataframe.\nmavoglurant_sum &lt;- mavoglurant %&gt;%\n  filter(TIME != 0) %&gt;% #Remove observations where time= 0\n  group_by(ID) %&gt;% #Group by subject\n  summarize(Y = sum(DV)) #The sum variable is called `Y`\n\n#Create a dataframe with observations where TIME= 0.\nmavoglurant_zero &lt;- mavoglurant %&gt;% \n  filter(TIME == 0) %&gt;% \n  group_by(ID)\n\n#Join the previous dataframes using left_join()\nmavoglurant_new &lt;- inner_join(mavoglurant_sum, mavoglurant_zero, by = \"ID\")\n\nFinally, I filtered out unnecessary variables for this exercise and RACE, and SEX were converted to factor type variables.\n\n#Mutate SEX and RACE to factory type variables and then only keep Y, DOSE, AGE, SEX, RACE, WT and HT.\nmavoglurant_new &lt;- mavoglurant_new %&gt;% \n  mutate(RACE = as.factor(RACE), SEX = as.factor(SEX)) %&gt;% \n  select(c(Y, DOSE, AGE, SEX, RACE, WT, HT))\n\n#Check the structure of the new dataframe\nstr(mavoglurant_new)\n\ntibble [120 × 7] (S3: tbl_df/tbl/data.frame)\n $ Y   : num [1:120] 2691 2639 2150 1789 3126 ...\n $ DOSE: Factor w/ 3 levels \"25\",\"37.5\",\"50\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AGE : num [1:120] 42 24 31 46 41 27 23 20 23 28 ...\n $ SEX : Factor w/ 2 levels \"1\",\"2\": 1 1 1 2 2 1 1 1 1 1 ...\n $ RACE: Factor w/ 4 levels \"1\",\"2\",\"7\",\"88\": 2 2 1 1 2 2 1 4 2 1 ...\n $ WT  : num [1:120] 94.3 80.4 71.8 77.4 64.3 ...\n $ HT  : num [1:120] 1.77 1.76 1.81 1.65 1.56 ...\n\n\n\n\n\nThe following plots and tables summarize the data observed from the mavoglurant_new dataframe.\nFirst, a Boxplot that shows the dependent variable (Y) across the three different doses.\n\n#Using ggplot() to create a boxplot of the predicted variable Y and the DOSE\nggplot(mavoglurant_new, aes(x= DOSE, y= Y))+\n  geom_boxplot(fill= \"aquamarine3\")+\n  theme_classic()+\n  labs(x= \"Dose\", y= \"Mavoglurant concentration\")\n\n\n\n\n\n\n\n\nBased on the previous plot, it can be observed that at higher dose, the concentration of mavoglurant (predicted variable) increases. It is also seen that the range of concentrations is higher at the higher dose (50).\nNow some plots that show the distribution of the dependent variable (Y) and the numeric independent variables AGE, WT and HT.\n\n#Histogram of the dependent variable (Y)\nggplot(mavoglurant_new, aes(x= Y))+\n  geom_histogram(fill= \"aquamarine3\", color= \"red\")+\n  labs(x= \"Mavoglurant concentration\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n#Histogram of AGE\nggplot(mavoglurant_new, aes(x= AGE))+\n  geom_histogram(fill= \"darkgoldenrod1\", color= \"red\")+\n  labs(x= \"Age\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n#Histogram of WT\nggplot(mavoglurant_new, aes(x= WT))+\n  geom_histogram(fill= \"darkgoldenrod1\", color= \"red\")+\n  labs(x= \"Weight\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n#Histogram of HT\nggplot(mavoglurant_new, aes(x= HT))+\n  geom_histogram(fill= \"darkgoldenrod1\", color= \"red\")+\n  labs(x= \"Height\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nIn the previous plots in can be seen that the dependent (Y) variable and the Weight, follow a normal distribution. Height is observed that is skewed to the right, so this variable could not be following a normal distribution. On the other hand, it is observed that Age follows a bi-modal distribution. This is providing an insight about maybe first applying a regression model to this dataset.\nThe following table summarizes the previous variables, categorized by SEX (1 or 2). Here, it is shown the mean (sd), median (IQR) and the range.\n\n#Creating a summary table using the tbl_summary() function from `gtsummary`\nsumtable &lt;- mavoglurant_new %&gt;% select(Y, AGE, HT, WT, SEX) %&gt;% \n  tbl_summary(by= SEX, \n              type = all_continuous() ~ \"continuous2\",\n              statistic = all_continuous() ~ c(\"{mean} ({sd})\", \"{median} ({p25}, {p75})\", \"{min}, {max}\")) %&gt;% \n  bold_labels()\n\n#Visualize the table\nsumtable\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n1, N = 104\n2, N = 16\n\n\n\n\nY\n\n\n\n\n\n\n    Mean (SD)\n2,478 (959)\n2,236 (983)\n\n\n    Median (IQR)\n2,398 (1,727, 3,072)\n2,060 (1,491, 2,698)\n\n\n    Range\n826, 5,607\n1,044, 4,835\n\n\nAGE\n\n\n\n\n\n\n    Mean (SD)\n32 (9)\n41 (7)\n\n\n    Median (IQR)\n30 (25, 39)\n42 (38, 45)\n\n\n    Range\n18, 49\n28, 50\n\n\nHT\n\n\n\n\n\n\n    Mean (SD)\n1.78 (0.07)\n1.63 (0.06)\n\n\n    Median (IQR)\n1.78 (1.73, 1.82)\n1.63 (1.58, 1.66)\n\n\n    Range\n1.59, 1.93\n1.52, 1.75\n\n\nWT\n\n\n\n\n\n\n    Mean (SD)\n84 (12)\n73 (11)\n\n\n    Median (IQR)\n83 (75, 92)\n70 (64, 81)\n\n\n    Range\n57, 115\n58, 90\n\n\n\n\n\n\n\nAnd here, showing barplots for the categorical variables SEX and RACE.\n\n#Creating a bar plot that shows the counts for each race category by sex.\nggplot(mavoglurant_new, aes(x= RACE, fill= SEX))+\n  geom_bar(position = \"dodge\")+\n  theme_classic()+\n  labs(x= \"Race\")\n\n\n\n\n\n\n\n\nIt is observed on the previous plot that there are more subjects of sex 1, than 2 for the 1, 2 and 88 race categories. Meanwhile for the race category 7, it seems that there is the same amount of subjects by sex category. It is a shame that the correct labels for these categories are not known for sure.\nAnd finally, exploring correlations between all the variables, visualizing by a plot:\n\n#Creating a correlation plot using the ggpairs() function from the GGally package.\nggpairs(mavoglurant_new, columns = c(1, 3, 6, 7), progress = F)\n\n\n\n\n\n\n\n\nBased on this plot it is observed that the highest correlation is between the variables Height and Weight (0.6), and the linear plots in the middle confirm the distribution of each one of the variables.\n\n\n\n\n\nFirst, I fitted a linear model using the continuous outcome (Y) and DOSE as the predictor.\n\n# Define the model specification for linear regression\nlinear_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;% #Specify the linear model to fit the model\n  set_mode(\"regression\") #Setting the mode as a regression model\n\n# Define the formula\nformula1 &lt;- Y ~ DOSE\n\n# Fit the model\nlm_simple &lt;- linear_model %&gt;%\n  fit(formula1, data = mavoglurant_new) #Calling the formula and the dataframe to compute the linear model\n\n# Output the model summary\nsummary(lm_simple$fit)\n\n\nCall:\nstats::lm(formula = Y ~ DOSE, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1290.1  -445.6   -90.9   352.2  2367.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1782.67      87.85  20.292  &lt; 2e-16 ***\nDOSE37.5      681.24     213.69   3.188  0.00184 ** \nDOSE50       1456.20     130.43  11.165  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 674.8 on 117 degrees of freedom\nMultiple R-squared:  0.5159,    Adjusted R-squared:  0.5076 \nF-statistic: 62.33 on 2 and 117 DF,  p-value: &lt; 2.2e-16\n\n\nBased on the model it can be infered that the outcome increases by around 681.24 units with the dose 37.5 and increases by 1456.20 with the dose 50, all compared with the dose 25. It is also observed that the differences are statistically significant, given the p-values are less than 0.001.\nNow, fitting a linear model using the continuous outcome (Y) and using the rest of the variables as predictors.\n\n#The model specification has already been set in the previous code chunk, so there is no need to set it again.\n\n# Define the formula\nformula2 &lt;- Y ~ AGE + WT + HT + DOSE + SEX + RACE\n\n# Fit the model\nlm_multi &lt;- linear_model %&gt;%\n  fit(formula2, data = mavoglurant_new)\n\n# Output the model summary\nsummary(lm_multi$fit)\n\n\nCall:\nstats::lm(formula = Y ~ AGE + WT + HT + DOSE + SEX + RACE, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1496.97  -362.81   -71.26   285.84  2421.48 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4890.923   1822.710   2.683 0.008415 ** \nAGE            3.521      7.895   0.446 0.656517    \nWT           -23.281      6.440  -3.615 0.000454 ***\nHT          -741.050   1108.100  -0.669 0.505051    \nDOSE37.5     663.683    200.448   3.311 0.001258 ** \nDOSE50      1499.048    122.462  12.241  &lt; 2e-16 ***\nSEX2        -360.048    217.775  -1.653 0.101121    \nRACE2        148.883    129.821   1.147 0.253936    \nRACE7       -420.950    451.163  -0.933 0.352846    \nRACE88       -65.300    246.961  -0.264 0.791954    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 616.6 on 110 degrees of freedom\nMultiple R-squared:   0.62, Adjusted R-squared:  0.5889 \nF-statistic: 19.94 on 9 and 110 DF,  p-value: &lt; 2.2e-16\n\n\nFor the interpretation of this model I will focus only on the statistically significant predictors (p-value &lt; 0.001). Besides dose 37.5 with an increase of the outcome by a factor of ~664 and dose 50 with an increase by a factor of ~1500, Weight is also another variable associated with a decrease of the outcome by a factor of ~23.\nIn summary, it can be observed that the coefficients slightly changed between both models, however the second model seems a better fit. To evaluate which model is best, I computed the root mean square error (RMSE) and R-squared as metrics. First for the linear model using one predictor, and then using multiple predictors.\n\n#ONE VARIABLE AS PREDICTOR\n#Create a prediction from the dataframe\nlmsimple_pred &lt;- predict(lm_simple, new_data = mavoglurant_new %&gt;% select(-Y))\n\n#Match predicted with observed\nlmsimple_pred &lt;- bind_cols(lmsimple_pred, mavoglurant_new %&gt;% select(Y))\n\n#Estimate the metrics\nlmsimple_metrics &lt;- metric_set(rmse, rsq)\nlmsimple_metrics(lmsimple_pred, truth = Y, estimate = .pred)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     666.   \n2 rsq     standard       0.516\n\n#MULTIPLE VARIABLES AS PREDICTORS\n#Create a prediction from the dataframe\nlmmulti_pred &lt;- predict(lm_multi, new_data = mavoglurant_new %&gt;% select(-Y))\n\n#Match predicted with observed\nlmmulti_pred &lt;- bind_cols(lmmulti_pred, mavoglurant_new %&gt;% select(Y))\n\n#Estimate the metrics\nlmmulti_metrics &lt;- metric_set(rmse, rsq)\nlmmulti_metrics(lmmulti_pred, truth = Y, estimate = .pred)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     590.   \n2 rsq     standard       0.620\n\n\nWe can observe that the RMSE is lower (590.3) in the model that inputs all the variables as predictors compared to the linear model that uses Dose as a predictor (RMSE= 666.3). We also observe that the R2 is slightly higher in the second model (0.62) compared to the first model (0.52). In this case we can conclude that the second model (linear model with multiple predictors) is a better fit to this dataset.\n\n\n\nNow, I fitted a logistic model to the outcome SEX, and using DOSE as a predictor. I also evaluated the Accuracy and ROC-AUC of this model in the following steps.\n\n# Define the model specification\nlogistic_spec &lt;- logistic_reg() %&gt;%  #Defining as logistic\n  set_engine(\"glm\") %&gt;% #...From the GLM family\n  set_mode(\"classification\") #Classification, since it involves categorical variables\n\n# Create the recipe\nrecipe &lt;- recipe(SEX ~ DOSE, data = mavoglurant_new) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes())\n\n# Split the data into training and testing sets\nset.seed(123) #For reproducibility\ndata_split &lt;- initial_split(mavoglurant_new, prop = 0.75)\ntrain_data &lt;- training(data_split) #Create a training data to apply the model\ntest_data &lt;- testing(data_split) #Create a test data to apply the model evaluation\n\n# Fit the model\nlogistic_fit &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(logistic_spec) %&gt;%\n  fit(data = train_data)\n\n# Make predictions on the test set to determine the ROC-AUC of the model\npredictions &lt;- predict(logistic_fit, test_data, type = \"prob\")\n\n#Make predictions on the test set to determine the Accuracy of the model\npredictions2 &lt;- logistic_fit %&gt;% predict(new_data = test_data)\n\n# Bind the predictions to the testing set\nresults &lt;- bind_cols(test_data, predictions) #ROC-AUC\nresults2 &lt;- bind_cols(test_data, predictions2) #Accuracy\n\n# Calculate ROC-AUC\nroc_auc &lt;- roc_auc(results, truth = SEX, .pred_1)\n\n# Calculate Accuracy\naccuracy &lt;- accuracy(results2, truth = SEX, estimate = .pred_class)\n\n# Output the model and the metrics\nlog1 &lt;- glm(formula = SEX ~ DOSE, family = binomial(link = \"logit\"), \n    data = train_data)\nsummary(log1)\n\n\nCall:\nglm(formula = SEX ~ DOSE, family = binomial(link = \"logit\"), \n    data = train_data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.3581     0.3737  -3.634 0.000279 ***\nDOSE37.5      0.2595     0.8980   0.289 0.772583    \nDOSE50       -1.0986     0.7082  -1.551 0.120851    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 77.801  on 89  degrees of freedom\nResidual deviance: 74.572  on 87  degrees of freedom\nAIC: 80.572\n\nNumber of Fisher Scoring iterations: 5\n\nlist(Accuracy = accuracy, ROC_AUC = roc_auc)\n\n$Accuracy\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.933\n\n$ROC_AUC\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.393\n\n\nAnd finally, fitting a logistic model to the outcome SEX, using all of the variables as predictors. I also computed the ROC-AUC and Accuracy of this model.\n\n# The model has been defined before 'logistic_spec', so there is no need to define it again.\n\n# Create the recipe of this model\nrecipe2 &lt;- recipe(SEX ~ Y + AGE + WT + HT + DOSE + RACE, data = mavoglurant_new) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_predictors())\n\n# Split the data into training and testing sets\nset.seed(123) #For reproducibility\ndata_split2 &lt;- initial_split(mavoglurant_new, prop = 0.75)\ntrain_data2 &lt;- training(data_split2) #Create a training data to apply the model\ntest_data2 &lt;- testing(data_split2) #Create a test data to apply the model evaluation\n\n# Fit the model\nlogistic_fit2 &lt;- workflow() %&gt;%\n  add_recipe(recipe2) %&gt;%\n  add_model(logistic_spec) %&gt;%\n  fit(data = train_data)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n# Make predictions on the test set to determine the ROC-AUC of the model\npredictions_auc &lt;- predict(logistic_fit2, test_data2, type = \"prob\")\n\n#Make predictions on the test set to determine the Accuracy of the model\npredictions_acc &lt;- logistic_fit %&gt;% predict(new_data = test_data2)\n\n# Bind the predictions to the testing set\nresults_auc2 &lt;- bind_cols(test_data2, predictions_auc) #ROC-AUC\nresults_acc2 &lt;- bind_cols(test_data2, predictions_acc) #Accuracy\n\n# Calculate ROC-AUC\nroc_auc2 &lt;- roc_auc(results_auc2, truth = SEX, .pred_1)\n\n# Calculate Accuracy\naccuracy2 &lt;- accuracy(results_acc2, truth = SEX, estimate = .pred_class)\n\n# Output the metrics using list()\nlog2 &lt;- glm(formula = SEX ~ Y + AGE + WT + HT + DOSE + RACE, family = binomial(link = \"logit\"), \n    data = train_data2)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nsummary(log2)\n\n\nCall:\nglm(formula = SEX ~ Y + AGE + WT + HT + DOSE + RACE, family = binomial(link = \"logit\"), \n    data = train_data2)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) 119.467600  47.800737   2.499   0.0124 *\nY            -0.002105   0.002031  -1.036   0.3000  \nAGE           0.319506   0.174943   1.826   0.0678 .\nWT           -0.190608   0.130000  -1.466   0.1426  \nHT          -64.948813  26.352756  -2.465   0.0137 *\nDOSE37.5     -7.347424   8.122569  -0.905   0.3657  \nDOSE50       -3.665713   5.095677  -0.719   0.4719  \nRACE2        -6.722388   4.881126  -1.377   0.1684  \nRACE7        -3.259653  17.820091  -0.183   0.8549  \nRACE88       -5.593682  12.167640  -0.460   0.6457  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 77.801  on 89  degrees of freedom\nResidual deviance: 12.013  on 80  degrees of freedom\nAIC: 32.013\n\nNumber of Fisher Scoring iterations: 10\n\nlist(Accuracy = accuracy2, ROC_AUC = roc_auc2)\n\n$Accuracy\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.933\n\n$ROC_AUC\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.964\n\n\nBased on the previous logistic models, it is observed that appears there is no association between the dose of mavoglurant and sex. However, when observing the second logistic model, it appears there is a statistically significant association between height and sex (p-value &lt; 0.05). While looking at the accuracy from both models, we can see that both have the same accuracy (93%), however, the ROC-AUC value is pretty low for the model that uses only Dose as a predictor (0.39), meanwhile, the model that uses dose and all the other variables as predictors has a better value (0.96), which reflects better sensitivity and specificity."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html",
    "href": "coding-exercise/coding-exercise.html",
    "title": "R Coding Exercise",
    "section": "",
    "text": "The following space lists all the packages I used for this exercise:\nlibrary(dslabs)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nI explored the gapminder dataset from the dslabs package using the help() function to pull out the help page that describes the dataset\nhelp(\"gapminder\")\n\nstarting httpd help server ... done\nI explored the structure of the dataset using the str() function, the summary using the summary() function and used the class() function to check the type of object for this dataset.\nstr(gapminder)\n\n'data.frame':   10545 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 1 2 3 4 5 6 7 8 9 10 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  115.4 148.2 208 NA 59.9 ...\n $ life_expectancy : num  62.9 47.5 36 63 65.4 ...\n $ fertility       : num  6.19 7.65 7.32 4.43 3.11 4.55 4.82 3.45 2.7 5.57 ...\n $ population      : num  1636054 11124892 5270844 54681 20619075 ...\n $ gdp             : num  NA 1.38e+10 NA NA 1.08e+11 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 4 1 1 2 2 3 2 5 4 3 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 19 11 10 2 15 21 2 1 22 21 ...\n\nsummary(gapminder)\n\n                country           year      infant_mortality life_expectancy\n Albania            :   57   Min.   :1960   Min.   :  1.50   Min.   :13.20  \n Algeria            :   57   1st Qu.:1974   1st Qu.: 16.00   1st Qu.:57.50  \n Angola             :   57   Median :1988   Median : 41.50   Median :67.54  \n Antigua and Barbuda:   57   Mean   :1988   Mean   : 55.31   Mean   :64.81  \n Argentina          :   57   3rd Qu.:2002   3rd Qu.: 85.10   3rd Qu.:73.00  \n Armenia            :   57   Max.   :2016   Max.   :276.90   Max.   :83.90  \n (Other)            :10203                  NA's   :1453                    \n   fertility       population             gdp               continent   \n Min.   :0.840   Min.   :3.124e+04   Min.   :4.040e+07   Africa  :2907  \n 1st Qu.:2.200   1st Qu.:1.333e+06   1st Qu.:1.846e+09   Americas:2052  \n Median :3.750   Median :5.009e+06   Median :7.794e+09   Asia    :2679  \n Mean   :4.084   Mean   :2.701e+07   Mean   :1.480e+11   Europe  :2223  \n 3rd Qu.:6.000   3rd Qu.:1.523e+07   3rd Qu.:5.540e+10   Oceania : 684  \n Max.   :9.220   Max.   :1.376e+09   Max.   :1.174e+13                  \n NA's   :187     NA's   :185         NA's   :2972                       \n             region    \n Western Asia   :1026  \n Eastern Africa : 912  \n Western Africa : 912  \n Caribbean      : 741  \n South America  : 684  \n Southern Europe: 684  \n (Other)        :5586  \n\nclass(gapminder)\n\n[1] \"data.frame\"\nFirst, I created the object africadata, which includes only observations from Africa. And then I checked the structure and summary of the new object using str() and summary()\n#I used the 'filter()' function to select only the observations from Africa, using the variable 'continent'\nafricadata &lt;- gapminder %&gt;% filter(continent == \"Africa\")\n\n#Check if the new object was correctly saved using the 'str()' and 'summary()' functions\nstr(africadata)\n\n'data.frame':   2907 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  1960 1960 1960 1960 1960 1960 1960 1960 1960 1960 ...\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n $ fertility       : num  7.65 7.32 6.28 6.62 6.29 6.95 5.65 6.89 5.84 6.25 ...\n $ population      : num  11124892 5270844 2431620 524029 4829291 ...\n $ gdp             : num  1.38e+10 NA 6.22e+08 1.24e+08 5.97e+08 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africadata)\n\n         country          year      infant_mortality life_expectancy\n Algeria     :  57   Min.   :1960   Min.   : 11.40   Min.   :13.20  \n Angola      :  57   1st Qu.:1974   1st Qu.: 62.20   1st Qu.:48.23  \n Benin       :  57   Median :1988   Median : 93.40   Median :53.98  \n Botswana    :  57   Mean   :1988   Mean   : 95.12   Mean   :54.38  \n Burkina Faso:  57   3rd Qu.:2002   3rd Qu.:124.70   3rd Qu.:60.10  \n Burundi     :  57   Max.   :2016   Max.   :237.40   Max.   :77.60  \n (Other)     :2565                  NA's   :226                     \n   fertility       population             gdp               continent   \n Min.   :1.500   Min.   :    41538   Min.   :4.659e+07   Africa  :2907  \n 1st Qu.:5.160   1st Qu.:  1605232   1st Qu.:8.373e+08   Americas:   0  \n Median :6.160   Median :  5570982   Median :2.448e+09   Asia    :   0  \n Mean   :5.851   Mean   : 12235961   Mean   :9.346e+09   Europe  :   0  \n 3rd Qu.:6.860   3rd Qu.: 13888152   3rd Qu.:6.552e+09   Oceania :   0  \n Max.   :8.450   Max.   :182201962   Max.   :1.935e+11                  \n NA's   :51      NA's   :51          NA's   :637                        \n                       region   \n Eastern Africa           :912  \n Western Africa           :912  \n Middle Africa            :456  \n Northern Africa          :342  \n Southern Africa          :285  \n Australia and New Zealand:  0  \n (Other)                  :  0\nHere, I created two new objects. africachild contains only the variables infant_mortality and life_expectancy, meanwhile africapop contains the population and life_expectancy variables. To do this I used the select() function. And then I explored the structure and summary of both new objects using str() and summary() to check if the objects were correctly created.\n#Creating new objects, using the 'select()' function to choose only the variables I need from the original dataset\nafricachild &lt;- africadata %&gt;% select(c(infant_mortality, life_expectancy))\nafricapop &lt;- africadata %&gt;% select(c(population, life_expectancy))\n\n#Using 'str()' to check the structure of the new objects\nstr(africachild)\n\n'data.frame':   2907 obs. of  2 variables:\n $ infant_mortality: num  148 208 187 116 161 ...\n $ life_expectancy : num  47.5 36 38.3 50.3 35.2 ...\n\nstr(africapop)\n\n'data.frame':   2907 obs. of  2 variables:\n $ population     : num  11124892 5270844 2431620 524029 4829291 ...\n $ life_expectancy: num  47.5 36 38.3 50.3 35.2 ...\n\n#Using 'summary()' to check the summary of the new objects\nsummary(africachild)\n\n infant_mortality life_expectancy\n Min.   : 11.40   Min.   :13.20  \n 1st Qu.: 62.20   1st Qu.:48.23  \n Median : 93.40   Median :53.98  \n Mean   : 95.12   Mean   :54.38  \n 3rd Qu.:124.70   3rd Qu.:60.10  \n Max.   :237.40   Max.   :77.60  \n NA's   :226                     \n\nsummary(africapop)\n\n   population        life_expectancy\n Min.   :    41538   Min.   :13.20  \n 1st Qu.:  1605232   1st Qu.:48.23  \n Median :  5570982   Median :53.98  \n Mean   : 12235961   Mean   :54.38  \n 3rd Qu.: 13888152   3rd Qu.:60.10  \n Max.   :182201962   Max.   :77.60  \n NA's   :51\nNow, I created a graph that illustrates life expectancy as a function of child mortality using ggplot(), with the africachild object.\n#Plotting life expenctancy as a function of infant mortality, using the 'ggplot()' package, in this case I used a point geometry using 'geom_point()'\nggplot(data= africachild, aes(x= infant_mortality, y= life_expectancy))+\n  geom_point()\n\nWarning: Removed 226 rows containing missing values (`geom_point()`).\nIt is observed that there is a negative correlation between infant mortality and life expectancy.\nAnd here, I created a graph that shows life expectancy as a function of population using ggplot(), with the africapop object.\n#I used the 'geom_point()' geometry to plot the data points and set the x-axis to log scale using 'scale_x_log10()'.\nggplot(data= africapop ,aes(x= population, y= life_expectancy))+\n  geom_point()+\n  scale_x_log10()\n\nWarning: Removed 51 rows containing missing values (`geom_point()`).\nThere is a positive correlation between life expectancy and population size, however there can be observed some weird ‘streaks’. If looking at the original dataset, this includes the population size from 1960 to 2016, so there are actually many years of observations instead of just one.\nI noticed there are years with missing data (NAs). To figure out which years have missing data I used the is.na() function in combination with filter().\n#Use `filter(is.na(infant_mortality)) to filter all observation that have missing values (NAs). Then I used 'count(year)', to summarize which years have the selected missing data.\nafricadata %&gt;% filter(is.na(infant_mortality)) %&gt;% count(year)\n\n   year  n\n1  1960 10\n2  1961 17\n3  1962 16\n4  1963 16\n5  1964 15\n6  1965 14\n7  1966 13\n8  1967 11\n9  1968 11\n10 1969  7\n11 1970  5\n12 1971  6\n13 1972  6\n14 1973  6\n15 1974  5\n16 1975  5\n17 1976  3\n18 1977  3\n19 1978  2\n20 1979  2\n21 1980  1\n22 1981  1\n23 2016 51\nThere is missing info on infant mortality from the years 1960 to 1981 and 2016. So it was best to choose a specific year for the analysis. Here, I created a new object africa2000 with only data from the year 2000. Then I checked the new data frame using str() and summary()\n#Create the object 'africa2000' using the 'filter()' function on the variable 'year' to select only the observations from the year 2000.\nafrica2000 &lt;- africadata %&gt;% filter(year == 2000)\n\n#Check the new object using 'str()' and 'summary()'\nstr(africa2000)\n\n'data.frame':   51 obs. of  9 variables:\n $ country         : Factor w/ 185 levels \"Albania\",\"Algeria\",..: 2 3 18 22 26 27 29 31 32 33 ...\n $ year            : int  2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 ...\n $ infant_mortality: num  33.9 128.3 89.3 52.4 96.2 ...\n $ life_expectancy : num  73.3 52.3 57.2 47.6 52.6 46.7 54.3 68.4 45.3 51.5 ...\n $ fertility       : num  2.51 6.84 5.98 3.41 6.59 7.06 5.62 3.7 5.45 7.35 ...\n $ population      : num  31183658 15058638 6949366 1736579 11607944 ...\n $ gdp             : num  5.48e+10 9.13e+09 2.25e+09 5.63e+09 2.61e+09 ...\n $ continent       : Factor w/ 5 levels \"Africa\",\"Americas\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ region          : Factor w/ 22 levels \"Australia and New Zealand\",..: 11 10 20 17 20 5 10 20 10 10 ...\n\nsummary(africa2000)\n\n         country        year      infant_mortality life_expectancy\n Algeria     : 1   Min.   :2000   Min.   : 12.30   Min.   :37.60  \n Angola      : 1   1st Qu.:2000   1st Qu.: 60.80   1st Qu.:51.75  \n Benin       : 1   Median :2000   Median : 80.30   Median :54.30  \n Botswana    : 1   Mean   :2000   Mean   : 78.93   Mean   :56.36  \n Burkina Faso: 1   3rd Qu.:2000   3rd Qu.:103.30   3rd Qu.:60.00  \n Burundi     : 1   Max.   :2000   Max.   :143.30   Max.   :75.00  \n (Other)     :45                                                  \n   fertility       population             gdp               continent \n Min.   :1.990   Min.   :    81154   Min.   :2.019e+08   Africa  :51  \n 1st Qu.:4.150   1st Qu.:  2304687   1st Qu.:1.274e+09   Americas: 0  \n Median :5.550   Median :  8799165   Median :3.238e+09   Asia    : 0  \n Mean   :5.156   Mean   : 15659800   Mean   :1.155e+10   Europe  : 0  \n 3rd Qu.:5.960   3rd Qu.: 17391242   3rd Qu.:8.654e+09   Oceania : 0  \n Max.   :7.730   Max.   :122876723   Max.   :1.329e+11                \n                                                                      \n                       region  \n Eastern Africa           :16  \n Western Africa           :16  \n Middle Africa            : 8  \n Northern Africa          : 6  \n Southern Africa          : 5  \n Australia and New Zealand: 0  \n (Other)                  : 0\nAnd here I made a new plot for life expectancy as a function of infant mortality for the year 2000 only.\nggplot(data= africa2000 ,aes(x= infant_mortality, y= life_expectancy))+\n  geom_point()\nAnd another plot of life expectancy as a function of population for the year 2000 only.\nggplot(data= africa2000 ,aes(x= population, y= life_expectancy))+\n  geom_point()+\n  scale_x_log10()\nIt is observed a negative association between infant mortality and life expectancy, however, the association between life expectancy and population is not clear. To do additional modeling, I tried to fit a linear model using the lm() function on the africa2000 dataset. I used life expectancy as the outcome and infant mortality as a predictor in the fit1 model, and life_expectancy also as outcome and population as the predictor in the fit2 model.\nfit1 &lt;- lm(life_expectancy ~ infant_mortality, data = africa2000)\nfit2 &lt;- lm(life_expectancy ~ population, data = africa2000)\nAnd finally I used the summary() function to look at the fit results for both models.\nsummary(fit1)\n\n\nCall:\nlm(formula = life_expectancy ~ infant_mortality, data = africa2000)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-22.6651  -3.7087   0.9914   4.0408   8.6817 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)      71.29331    2.42611  29.386  &lt; 2e-16 ***\ninfant_mortality -0.18916    0.02869  -6.594 2.83e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.221 on 49 degrees of freedom\nMultiple R-squared:  0.4701,    Adjusted R-squared:  0.4593 \nF-statistic: 43.48 on 1 and 49 DF,  p-value: 2.826e-08\n\nsummary(fit2)\n\n\nCall:\nlm(formula = life_expectancy ~ population, data = africa2000)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-18.429  -4.602  -2.568   3.800  18.802 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.593e+01  1.468e+00  38.097   &lt;2e-16 ***\npopulation  2.756e-08  5.459e-08   0.505    0.616    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.524 on 49 degrees of freedom\nMultiple R-squared:  0.005176,  Adjusted R-squared:  -0.01513 \nF-statistic: 0.2549 on 1 and 49 DF,  p-value: 0.6159\nBased on the fit1 model, it is observed that infant mortality is statistically associated to life expectancy (p&lt;0.001). It seems that life expectancy decreases as infant mortality increases. However, it appears there is no association between life expectancy and population size, according to the fit2 model, given p= 0.616, so there could be a different model that explains this association."
  },
  {
    "objectID": "coding-exercise/coding-exercise.html#this-section-is-contributed-by-kelly-cao",
    "href": "coding-exercise/coding-exercise.html#this-section-is-contributed-by-kelly-cao",
    "title": "R Coding Exercise",
    "section": "This section is contributed by Kelly Cao",
    "text": "This section is contributed by Kelly Cao\n\nLoading and Checking Data\nThe following section shows that the data set us_contagious_diseases has 6 potential variables to analyze: disease (name), state, year, weeks_reporting, count (number of cases), and population. The 6 variables are reported for the following contagious diseases: Hepatitis A, Measles, Mumps, Pertussis, Polio, Rubella, and Smallpox for the US states.\n\n#Installing and loading needed packages with the command install.packages() and the library() function\nlibrary(\"dslabs\")\nlibrary(renv)\nlibrary(tidyverse)\n\n#The help() function is used to look at data provided in 'us_contagious_diseases'\nhelp(us_contagious_diseases)\n\n#Check the summary, class, and structure of the data set 'us_contagious_diseases'\nstr(us_contagious_diseases)\n\n'data.frame':   16065 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ year           : num  1966 1967 1968 1969 1970 ...\n $ weeks_reporting: num  50 49 52 49 51 51 45 45 45 46 ...\n $ count          : num  321 291 314 380 413 378 342 467 244 286 ...\n $ population     : num  3345787 3364130 3386068 3412450 3444165 ...\n\nsummary(us_contagious_diseases)\n\n        disease            state            year      weeks_reporting\n Hepatitis A:2346   Alabama   :  315   Min.   :1928   Min.   : 0.00  \n Measles    :3825   Alaska    :  315   1st Qu.:1950   1st Qu.:31.00  \n Mumps      :1785   Arizona   :  315   Median :1975   Median :46.00  \n Pertussis  :2856   Arkansas  :  315   Mean   :1971   Mean   :37.38  \n Polio      :2091   California:  315   3rd Qu.:1990   3rd Qu.:50.00  \n Rubella    :1887   Colorado  :  315   Max.   :2011   Max.   :52.00  \n Smallpox   :1275   (Other)   :14175                                 \n     count          population      \n Min.   :     0   Min.   :   86853  \n 1st Qu.:     7   1st Qu.: 1018755  \n Median :    69   Median : 2749249  \n Mean   :  1492   Mean   : 4107584  \n 3rd Qu.:   525   3rd Qu.: 4996229  \n Max.   :132342   Max.   :37607525  \n                  NA's   :214       \n\nclass(us_contagious_diseases)\n\n[1] \"data.frame\"\n\n\n\n\nProcessing and Cleaning Data\nI chose to use the count, weeks_reporting, and disease columns to process and clean. After processing, the disease_count object was created to represent the columns of count and disease. No further filtering was performed on that object. Another object, Weeks_Count, is used to represent count and weeks_reporting. In addition to the select() function used to select the two columns, the filter() function was used to remove any rows that had weeks_reporting= 0, as that suggest no reporting was done for those selected data points. After that was excluded, 288 observations were left in the Weeks_Count object.\n\n#An object is created that includes only data from the state of Georgia using the pipe operator and the filter() function.\nGeorgia_data &lt;-\n  us_contagious_diseases %&gt;%\n  filter(state == 'Georgia')\n\n#The str() and summary() function is used to verify that the object was made correctly\nstr(Georgia_data)\n\n'data.frame':   315 obs. of  6 variables:\n $ disease        : Factor w/ 7 levels \"Hepatitis A\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ state          : Factor w/ 51 levels \"Alabama\",\"Alaska\",..: 11 11 11 11 11 11 11 11 11 11 ...\n $ year           : num  1966 1967 1968 1969 1970 ...\n $ weeks_reporting: num  47 51 50 48 51 50 47 50 48 48 ...\n $ count          : num  509 922 990 750 763 ...\n $ population     : num  4306523 4373252 4442463 4514462 4589575 ...\n\nsummary(Georgia_data)\n\n        disease          state          year      weeks_reporting\n Hepatitis A:46   Georgia   :315   Min.   :1928   Min.   : 0.00  \n Measles    :75   Alabama   :  0   1st Qu.:1950   1st Qu.:33.00  \n Mumps      :35   Alaska    :  0   Median :1975   Median :45.00  \n Pertussis  :56   Arizona   :  0   Mean   :1971   Mean   :37.66  \n Polio      :41   Arkansas  :  0   3rd Qu.:1990   3rd Qu.:49.00  \n Rubella    :37   California:  0   Max.   :2011   Max.   :52.00  \n Smallpox   :25   (Other)   :  0                                 \n     count           population     \n Min.   :    0.0   Min.   :2901933  \n 1st Qu.:    8.5   1st Qu.:3444578  \n Median :   42.0   Median :5009127  \n Mean   :  643.0   Mean   :5235135  \n 3rd Qu.:  352.0   3rd Qu.:6478216  \n Max.   :22965.0   Max.   :9830160  \n                                    \n\n#Another object is created with 'weeks_reporting' and 'count'. This will group the total number of reported cases with the number of weeks counts were reported for that year. The rows that had  0 in \"weeks_reporting\" were excluded with the filter() function.\nWeeks_Count &lt;- \n  select(Georgia_data, weeks_reporting, count) %&gt;%\n  filter(weeks_reporting !=0)\n\n#The str() and summary() function is used to verify that the object was made correctly\nstr(Weeks_Count)\n\n'data.frame':   288 obs. of  2 variables:\n $ weeks_reporting: num  47 51 50 48 51 50 47 50 48 48 ...\n $ count          : num  509 922 990 750 763 ...\n\nsummary(Weeks_Count)\n\n weeks_reporting     count        \n Min.   : 1.00   Min.   :    0.0  \n 1st Qu.:37.00   1st Qu.:   16.0  \n Median :46.00   Median :   55.5  \n Mean   :41.19   Mean   :  703.2  \n 3rd Qu.:50.00   3rd Qu.:  468.8  \n Max.   :52.00   Max.   :22965.0  \n\n#Another object is created with 'disease' and 'count', which would group the disease type with the total number of reported cases in Georgia into one object. \ndisease_count&lt;- select(Georgia_data, count, disease)\n\n#The str() and summary() function is used to verify that the object was made correctly\nstr(disease_count)\n\n'data.frame':   315 obs. of  2 variables:\n $ count  : num  509 922 990 750 763 ...\n $ disease: Factor w/ 7 levels \"Hepatitis A\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\nsummary(disease_count)\n\n     count                disease  \n Min.   :    0.0   Hepatitis A:46  \n 1st Qu.:    8.5   Measles    :75  \n Median :   42.0   Mumps      :35  \n Mean   :  643.0   Pertussis  :56  \n 3rd Qu.:  352.0   Polio      :41  \n Max.   :22965.0   Rubella    :37  \n                   Smallpox   :25  \n\n\n\n\nPlots\nI plotted the Weeks_Count object as a scatter plot, with the total number of reported cases being plotted as a function of the number of weeks reported in the state of Georgia. When fully plotted with a line of best fit, visually, there appears to be a positive correlation between the number of reported case and the number of weeks that the counts were reported, which makes logical sense. The total number of cases reported will increase when the time that is used to collect the reported response increase.\n\n#Using the package ggplot2 (which is found in tidyverse) to create a scatter plot using the variables in the object 'Weeks_Count' and assigning it to the variable 'p1\n\np1 &lt;- ggplot(Weeks_Count, aes(x = weeks_reporting, y = log(count))) +geom_point()+ geom_smooth(method = \"lm\")+\n      ggtitle('Number of Weeks Reporting vs the Number of Reported Case')+\n      theme(plot.title= element_text(hjust=0.5))+\n      xlab('Number of Weeks Reporting')+\n      ylab('Total Number of Reported Cases (log-scaled)')+\n      labs(caption = \"Figure 1: The total number of reported cases plotted as a function of the number of weeks reported in the state of Georgia\")+\n      theme(plot.caption = element_text(hjust=0, face=\"bold\"))\n\n#The theme() function was a great way to manipulate the specific element in the plot, whether it was color, size, or orientation of the text or visuals.\n\n#Using the plot() function to determine if the plot was properly created\nplot(p1)\n\n\n\n\n\n\n\n\nSimilar to P1, I initially plotted disease_count as a bar graph to visualize the number of reported cases per disease type. This is shown in p2. Upon initial observation of the graph, it is clear that compared to the other disease listed, Measles easily acted as an outlier in the data set with the greatest number of reported case by far. This is followed by Pertussis and Hepatitis A.\n\n#Repeating the previous two steps to plot 'disease_count' in 'p2'\np2 &lt;- ggplot(disease_count, aes(x = disease, y = count)) + geom_bar(stat = \"identity\", position = \"dodge\", fill = \"skyblue\")+\n      ggtitle('The Type of Disease vs The Number of Reported Case')+\n      theme(plot.title= element_text(hjust=0.5))+\n      xlab('Disease Type')+\n      ylab('Total Number of Reported Cases')+\n      labs(caption = \"Figure 2: The total number of reported cases plotted as a function of the disease type reported in the state of Georgia\")+\n      theme(plot.caption = element_text(hjust=0, face=\"bold\"))\n\n#print the plot\nplot(p2)\n\n\n\n\n\n\n\n\nAs I was unable to discern any valuable information for the disease Smallpox or Rubella due to the scaling, I replotted the graph as a violin plot overlayed with plot points to better visualize the spread of the data over the different disease while accounting for the outlier that caused the scaling issue. I plotted this as p2_2.The violin plot showed the data distribution and the density of each variable. The individual data points overlaps it for more visual clarity.\n\n#When looking at the previous plot, there is a distinct outlier for the Measles category, thus making it difficult to determine the value that's provided from the other disease. To combat that, another plot was created to improve the clarity of the visualization of the same data set.\np2_2 &lt;- ggplot(disease_count, aes(x = disease, y = log(count))) + \n      geom_violin(fill = \"lightblue\", color = \"blue\", alpha = 0.7) +\n      geom_jitter(width = 0.2, color = \"red\", alpha = 0.7) + \n      ggtitle('The Type of Disease vs The Number of Reported Case')+\n      theme(plot.title= element_text(hjust=0.5))+\n      xlab('Disease Type')+\n      ylab('Total Number of Reported Cases (log-scaled)')+\n      labs(caption = \"Figure 3: The total number of reported cases plotted as a function of the disease type reported in the state of Georgia\")+\n      theme(plot.caption = element_text(hjust=0, face=\"bold\"))\n\n#print the plot\nplot(p2_2)\n\n\n\n\n\n\n\n\n\nAdditional plot for fun\nLooking at Figure 4, one can see that aside from Polio all of the other diseases listed show a upward positive trend. This suggests that Polio have little change case reported despite the time elapse, which aligns with historical and public vaccination efforts. Looking at Measles, it showed the greatest positive slope, showing the greatest growth in number of case over some set of time, which corroborates what is shown in Figure 2.\n\n#Replotting p1 with clearly assigned data point to disease variable. Compared to p1, I added color and a legend to the points to distinguish which data point represent which disease type. \np4 &lt;- ggplot(Georgia_data, aes(x = weeks_reporting, y = log(count), color = disease, label = disease)) +geom_point()+ geom_smooth(method = \"lm\")+\n      ggtitle('Number of Weeks Reporting vs the Number of Reported Case')+\n      theme(plot.title= element_text(hjust=0.5))+\n      xlab('Number of Weeks Reporting')+\n      ylab('Total Number of Reported Cases (log-scaled)')+\n      labs(caption = \"Figure 4: The total number of reported cases plotted as a function of the number of weeks reported in the state of Georgia\")+\n      theme(plot.caption = element_text(hjust=0, face=\"bold\"))+\n      labs(color = \"Disease Type\")\n\n#The theme() function was a great way to manipulate the specific element in the plot, whether it was color, size, or orientation of the text or visuals.\n\n#Using the plot() function to determine if the plot was properly created\nplot(p4)\n\n\n\n\n\n\n\n\n\n\n\nAnalysis and Conclusion\nI then fitted both objects to a linear model, with count as thte outcome for both models and weeks_reporting and disease as the predictor respectively.\nBased on the results of the linear model analysis, both models seem fairly reliable with lmfit2 having the edge.\nThe second model has a higher R-squared, suggesting that it explains a larger proportion of variance in the response variable. The F-statistic is greater in this model, indicating a better overall fit.\nThe low p-value for lmfit1 suggests that weeks_reporting may have some significance in predicting the outcome count. The same is especially true for the second model.\nThe lmfit2 model also had a lower residual standard error compared to lmfit1, further suggesting that the second model’s prediction are closer to the actual value on average.\nBased on the following observation, lmfit2 appears to perform as the better model, with a more significant predictor, a higher proportion of explained variance, a higher F-statistic, and a lower residual standard error.\n\n#Fitting the data set to a linear model with 'count' as the outcome and 'weekly_reporting' as the predictor\nlmfit1 &lt;- lm(count ~ weeks_reporting, Weeks_Count)\n\n#Use the summary() command to check over the fit \nsummary(lmfit1)\n\n\nCall:\nlm(formula = count ~ weeks_reporting, data = Weeks_Count)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n -910.6  -761.0  -519.6   -99.6 22533.8 \n\nCoefficients:\n                Estimate Std. Error t value Pr(&gt;|t|)  \n(Intercept)      -86.531    421.657  -0.205   0.8375  \nweeks_reporting   19.175      9.821   1.952   0.0519 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 2020 on 286 degrees of freedom\nMultiple R-squared:  0.01315,   Adjusted R-squared:  0.009702 \nF-statistic: 3.812 on 1 and 286 DF,  p-value: 0.05187\n\n#Fitting the data set to a linear model with 'count' as the outcome and 'disease' as the predictor\nlmfit2 &lt;- lm(count ~ disease, disease_count)\n\n#Use the summary() command to check over the fit \nsummary(lmfit2)\n\n\nCall:\nlm(formula = count ~ disease, data = disease_count)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2073.1  -306.5   -21.2    31.7 20891.9 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)        431.13     263.86   1.634    0.103    \ndiseaseMeasles    1642.02     335.15   4.899 1.56e-06 ***\ndiseaseMumps      -401.93     401.41  -1.001    0.317    \ndiseasePertussis   -85.09     356.11  -0.239    0.811    \ndiseasePolio      -282.76     384.37  -0.736    0.462    \ndiseaseRubella    -421.78     395.20  -1.067    0.287    \ndiseaseSmallpox   -415.81     444.67  -0.935    0.350    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1790 on 308 degrees of freedom\nMultiple R-squared:  0.1744,    Adjusted R-squared:  0.1583 \nF-statistic: 10.84 on 6 and 308 DF,  p-value: 5.986e-11"
  },
  {
    "objectID": "aboutme.html",
    "href": "aboutme.html",
    "title": "About me",
    "section": "",
    "text": "When I presented my Master’s thesis in a poster format"
  },
  {
    "objectID": "aboutme.html#background",
    "href": "aboutme.html#background",
    "title": "About me",
    "section": "Background",
    "text": "Background\nMy name is Erick Mollinedo, I was born and raised in Guatemala and I lived in here until I moved to the US in 2021 to become a graduate student. I am currently a PhD student from the Interdisciplinary Toxicology Program, where my research area is on Air pollution and Exposure assessment."
  },
  {
    "objectID": "aboutme.html#education",
    "href": "aboutme.html#education",
    "title": "About me",
    "section": "Education",
    "text": "Education\nCurrent:\n\nPhD in Toxicology, Environmental Health Science department at the University of Georgia\n\nPrevious:\n\nMS in Environmental Health Science at the University of Georgia\nBS in Biology at the Universidad del Valle de Guatemala, Guatemala"
  },
  {
    "objectID": "aboutme.html#professional-experience",
    "href": "aboutme.html#professional-experience",
    "title": "About me",
    "section": "Professional Experience",
    "text": "Professional Experience\nI have worked as a Graduate research assistant at UGA at the Naeher Lab in the Environmental Health Science department for 3 years. I also worked as a laboratory assistant at the Emerging Infectious Diseases Molecular biology laboratory at the Universidad del Valle de Guatemala, working in different projects about surveillance of respiratory, febrile and diarrhea diseases in the coast and highlands of Guatemala. These projects have been in collaboration with the CDC offices in Central America. I also worked in Guatemala as a field project manager for the Household Air Pollution Intervention Network Trial (HAPIN), one of the largest liquefied petrolleum gas (LPG) trials about the effects of household air pollution in low- and middle- income populations. You can find more about the HAPIN trial in this site. In general, this project assessed the health effects on pregnant women in rural communities from four different countries, by changing the source of cooking from a biomass fuel stove to an LPG stove."
  },
  {
    "objectID": "aboutme.html#data-analysis-experience",
    "href": "aboutme.html#data-analysis-experience",
    "title": "About me",
    "section": "Data Analysis Experience",
    "text": "Data Analysis Experience\nI have some knowledge about SAS statistical program and R and R Studio programming for statistical analysis. I recently took the EPID7500 class where I learned more and gained practice about R programming. This course, alongside other biostatistics courses such as BIOS 7020 and BIOS 8150, have helped me be more efficient in performing statistical analysis. Before that, I learned R using online tools such as textbooks, stack overflow and video tutorials, which I put on practice to work on my Master’s thesis, derived from my work at the HAPIN trial. I have done data cleaning, mostly using excel, with the huge amount of datapoints obtained from HAPIN (more than 5000 records and over 2000 variables each), but with the recent skills from the EPID7500 course, I expect to practice data cleanign using R."
  },
  {
    "objectID": "aboutme.html#fun-fact",
    "href": "aboutme.html#fun-fact",
    "title": "About me",
    "section": "Fun Fact",
    "text": "Fun Fact\nI have met and talked to two Nobel prize winners: Ada Yonath, as a Chemistry nobel prize winner for mapping the structure of the ribosomes and Rigoberta Menchu: Peace nobel prize winner for her continuous work to promote the right of indigenous people in Guatemala after the civil war. (Attaching pics that prove it haha)\n\n\n\n2016\n2018"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html",
    "href": "cdcdata-exercise/cdcdata-exercise.html",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "These are the packages used for this exercise:\n\nlibrary(here)\n\nhere() starts at C:/Users/molli/OneDrive/Documentos/UGA/Spring 2024/MADA/erickmollinedo-MADA-portfolio\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThe following dataset is about the provisional cases of Salmonellosis for the year 2019 in the United States regions, the US territories and non-US residents. Salmonellosis is part of the national notifiable diseases reported from the National Notifiable Diseases Surveillance System (NNDSS). The cases are reported by the state health departments to the Centers for Disease Control and Prevention (CDC) on a weekly basis. This dataset was obtained from the CDC data website https://data.cdc.gov/, and the original dataset was downloaded from this link.\n\n\nThe following code chunk details about loading the dataset into the salmonella object.\n\n#Load the dataset into the `salmonella` object\nsalmonella &lt;- read_csv(here(\"cdcdata-exercise\", \"data\", \"Salmonella_CDC_2019.csv\"))\n\nRows: 1470 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (15): Reporting Area, Salmonella Paratyphi infection§, Current week, fla...\ndbl  (8): MMWR Year, MMWR Week, Salmonella Paratyphi infection§, Current wee...\nlgl  (6): Salmonella Paratyphi infection§, Previous 52 weeks Max†, Salmonell...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Explore the dimensions of the dataset\nnrow(salmonella)\n\n[1] 1470\n\nncol(salmonella)\n\n[1] 29\n\n\nThis dataset has 1470 observations and 29 variables, among those variables I am only interested in some, since some are repetitive or have a lot of non-reported (blank or missing values). So in this part of the code I am deleting most of the variables, so I only have the location area, the week, and the weekly reported cases of Salmonella typhi, Salmonella paratyphi and Salmonellosis (which represents cases of Salmonella other than S. paratyphi and S. typhi).\n\n#Using the `select()` function to choose only 5 variables\nsalmonella &lt;- salmonella %&gt;% select(c(`Reporting Area`, `MMWR Week`, `Salmonella Paratyphi infection§, Current week`, `Salmonella Typhi infection¶, Current week`, `Salmonellosis (excluding Salmonella Paratyphi infection and Salmonella Typhoid infection)**, Current week`))\n\nHere I am changing the name of the variables, so they are easier to read, then I am changing the NA values to 0, so they are not inputed as NAs.\n\n#First renaming all the columns using the `rename()` function\nsalmonella &lt;- salmonella %&gt;% rename(Region = `Reporting Area`,\n                                    Week = `MMWR Week`,\n                                    `S. paratyphi` = `Salmonella Paratyphi infection§, Current week`,\n                                    `S. typhi` = `Salmonella Typhi infection¶, Current week`,\n                                    `Other Salmonella` = `Salmonellosis (excluding Salmonella Paratyphi infection and Salmonella Typhoid infection)**, Current week`)\n\n#Then changing all `NAs` to `0`\nna_index &lt;- is.na(salmonella)\nsalmonella[na_index] &lt;- 0\n\nFinally, I decided to keep only the records that belong to any of the 9 Census Bureau designated regions. This means removing all data from the individual 50 US states and 6 US territories. But first I wanted to explore if there are any typos in some of the locations.\n\n#First exploring how many unique values are from the `location` variable using `unique()`\nunique(salmonella$Region) %&gt;% sort(decreasing = F)\n\n [1] \"ALABAMA\"                  \"ALASKA\"                  \n [3] \"AMERICAN SAMOA\"           \"ARIZONA\"                 \n [5] \"ARKANSAS\"                 \"CALIFORNIA\"              \n [7] \"COLORADO\"                 \"CONNECTICUT\"             \n [9] \"DELAWARE\"                 \"DISTRICT OF COLUMBIA\"    \n[11] \"EAST NORTH CENTRAL\"       \"EAST SOUTH CENTRAL\"      \n[13] \"FLORIDA\"                  \"GEORGIA\"                 \n[15] \"GUAM\"                     \"HAWAII\"                  \n[17] \"IDAHO\"                    \"ILLINOIS\"                \n[19] \"INDIANA\"                  \"IOWA\"                    \n[21] \"KANSAS\"                   \"KENTUCKY\"                \n[23] \"LOUISIANA\"                \"MAINE\"                   \n[25] \"MARYLAND\"                 \"MASSACHUSETTS\"           \n[27] \"MICHIGAN\"                 \"MIDDDLE ATLANTIC\"        \n[29] \"MIDDLE ATLANTIC\"          \"MINNESOTA\"               \n[31] \"MISSISSIPPI\"              \"MISSOURI\"                \n[33] \"MONTANA\"                  \"MOUNTAIN\"                \n[35] \"NEBRASKA\"                 \"NEVADA\"                  \n[37] \"NEW ENGLAND\"              \"NEW HAMPSHIRE\"           \n[39] \"NEW JERSEY\"               \"NEW MEXICO\"              \n[41] \"NEW YORK\"                 \"NEW YORK CITY\"           \n[43] \"NON-US RESIDENTS\"         \"NORTH CAROLINA\"          \n[45] \"NORTH DAKOTA\"             \"NORTHERN MARIANA ISLANDS\"\n[47] \"OHIO\"                     \"OKLAHOMA\"                \n[49] \"OREGON\"                   \"PACIFIC\"                 \n[51] \"PENNSYLVANIA\"             \"PUERTO RICO\"             \n[53] \"RHODE ISLAND\"             \"SOUTH ATLANTIC\"          \n[55] \"SOUTH CAROLINA\"           \"SOUTH DAKOTA\"            \n[57] \"TENNESSEE\"                \"TEXAS\"                   \n[59] \"TOTAL\"                    \"U.S. VIRGIN ISLANDS\"     \n[61] \"US RESIDENTS\"             \"US TERRITORIES\"          \n[63] \"UTAH\"                     \"VERMONT\"                 \n[65] \"VIRGINIA\"                 \"WASHINGTON\"              \n[67] \"WEST NORTH CENTRAL\"       \"WEST SOUTH CENTRAL\"      \n[69] \"WEST VIRGINIA\"            \"WISCONSIN\"               \n[71] \"WYOMING\"                 \n\n\nAs seen above there are two middle atlantic variables: MIDDLE ATLANTIC and MIDDDLE ATLANTIC so I corrected the later one.\n\n#Rename all `MIDDDLE ATLANTIC` observations to `MIDDLE ATLANTIC` using `mutate()` and `recode()`\nsalmonella &lt;- salmonella %&gt;% mutate(Region = recode(Region, `MIDDDLE ATLANTIC` = \"MIDDLE ATLANTIC\"))\n\n#Check again if the operation worked out using the `unique()` function\nunique(salmonella$Region) %&gt;% sort(decreasing = F)\n\n [1] \"ALABAMA\"                  \"ALASKA\"                  \n [3] \"AMERICAN SAMOA\"           \"ARIZONA\"                 \n [5] \"ARKANSAS\"                 \"CALIFORNIA\"              \n [7] \"COLORADO\"                 \"CONNECTICUT\"             \n [9] \"DELAWARE\"                 \"DISTRICT OF COLUMBIA\"    \n[11] \"EAST NORTH CENTRAL\"       \"EAST SOUTH CENTRAL\"      \n[13] \"FLORIDA\"                  \"GEORGIA\"                 \n[15] \"GUAM\"                     \"HAWAII\"                  \n[17] \"IDAHO\"                    \"ILLINOIS\"                \n[19] \"INDIANA\"                  \"IOWA\"                    \n[21] \"KANSAS\"                   \"KENTUCKY\"                \n[23] \"LOUISIANA\"                \"MAINE\"                   \n[25] \"MARYLAND\"                 \"MASSACHUSETTS\"           \n[27] \"MICHIGAN\"                 \"MIDDLE ATLANTIC\"         \n[29] \"MINNESOTA\"                \"MISSISSIPPI\"             \n[31] \"MISSOURI\"                 \"MONTANA\"                 \n[33] \"MOUNTAIN\"                 \"NEBRASKA\"                \n[35] \"NEVADA\"                   \"NEW ENGLAND\"             \n[37] \"NEW HAMPSHIRE\"            \"NEW JERSEY\"              \n[39] \"NEW MEXICO\"               \"NEW YORK\"                \n[41] \"NEW YORK CITY\"            \"NON-US RESIDENTS\"        \n[43] \"NORTH CAROLINA\"           \"NORTH DAKOTA\"            \n[45] \"NORTHERN MARIANA ISLANDS\" \"OHIO\"                    \n[47] \"OKLAHOMA\"                 \"OREGON\"                  \n[49] \"PACIFIC\"                  \"PENNSYLVANIA\"            \n[51] \"PUERTO RICO\"              \"RHODE ISLAND\"            \n[53] \"SOUTH ATLANTIC\"           \"SOUTH CAROLINA\"          \n[55] \"SOUTH DAKOTA\"             \"TENNESSEE\"               \n[57] \"TEXAS\"                    \"TOTAL\"                   \n[59] \"U.S. VIRGIN ISLANDS\"      \"US RESIDENTS\"            \n[61] \"US TERRITORIES\"           \"UTAH\"                    \n[63] \"VERMONT\"                  \"VIRGINIA\"                \n[65] \"WASHINGTON\"               \"WEST NORTH CENTRAL\"      \n[67] \"WEST SOUTH CENTRAL\"       \"WEST VIRGINIA\"           \n[69] \"WISCONSIN\"                \"WYOMING\"                 \n\n\nThe operation worked, so now I will filter only the 9 US regions. Then, checking again if the operation worked.\n\n#Using `filter()` to keep only the 9 Census Bureau designated regions\nsalmonella &lt;- filter(salmonella, Region %in% c(\"NEW ENGLAND\", \"MIDDLE ATLANTIC\", \"EAST NORTH CENTRAL\", \"WEST NORTH CENTRAL\",\n                                                 \"SOUTH ATLANTIC\", \"EAST SOUTH CENTRAL\", \"WEST SOUTH CENTRAL\", \"MOUNTAIN\", \"PACIFIC\"))\n\n#Check again if the operation worked out using the `unique()` function\nunique(salmonella$Region) %&gt;% sort(decreasing = F)\n\n[1] \"EAST NORTH CENTRAL\" \"EAST SOUTH CENTRAL\" \"MIDDLE ATLANTIC\"   \n[4] \"MOUNTAIN\"           \"NEW ENGLAND\"        \"PACIFIC\"           \n[7] \"SOUTH ATLANTIC\"     \"WEST NORTH CENTRAL\" \"WEST SOUTH CENTRAL\"\n\n\n\n\n\nFirst, I created a dataframe salmonella_summary that summarizes the number of infections of each type of Salmonella by each region.\n\n#First I grouped the observations using `group_by()`, and then used `summarize()` with `sum()` to create the summary of infections for each type of Salmonella by each region\nsalmonella_summary &lt;- salmonella %&gt;% group_by(Region) %&gt;% \n summarize(`S. paratyphi` = sum(`S. paratyphi`),\n            `S. typhi` = sum(`S. typhi`),\n            `Other Salmonella` = sum(`Other Salmonella`))\n\n#View the dataframe\nsalmonella_summary\n\n# A tibble: 9 × 4\n  Region             `S. paratyphi` `S. typhi` `Other Salmonella`\n  &lt;chr&gt;                       &lt;dbl&gt;      &lt;dbl&gt;              &lt;dbl&gt;\n1 EAST NORTH CENTRAL              0          1                504\n2 EAST SOUTH CENTRAL              0          0                117\n3 MIDDLE ATLANTIC                 1          8                322\n4 MOUNTAIN                        1          2                283\n5 NEW ENGLAND                     1          0                 50\n6 PACIFIC                         0          0                 51\n7 SOUTH ATLANTIC                  4         15                331\n8 WEST NORTH CENTRAL              0          1                224\n9 WEST SOUTH CENTRAL              5          1                242\n\n\nTo create a table that shows the frequency of cases by region and their percentages, I decided to transpose the data frame, creating the salmonella_summary_transp object.\n\n#Transpose data using the `data.frame()` function to create the data frame, then using `t()` to transpose column by rows\nsalmonella_summary_transp &lt;- data.frame(cbind(names(salmonella_summary), t(salmonella_summary)))\n\n#Since this function didn't properly named the columns, I manually set them using the `colnames()` function\ncolnames(salmonella_summary_transp) &lt;- c(\"Bacteria\",\n                                         \"East North Central\", \n                                         \"East South Central\",\n                                         \"Middle Atlantic\",\n                                         \"Mountain\",\n                                         \"New England\",\n                                         \"Pacific\",\n                                         \"South Atlantic\",\n                                         \"West North Central\",\n                                         \"West South Central\")\n\n#Here I also specified that the rows shouldn't be named, using the `rownames()` and then set to NULL\nrownames(salmonella_summary_transp) &lt;- NULL\n\n#I also deleted the first row of the new data frame, since it contained the name of the columns, I did this using base R.\nsalmonella_summary_transp &lt;- salmonella_summary_transp[-1,]\n\n#View the data frame\nsalmonella_summary_transp\n\n          Bacteria East North Central East South Central Middle Atlantic\n2     S. paratyphi                  0                  0               1\n3         S. typhi                  1                  0               8\n4 Other Salmonella                504                117             322\n  Mountain New England Pacific South Atlantic West North Central\n2        1           1       0              4                  0\n3        2           0       0             15                  1\n4      283          50      51            331                224\n  West South Central\n2                  5\n3                  1\n4                242\n\n\nAs seen above, there is the problem that all the columns are character type variable, so I changed them to numeric in the following code chunk.\n\n#Use the `mutate_at()` function and then the as.numeric statement to change all the variables, except the first one to numeric type\nsalmonella_summary_transp &lt;- salmonella_summary_transp %&gt;% mutate_at(c(\"East North Central\", \n                                         \"East South Central\",\n                                         \"Middle Atlantic\",\n                                         \"Mountain\",\n                                         \"New England\",\n                                         \"Pacific\",\n                                         \"South Atlantic\",\n                                         \"West North Central\",\n                                         \"West South Central\"), as.numeric)\n\n#Using `str()` to check if the dataframe was changed\nstr(salmonella_summary_transp)\n\n'data.frame':   3 obs. of  10 variables:\n $ Bacteria          : chr  \"S. paratyphi\" \"S. typhi\" \"Other Salmonella\"\n $ East North Central: num  0 1 504\n $ East South Central: num  0 0 117\n $ Middle Atlantic   : num  1 8 322\n $ Mountain          : num  1 2 283\n $ New England       : num  1 0 50\n $ Pacific           : num  0 0 51\n $ South Atlantic    : num  4 15 331\n $ West North Central: num  0 1 224\n $ West South Central: num  5 1 242\n\n\nFinally, I created a table that summarizes the frequency and percentage of cases by each type of bacteria and by region under the salmonella_freq object.\n\nsalmonella_freq &lt;- data.frame(salmonella_summary_transp %&gt;% \n  group_by(Bacteria) %&gt;% #Grouping by type of bacteria\n  summarize(`East North Central` = paste0(sum(`East North Central`), \"(\", #To sum all cases of salmonella from this region\n                                          round(sum(`East North Central`)/sum(salmonella_summary_transp$`East North Central`) *100,2), #To also estimate the percentage of cases for this region (The following lines of code repeat the two steps shown here)\n                                          \"%)\"),\n            `East South Central` = paste0(sum(`East South Central`), \"(\",\n                                          round(sum(`East South Central`)/sum(salmonella_summary_transp$`East South Central`) *100,2),\n                                          \"%)\"),\n            `Middle Atlantic` = paste0(sum(`Middle Atlantic`), \"(\",\n                                          round(sum(`Middle Atlantic`)/sum(salmonella_summary_transp$`Middle Atlantic`) *100,2),\n                                          \"%)\"),\n            `Mountain` = paste0(sum(`Mountain`), \"(\",\n                                          round(sum(`Mountain`)/sum(salmonella_summary_transp$`Mountain`) *100,2),\n                                          \"%)\"),\n            `New England` = paste0(sum(`New England`), \"(\",\n                                          round(sum(`New England`)/sum(salmonella_summary_transp$`New England`) *100,2),\n                                          \"%)\"),\n            `Pacific` = paste0(sum(`Pacific`), \"(\",\n                                          round(sum(`Pacific`)/sum(salmonella_summary_transp$`Pacific`) *100,2),\n                                          \"%)\"),\n            `South Atlantic` = paste0(sum(`South Atlantic`), \"(\",\n                                          round(sum(`South Atlantic`)/sum(salmonella_summary_transp$`South Atlantic`) *100,2),\n                                          \"%)\"),\n            `West North Central` = paste0(sum(`West North Central`), \"(\",\n                                          round(sum(`West North Central`)/sum(salmonella_summary_transp$`West North Central`) *100,2),\n                                          \"%)\"),\n            `West South Central` = paste0(sum(`West South Central`), \"(\",\n                                          round(sum(`West South Central`)/sum(salmonella_summary_transp$`West South Central`) *100,2),\n                                          \"%)\")))\n\n#View the table\nsalmonella_freq\n\n          Bacteria East.North.Central East.South.Central Middle.Atlantic\n1 Other Salmonella         504(99.8%)          117(100%)     322(97.28%)\n2     S. paratyphi              0(0%)              0(0%)         1(0.3%)\n3         S. typhi            1(0.2%)              0(0%)        8(2.42%)\n     Mountain New.England  Pacific South.Atlantic West.North.Central\n1 283(98.95%)  50(98.04%) 51(100%)    331(94.57%)        224(99.56%)\n2    1(0.35%)    1(1.96%)    0(0%)       4(1.14%)              0(0%)\n3     2(0.7%)       0(0%)    0(0%)      15(4.29%)           1(0.44%)\n  West.South.Central\n1        242(97.58%)\n2           5(2.02%)\n3            1(0.4%)\n\n\nIn this table it is observed that the majority of cases of Salmonellosis in all the regions belong to the types of Salmonella other than S. typhi or S. paratyphi.\nAnd now, to have a visual representation of how the cases of each type of Salmonella look by week, I plotted the following figures. The first figure represents the number of Salmonella paratyphi cases by week and color coded by US region\n\n#Using `ggplot()` and the `geom_col()` functions to plot the cases of S. paratyphi through time\nggplot(salmonella, aes(x= Week, y= `S. paratyphi`, fill= Region))+\n  geom_col()+\n  labs(x= \"Week\", y= \"No. Cases\")+\n  scale_x_continuous(breaks = seq(1, 21, by= 1))\n\n\n\n\n\n\n\n\nThis figure represents the number of Salmonella typhi cases by week and color coded by US region\n\n#Using `ggplot()` and the `geom_col()` functions to plot the cases of S. typhi through time\nggplot(salmonella, aes(x= Week, y= `S. typhi`, fill= Region))+\n  geom_col()+\n  labs(x= \"Week\", y= \"No. Cases\")+\n  scale_x_continuous(breaks = seq(1, 21, by= 1))\n\n\n\n\n\n\n\n\nAnd finally, the next figure shows the number of cases of Other types of Salmonella (the majority of them) by week and color coded by US region.\n\n#Using `ggplot()` and the `geom_col()` functions to plot the cases of all other types of Salmonellosis through time\nggplot(salmonella, aes(x= Week, y= `Other Salmonella`, fill= Region))+\n  geom_col()+\n  labs(x= \"Week\", y= \"No. Cases\")+\n  scale_x_continuous(breaks = seq(1, 21, by= 1))+\n  scale_y_continuous(breaks = seq(0, 200, by= 20))\n\n\n\n\n\n\n\n\nThis section contributed by MUTSA NYAMURANGA"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#salmonella-paratyphi-and-salmonella-typhi-infection-to-salmonellosis-in-2019",
    "href": "cdcdata-exercise/cdcdata-exercise.html#salmonella-paratyphi-and-salmonella-typhi-infection-to-salmonellosis-in-2019",
    "title": "CDC Data Exercise",
    "section": "",
    "text": "These are the packages used for this exercise:\n\nlibrary(here)\n\nhere() starts at C:/Users/molli/OneDrive/Documentos/UGA/Spring 2024/MADA/erickmollinedo-MADA-portfolio\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThe following dataset is about the provisional cases of Salmonellosis for the year 2019 in the United States regions, the US territories and non-US residents. Salmonellosis is part of the national notifiable diseases reported from the National Notifiable Diseases Surveillance System (NNDSS). The cases are reported by the state health departments to the Centers for Disease Control and Prevention (CDC) on a weekly basis. This dataset was obtained from the CDC data website https://data.cdc.gov/, and the original dataset was downloaded from this link.\n\n\nThe following code chunk details about loading the dataset into the salmonella object.\n\n#Load the dataset into the `salmonella` object\nsalmonella &lt;- read_csv(here(\"cdcdata-exercise\", \"data\", \"Salmonella_CDC_2019.csv\"))\n\nRows: 1470 Columns: 29\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (15): Reporting Area, Salmonella Paratyphi infection§, Current week, fla...\ndbl  (8): MMWR Year, MMWR Week, Salmonella Paratyphi infection§, Current wee...\nlgl  (6): Salmonella Paratyphi infection§, Previous 52 weeks Max†, Salmonell...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#Explore the dimensions of the dataset\nnrow(salmonella)\n\n[1] 1470\n\nncol(salmonella)\n\n[1] 29\n\n\nThis dataset has 1470 observations and 29 variables, among those variables I am only interested in some, since some are repetitive or have a lot of non-reported (blank or missing values). So in this part of the code I am deleting most of the variables, so I only have the location area, the week, and the weekly reported cases of Salmonella typhi, Salmonella paratyphi and Salmonellosis (which represents cases of Salmonella other than S. paratyphi and S. typhi).\n\n#Using the `select()` function to choose only 5 variables\nsalmonella &lt;- salmonella %&gt;% select(c(`Reporting Area`, `MMWR Week`, `Salmonella Paratyphi infection§, Current week`, `Salmonella Typhi infection¶, Current week`, `Salmonellosis (excluding Salmonella Paratyphi infection and Salmonella Typhoid infection)**, Current week`))\n\nHere I am changing the name of the variables, so they are easier to read, then I am changing the NA values to 0, so they are not inputed as NAs.\n\n#First renaming all the columns using the `rename()` function\nsalmonella &lt;- salmonella %&gt;% rename(Region = `Reporting Area`,\n                                    Week = `MMWR Week`,\n                                    `S. paratyphi` = `Salmonella Paratyphi infection§, Current week`,\n                                    `S. typhi` = `Salmonella Typhi infection¶, Current week`,\n                                    `Other Salmonella` = `Salmonellosis (excluding Salmonella Paratyphi infection and Salmonella Typhoid infection)**, Current week`)\n\n#Then changing all `NAs` to `0`\nna_index &lt;- is.na(salmonella)\nsalmonella[na_index] &lt;- 0\n\nFinally, I decided to keep only the records that belong to any of the 9 Census Bureau designated regions. This means removing all data from the individual 50 US states and 6 US territories. But first I wanted to explore if there are any typos in some of the locations.\n\n#First exploring how many unique values are from the `location` variable using `unique()`\nunique(salmonella$Region) %&gt;% sort(decreasing = F)\n\n [1] \"ALABAMA\"                  \"ALASKA\"                  \n [3] \"AMERICAN SAMOA\"           \"ARIZONA\"                 \n [5] \"ARKANSAS\"                 \"CALIFORNIA\"              \n [7] \"COLORADO\"                 \"CONNECTICUT\"             \n [9] \"DELAWARE\"                 \"DISTRICT OF COLUMBIA\"    \n[11] \"EAST NORTH CENTRAL\"       \"EAST SOUTH CENTRAL\"      \n[13] \"FLORIDA\"                  \"GEORGIA\"                 \n[15] \"GUAM\"                     \"HAWAII\"                  \n[17] \"IDAHO\"                    \"ILLINOIS\"                \n[19] \"INDIANA\"                  \"IOWA\"                    \n[21] \"KANSAS\"                   \"KENTUCKY\"                \n[23] \"LOUISIANA\"                \"MAINE\"                   \n[25] \"MARYLAND\"                 \"MASSACHUSETTS\"           \n[27] \"MICHIGAN\"                 \"MIDDDLE ATLANTIC\"        \n[29] \"MIDDLE ATLANTIC\"          \"MINNESOTA\"               \n[31] \"MISSISSIPPI\"              \"MISSOURI\"                \n[33] \"MONTANA\"                  \"MOUNTAIN\"                \n[35] \"NEBRASKA\"                 \"NEVADA\"                  \n[37] \"NEW ENGLAND\"              \"NEW HAMPSHIRE\"           \n[39] \"NEW JERSEY\"               \"NEW MEXICO\"              \n[41] \"NEW YORK\"                 \"NEW YORK CITY\"           \n[43] \"NON-US RESIDENTS\"         \"NORTH CAROLINA\"          \n[45] \"NORTH DAKOTA\"             \"NORTHERN MARIANA ISLANDS\"\n[47] \"OHIO\"                     \"OKLAHOMA\"                \n[49] \"OREGON\"                   \"PACIFIC\"                 \n[51] \"PENNSYLVANIA\"             \"PUERTO RICO\"             \n[53] \"RHODE ISLAND\"             \"SOUTH ATLANTIC\"          \n[55] \"SOUTH CAROLINA\"           \"SOUTH DAKOTA\"            \n[57] \"TENNESSEE\"                \"TEXAS\"                   \n[59] \"TOTAL\"                    \"U.S. VIRGIN ISLANDS\"     \n[61] \"US RESIDENTS\"             \"US TERRITORIES\"          \n[63] \"UTAH\"                     \"VERMONT\"                 \n[65] \"VIRGINIA\"                 \"WASHINGTON\"              \n[67] \"WEST NORTH CENTRAL\"       \"WEST SOUTH CENTRAL\"      \n[69] \"WEST VIRGINIA\"            \"WISCONSIN\"               \n[71] \"WYOMING\"                 \n\n\nAs seen above there are two middle atlantic variables: MIDDLE ATLANTIC and MIDDDLE ATLANTIC so I corrected the later one.\n\n#Rename all `MIDDDLE ATLANTIC` observations to `MIDDLE ATLANTIC` using `mutate()` and `recode()`\nsalmonella &lt;- salmonella %&gt;% mutate(Region = recode(Region, `MIDDDLE ATLANTIC` = \"MIDDLE ATLANTIC\"))\n\n#Check again if the operation worked out using the `unique()` function\nunique(salmonella$Region) %&gt;% sort(decreasing = F)\n\n [1] \"ALABAMA\"                  \"ALASKA\"                  \n [3] \"AMERICAN SAMOA\"           \"ARIZONA\"                 \n [5] \"ARKANSAS\"                 \"CALIFORNIA\"              \n [7] \"COLORADO\"                 \"CONNECTICUT\"             \n [9] \"DELAWARE\"                 \"DISTRICT OF COLUMBIA\"    \n[11] \"EAST NORTH CENTRAL\"       \"EAST SOUTH CENTRAL\"      \n[13] \"FLORIDA\"                  \"GEORGIA\"                 \n[15] \"GUAM\"                     \"HAWAII\"                  \n[17] \"IDAHO\"                    \"ILLINOIS\"                \n[19] \"INDIANA\"                  \"IOWA\"                    \n[21] \"KANSAS\"                   \"KENTUCKY\"                \n[23] \"LOUISIANA\"                \"MAINE\"                   \n[25] \"MARYLAND\"                 \"MASSACHUSETTS\"           \n[27] \"MICHIGAN\"                 \"MIDDLE ATLANTIC\"         \n[29] \"MINNESOTA\"                \"MISSISSIPPI\"             \n[31] \"MISSOURI\"                 \"MONTANA\"                 \n[33] \"MOUNTAIN\"                 \"NEBRASKA\"                \n[35] \"NEVADA\"                   \"NEW ENGLAND\"             \n[37] \"NEW HAMPSHIRE\"            \"NEW JERSEY\"              \n[39] \"NEW MEXICO\"               \"NEW YORK\"                \n[41] \"NEW YORK CITY\"            \"NON-US RESIDENTS\"        \n[43] \"NORTH CAROLINA\"           \"NORTH DAKOTA\"            \n[45] \"NORTHERN MARIANA ISLANDS\" \"OHIO\"                    \n[47] \"OKLAHOMA\"                 \"OREGON\"                  \n[49] \"PACIFIC\"                  \"PENNSYLVANIA\"            \n[51] \"PUERTO RICO\"              \"RHODE ISLAND\"            \n[53] \"SOUTH ATLANTIC\"           \"SOUTH CAROLINA\"          \n[55] \"SOUTH DAKOTA\"             \"TENNESSEE\"               \n[57] \"TEXAS\"                    \"TOTAL\"                   \n[59] \"U.S. VIRGIN ISLANDS\"      \"US RESIDENTS\"            \n[61] \"US TERRITORIES\"           \"UTAH\"                    \n[63] \"VERMONT\"                  \"VIRGINIA\"                \n[65] \"WASHINGTON\"               \"WEST NORTH CENTRAL\"      \n[67] \"WEST SOUTH CENTRAL\"       \"WEST VIRGINIA\"           \n[69] \"WISCONSIN\"                \"WYOMING\"                 \n\n\nThe operation worked, so now I will filter only the 9 US regions. Then, checking again if the operation worked.\n\n#Using `filter()` to keep only the 9 Census Bureau designated regions\nsalmonella &lt;- filter(salmonella, Region %in% c(\"NEW ENGLAND\", \"MIDDLE ATLANTIC\", \"EAST NORTH CENTRAL\", \"WEST NORTH CENTRAL\",\n                                                 \"SOUTH ATLANTIC\", \"EAST SOUTH CENTRAL\", \"WEST SOUTH CENTRAL\", \"MOUNTAIN\", \"PACIFIC\"))\n\n#Check again if the operation worked out using the `unique()` function\nunique(salmonella$Region) %&gt;% sort(decreasing = F)\n\n[1] \"EAST NORTH CENTRAL\" \"EAST SOUTH CENTRAL\" \"MIDDLE ATLANTIC\"   \n[4] \"MOUNTAIN\"           \"NEW ENGLAND\"        \"PACIFIC\"           \n[7] \"SOUTH ATLANTIC\"     \"WEST NORTH CENTRAL\" \"WEST SOUTH CENTRAL\"\n\n\n\n\n\nFirst, I created a dataframe salmonella_summary that summarizes the number of infections of each type of Salmonella by each region.\n\n#First I grouped the observations using `group_by()`, and then used `summarize()` with `sum()` to create the summary of infections for each type of Salmonella by each region\nsalmonella_summary &lt;- salmonella %&gt;% group_by(Region) %&gt;% \n summarize(`S. paratyphi` = sum(`S. paratyphi`),\n            `S. typhi` = sum(`S. typhi`),\n            `Other Salmonella` = sum(`Other Salmonella`))\n\n#View the dataframe\nsalmonella_summary\n\n# A tibble: 9 × 4\n  Region             `S. paratyphi` `S. typhi` `Other Salmonella`\n  &lt;chr&gt;                       &lt;dbl&gt;      &lt;dbl&gt;              &lt;dbl&gt;\n1 EAST NORTH CENTRAL              0          1                504\n2 EAST SOUTH CENTRAL              0          0                117\n3 MIDDLE ATLANTIC                 1          8                322\n4 MOUNTAIN                        1          2                283\n5 NEW ENGLAND                     1          0                 50\n6 PACIFIC                         0          0                 51\n7 SOUTH ATLANTIC                  4         15                331\n8 WEST NORTH CENTRAL              0          1                224\n9 WEST SOUTH CENTRAL              5          1                242\n\n\nTo create a table that shows the frequency of cases by region and their percentages, I decided to transpose the data frame, creating the salmonella_summary_transp object.\n\n#Transpose data using the `data.frame()` function to create the data frame, then using `t()` to transpose column by rows\nsalmonella_summary_transp &lt;- data.frame(cbind(names(salmonella_summary), t(salmonella_summary)))\n\n#Since this function didn't properly named the columns, I manually set them using the `colnames()` function\ncolnames(salmonella_summary_transp) &lt;- c(\"Bacteria\",\n                                         \"East North Central\", \n                                         \"East South Central\",\n                                         \"Middle Atlantic\",\n                                         \"Mountain\",\n                                         \"New England\",\n                                         \"Pacific\",\n                                         \"South Atlantic\",\n                                         \"West North Central\",\n                                         \"West South Central\")\n\n#Here I also specified that the rows shouldn't be named, using the `rownames()` and then set to NULL\nrownames(salmonella_summary_transp) &lt;- NULL\n\n#I also deleted the first row of the new data frame, since it contained the name of the columns, I did this using base R.\nsalmonella_summary_transp &lt;- salmonella_summary_transp[-1,]\n\n#View the data frame\nsalmonella_summary_transp\n\n          Bacteria East North Central East South Central Middle Atlantic\n2     S. paratyphi                  0                  0               1\n3         S. typhi                  1                  0               8\n4 Other Salmonella                504                117             322\n  Mountain New England Pacific South Atlantic West North Central\n2        1           1       0              4                  0\n3        2           0       0             15                  1\n4      283          50      51            331                224\n  West South Central\n2                  5\n3                  1\n4                242\n\n\nAs seen above, there is the problem that all the columns are character type variable, so I changed them to numeric in the following code chunk.\n\n#Use the `mutate_at()` function and then the as.numeric statement to change all the variables, except the first one to numeric type\nsalmonella_summary_transp &lt;- salmonella_summary_transp %&gt;% mutate_at(c(\"East North Central\", \n                                         \"East South Central\",\n                                         \"Middle Atlantic\",\n                                         \"Mountain\",\n                                         \"New England\",\n                                         \"Pacific\",\n                                         \"South Atlantic\",\n                                         \"West North Central\",\n                                         \"West South Central\"), as.numeric)\n\n#Using `str()` to check if the dataframe was changed\nstr(salmonella_summary_transp)\n\n'data.frame':   3 obs. of  10 variables:\n $ Bacteria          : chr  \"S. paratyphi\" \"S. typhi\" \"Other Salmonella\"\n $ East North Central: num  0 1 504\n $ East South Central: num  0 0 117\n $ Middle Atlantic   : num  1 8 322\n $ Mountain          : num  1 2 283\n $ New England       : num  1 0 50\n $ Pacific           : num  0 0 51\n $ South Atlantic    : num  4 15 331\n $ West North Central: num  0 1 224\n $ West South Central: num  5 1 242\n\n\nFinally, I created a table that summarizes the frequency and percentage of cases by each type of bacteria and by region under the salmonella_freq object.\n\nsalmonella_freq &lt;- data.frame(salmonella_summary_transp %&gt;% \n  group_by(Bacteria) %&gt;% #Grouping by type of bacteria\n  summarize(`East North Central` = paste0(sum(`East North Central`), \"(\", #To sum all cases of salmonella from this region\n                                          round(sum(`East North Central`)/sum(salmonella_summary_transp$`East North Central`) *100,2), #To also estimate the percentage of cases for this region (The following lines of code repeat the two steps shown here)\n                                          \"%)\"),\n            `East South Central` = paste0(sum(`East South Central`), \"(\",\n                                          round(sum(`East South Central`)/sum(salmonella_summary_transp$`East South Central`) *100,2),\n                                          \"%)\"),\n            `Middle Atlantic` = paste0(sum(`Middle Atlantic`), \"(\",\n                                          round(sum(`Middle Atlantic`)/sum(salmonella_summary_transp$`Middle Atlantic`) *100,2),\n                                          \"%)\"),\n            `Mountain` = paste0(sum(`Mountain`), \"(\",\n                                          round(sum(`Mountain`)/sum(salmonella_summary_transp$`Mountain`) *100,2),\n                                          \"%)\"),\n            `New England` = paste0(sum(`New England`), \"(\",\n                                          round(sum(`New England`)/sum(salmonella_summary_transp$`New England`) *100,2),\n                                          \"%)\"),\n            `Pacific` = paste0(sum(`Pacific`), \"(\",\n                                          round(sum(`Pacific`)/sum(salmonella_summary_transp$`Pacific`) *100,2),\n                                          \"%)\"),\n            `South Atlantic` = paste0(sum(`South Atlantic`), \"(\",\n                                          round(sum(`South Atlantic`)/sum(salmonella_summary_transp$`South Atlantic`) *100,2),\n                                          \"%)\"),\n            `West North Central` = paste0(sum(`West North Central`), \"(\",\n                                          round(sum(`West North Central`)/sum(salmonella_summary_transp$`West North Central`) *100,2),\n                                          \"%)\"),\n            `West South Central` = paste0(sum(`West South Central`), \"(\",\n                                          round(sum(`West South Central`)/sum(salmonella_summary_transp$`West South Central`) *100,2),\n                                          \"%)\")))\n\n#View the table\nsalmonella_freq\n\n          Bacteria East.North.Central East.South.Central Middle.Atlantic\n1 Other Salmonella         504(99.8%)          117(100%)     322(97.28%)\n2     S. paratyphi              0(0%)              0(0%)         1(0.3%)\n3         S. typhi            1(0.2%)              0(0%)        8(2.42%)\n     Mountain New.England  Pacific South.Atlantic West.North.Central\n1 283(98.95%)  50(98.04%) 51(100%)    331(94.57%)        224(99.56%)\n2    1(0.35%)    1(1.96%)    0(0%)       4(1.14%)              0(0%)\n3     2(0.7%)       0(0%)    0(0%)      15(4.29%)           1(0.44%)\n  West.South.Central\n1        242(97.58%)\n2           5(2.02%)\n3            1(0.4%)\n\n\nIn this table it is observed that the majority of cases of Salmonellosis in all the regions belong to the types of Salmonella other than S. typhi or S. paratyphi.\nAnd now, to have a visual representation of how the cases of each type of Salmonella look by week, I plotted the following figures. The first figure represents the number of Salmonella paratyphi cases by week and color coded by US region\n\n#Using `ggplot()` and the `geom_col()` functions to plot the cases of S. paratyphi through time\nggplot(salmonella, aes(x= Week, y= `S. paratyphi`, fill= Region))+\n  geom_col()+\n  labs(x= \"Week\", y= \"No. Cases\")+\n  scale_x_continuous(breaks = seq(1, 21, by= 1))\n\n\n\n\n\n\n\n\nThis figure represents the number of Salmonella typhi cases by week and color coded by US region\n\n#Using `ggplot()` and the `geom_col()` functions to plot the cases of S. typhi through time\nggplot(salmonella, aes(x= Week, y= `S. typhi`, fill= Region))+\n  geom_col()+\n  labs(x= \"Week\", y= \"No. Cases\")+\n  scale_x_continuous(breaks = seq(1, 21, by= 1))\n\n\n\n\n\n\n\n\nAnd finally, the next figure shows the number of cases of Other types of Salmonella (the majority of them) by week and color coded by US region.\n\n#Using `ggplot()` and the `geom_col()` functions to plot the cases of all other types of Salmonellosis through time\nggplot(salmonella, aes(x= Week, y= `Other Salmonella`, fill= Region))+\n  geom_col()+\n  labs(x= \"Week\", y= \"No. Cases\")+\n  scale_x_continuous(breaks = seq(1, 21, by= 1))+\n  scale_y_continuous(breaks = seq(0, 200, by= 20))\n\n\n\n\n\n\n\n\nThis section contributed by MUTSA NYAMURANGA"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#creating-synthetic-replicate-data",
    "href": "cdcdata-exercise/cdcdata-exercise.html#creating-synthetic-replicate-data",
    "title": "CDC Data Exercise",
    "section": "Creating Synthetic Replicate Data",
    "text": "Creating Synthetic Replicate Data\n\n# make sure the packages are installed\n# Load required packages\nlibrary(here)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(skimr)\nlibrary(gtsummary)\n\nHere I set a seed so that my synthetic data will be reproducible to assess discrepencies with the original data.\n\nset.seed(189)\nn_observations &lt;- 189\n\n\nAnalyzing Orginal Data Set\nAlthough I have view Erick’s code and his analysis, I would like to also gain an understanding of what he looked at and how he got there. Taking a look at the data myself will help create the correct data frame for replication.\n\n#Skim the data structure to analyze observations and variable types\nskimr::skim(salmonella)\n\n\nData summary\n\n\nName\nsalmonella\n\n\nNumber of rows\n189\n\n\nNumber of columns\n5\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nRegion\n0\n1\n7\n18\n0\n9\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeek\n0\n1\n11.00\n6.07\n1\n6\n11\n16\n21\n▇▆▆▆▆\n\n\nS. paratyphi\n0\n1\n0.06\n0.41\n0\n0\n0\n0\n5\n▇▁▁▁▁\n\n\nS. typhi\n0\n1\n0.15\n0.41\n0\n0\n0\n0\n2\n▇▁▁▁▁\n\n\nOther Salmonella\n0\n1\n11.24\n11.17\n0\n1\n8\n18\n52\n▇▃▂▁▁\n\n\n\n\n#Collect distribution of variable observations\ngtsummary::tbl_summary(salmonella, statistic = list(\n  all_continuous() ~ \"{mean}/{median}/{min}/{max}/{sd}\",\n  all_categorical() ~ \"{n} / {N} ({p}%)\"\n),)\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nN = 1891\n\n\n\n\nRegion\n\n\n\n\n    EAST NORTH CENTRAL\n21 / 189 (11%)\n\n\n    EAST SOUTH CENTRAL\n21 / 189 (11%)\n\n\n    MIDDLE ATLANTIC\n21 / 189 (11%)\n\n\n    MOUNTAIN\n21 / 189 (11%)\n\n\n    NEW ENGLAND\n21 / 189 (11%)\n\n\n    PACIFIC\n21 / 189 (11%)\n\n\n    SOUTH ATLANTIC\n21 / 189 (11%)\n\n\n    WEST NORTH CENTRAL\n21 / 189 (11%)\n\n\n    WEST SOUTH CENTRAL\n21 / 189 (11%)\n\n\nWeek\n11.0/11.0/1.0/21.0/6.1\n\n\nS. paratyphi\n\n\n\n\n    0\n181 / 189 (96%)\n\n\n    1\n7 / 189 (3.7%)\n\n\n    5\n1 / 189 (0.5%)\n\n\nS. typhi\n\n\n\n\n    0\n165 / 189 (87%)\n\n\n    1\n20 / 189 (11%)\n\n\n    2\n4 / 189 (2.1%)\n\n\nOther Salmonella\n11/8/0/52/11\n\n\n\n1 n / N (%); Mean/Median/Minimum/Maximum/SD\n\n\n\n\n\n\n\n\n\n#Distributions Within each variable\ntable(salmonella$Region)\n\n\nEAST NORTH CENTRAL EAST SOUTH CENTRAL    MIDDLE ATLANTIC           MOUNTAIN \n                21                 21                 21                 21 \n       NEW ENGLAND            PACIFIC     SOUTH ATLANTIC WEST NORTH CENTRAL \n                21                 21                 21                 21 \nWEST SOUTH CENTRAL \n                21 \n\ntable(salmonella$Week)\n\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 \n 9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9  9 \n\ntable(salmonella$`S. paratyphi`)\n\n\n  0   1   5 \n181   7   1 \n\ntable(salmonella$`S. typhi`)\n\n\n  0   1   2 \n165  20   4 \n\ntable(salmonella$`Other Salmonella`)\n\n\n 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \n41  9  5  8  6  4 12  7  6  4  3  3  4 10  4  5  3  5  4  5  5  5  1  3  3  2 \n26 27 28 29 30 31 32 34 35 36 38 39 47 48 52 \n 1  3  3  1  2  1  1  1  3  1  1  1  1  1  1 \n\n\n\n\nSynthesis\nHere I create the synthetic data frame based on elements of Erick’s analysis\n\n# Create synthetic data frame similar to the original\nsyn_salmonella &lt;- data.frame(\n\n  Region = sample(c(\"NEW ENGLAND\", \"MIDDLE ATLANTIC\", \"EAST NORTH CENTRAL\", \"WEST NORTH CENTRAL\",\n                    \"SOUTH ATLANTIC\", \"EAST SOUTH CENTRAL\", \"WEST SOUTH CENTRAL\", \"MOUNTAIN\", \"PACIFIC\"),\n                  n_observations, replace = TRUE),\n  Week = sample(1:21, n_observations, replace = TRUE),\n  `S. paratyphi` = sample(c(0, 1), n_observations, replace = TRUE, prob = c(0.8, 0.2)),\n  `S. typhi` = sample(c(0, 1), n_observations, replace = TRUE, prob = c(0.9, 0.1)),  \n  `Other Salmonella` = sample(c(0, 1), n_observations, replace = TRUE, prob = c(0.7, 0.3)) \n)\n\nI take a look at the data structure to make sure everything has been created correctly.\n\nstr(syn_salmonella)\n\n'data.frame':   189 obs. of  5 variables:\n $ Region          : chr  \"MIDDLE ATLANTIC\" \"EAST NORTH CENTRAL\" \"EAST NORTH CENTRAL\" \"EAST SOUTH CENTRAL\" ...\n $ Week            : int  8 19 6 1 7 16 11 10 10 14 ...\n $ S..paratyphi    : num  1 1 1 0 0 0 0 1 1 0 ...\n $ S..typhi        : num  0 0 1 0 1 0 0 0 0 0 ...\n $ Other.Salmonella: num  0 1 0 1 1 0 1 0 0 0 ...\n\ncolnames(syn_salmonella)\n\n[1] \"Region\"           \"Week\"             \"S..paratyphi\"     \"S..typhi\"        \n[5] \"Other.Salmonella\"\n\nncol(syn_salmonella)\n\n[1] 5\n\n\n\n\nSummary Tables\nI, then, create summary tables for the data\n\n# Summary table similar to original\nsyn_salmonella_summary &lt;- syn_salmonella %&gt;%\n  group_by(Region) %&gt;%\n  summarize(`S..paratyphi` = sum(`S..paratyphi`, na.rm = TRUE),  # Add na.rm = TRUE if there are NA values\n            `S..typhi` = sum(`S..typhi`, na.rm = TRUE),\n            `Other.Salmonella` = sum(`Other.Salmonella`, na.rm = TRUE))\n\n# Transpose summary data frame\nsyn_salmonella_summary_transp &lt;- data.frame(t(syn_salmonella_summary[-1]))\ncolnames(syn_salmonella_summary_transp) &lt;- syn_salmonella_summary$Region\n\n# Change data types to numeric (excluding the first column)\nsyn_salmonella_summary_transp[, -1] &lt;- sapply(syn_salmonella_summary_transp[, -1], as.numeric)\n\n\n# Table of frequencies and percentages\nsyn_salmonella_freq &lt;- syn_salmonella_summary_transp %&gt;%\n  mutate(Total = rowSums(.)) %&gt;%\n  mutate(across(everything(), ~paste0(., \" (\", round(. / Total * 100, 2), \"%)\"), .names = \"{col}_Percent\")) %&gt;%\n  select(-Total) %&gt;%\n  rbind(c(\"Total\", colSums(syn_salmonella_summary_transp[, -1])))\n\nWarning in rbind(deparse.level, ...): number of columns of result, 19, is not a\nmultiple of vector length 9 of arg 2\n\nsyn_salmonella_summary_transp\n\n                 EAST NORTH CENTRAL EAST SOUTH CENTRAL MIDDLE ATLANTIC MOUNTAIN\nS..paratyphi                      8                  5               4        6\nS..typhi                          5                  3               0        2\nOther.Salmonella                  6                  5               5        4\n                 NEW ENGLAND PACIFIC SOUTH ATLANTIC WEST NORTH CENTRAL\nS..paratyphi               5       5              1                  7\nS..typhi                   1       2              2                  1\nOther.Salmonella           5       9              4                  5\n                 WEST SOUTH CENTRAL\nS..paratyphi                      3\nS..typhi                          2\nOther.Salmonella                  7\n\nsyn_salmonella_summary\n\n# A tibble: 9 × 4\n  Region             S..paratyphi S..typhi Other.Salmonella\n  &lt;chr&gt;                     &lt;dbl&gt;    &lt;dbl&gt;            &lt;dbl&gt;\n1 EAST NORTH CENTRAL            8        5                6\n2 EAST SOUTH CENTRAL            5        3                5\n3 MIDDLE ATLANTIC               4        0                5\n4 MOUNTAIN                      6        2                4\n5 NEW ENGLAND                   5        1                5\n6 PACIFIC                       5        2                9\n7 SOUTH ATLANTIC                1        2                4\n8 WEST NORTH CENTRAL            7        1                5\n9 WEST SOUTH CENTRAL            3        2                7\n\n\n\n\nPlotting similar to original\nFinally, I create plots similar to the plots made by Erick in his anaylsis.\n\n# Plot for S. paratyphi cases by week and region\nggplot(syn_salmonella, aes(x = Week, y = `S..paratyphi`, fill = Region)) +\n  geom_col() +\n  labs(x = \"Week\", y = \"No. Cases\") +\n  scale_x_continuous(breaks = seq(1, 21, by = 1))\n\n\n\n\n\n\n\n\n\n# Plot for S. typhi cases by week and region\nggplot(syn_salmonella, aes(x = Week, y = `S..typhi`, fill = Region)) +\n  geom_col() +\n  labs(x = \"Week\", y = \"No. Cases\") +\n  scale_x_continuous(breaks = seq(1, 21, by = 1))\n\n\n\n\n\n\n\n\n\n# Plot for Other Salmonella cases by week and region\nggplot(syn_salmonella, aes(x = Week, y = `Other.Salmonella`, fill = Region)) +\n  geom_col() +\n  labs(x = \"Week\", y = \"No. Cases\") +\n  scale_x_continuous(breaks = seq(1, 21, by = 1)) +\n  scale_y_continuous(breaks = seq(0, 200, by = 20))"
  },
  {
    "objectID": "cdcdata-exercise/cdcdata-exercise.html#data-comparison",
    "href": "cdcdata-exercise/cdcdata-exercise.html#data-comparison",
    "title": "CDC Data Exercise",
    "section": "Data Comparison",
    "text": "Data Comparison\nI believe that the data is quite similar in terms of volume, but the differences come in distribution throughout the week. The similarities that the synthetic data can replicate are not going to be on a week to week bases unless specified, but in that case, we would essentially be copy and pasting the original data."
  },
  {
    "objectID": "data-exercise/data-exercise.html",
    "href": "data-exercise/data-exercise.html",
    "title": "Data Exercise",
    "section": "",
    "text": "For this exercise I chose to create a dataset somewhat similar to the expected data I plain to obtain from a future research project I will be working on. The dataset is about a hypothetical project that assesses the health effects of exposure to air pollutants in wildland firefighters.\nThese are the packages used for this exercise:\n\nlibrary(here)\n\nhere() starts at C:/Users/molli/OneDrive/Documentos/UGA/Spring 2024/MADA/erickmollinedo-MADA-portfolio\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(gtsummary)\n\n#BlackLivesMatter\n\n\n\n\nFirst, I set a seed for reproducibility and then I defined the number of observations for this dataset\n\nset.seed(123)\n\n#Defined 60 as the number of observations (n=60 wildland firefighters)\nn_wffs &lt;- 60\n\nIn this part I created the empty data frame wff_data with thirteen variables, which includes the ID, Date, three socio-demographic variables, PM2.5 as an exposure, two potential confounders and two outcomes.\n\nwff_data &lt;- data.frame(\n  ID = numeric(n_wffs),\n  Date = as_date(character(n_wffs)),\n  Age = numeric(n_wffs),\n  Gender = character(n_wffs),\n  Ethnicity = character(n_wffs),\n  PM2.5 = numeric(n_wffs),\n  Medication = integer(n_wffs),\n  Smoking = integer(n_wffs),\n  FVC = numeric(n_wffs),\n  PAH = numeric(n_wffs)\n)\n\nAnd here I fill each variable with their respective values\n\n#Variable 1: ID\nwff_data$ID &lt;- 1:n_wffs\n\n#Variable 2: Date\nwff_data$Date &lt;- as_date(sample(seq(from = as_date(\"2024-01-08\"), to = as_date(\"2024-03-01\"), by= \"days\"), n_wffs, replace = T))\n\n#Variable 3: Age. Specifying that the ranges should be from 22 to 55 years old\nwff_data$Age &lt;- sample(22:55, 60, replace = TRUE)\n\n#Variable 4: Gender. Specifying that there should be more males than females using the `prob =` statement\nwff_data$Gender &lt;- map_chr(sample(c(\"Male\", \"Female\"), n_wffs, replace = T, prob = c(0.84, 0.16)), as.character)\n\n#Variable 5: Ethnicity. Specifying the proportions for each ethnicity.\nwff_data$Ethnicity &lt;- map_chr(sample(c(\"Caucasian\", \"African American\", \"Hispanic/Latino\", \"American Indian\"),\n                                  n_wffs, replace = T, prob = c(0.75, 0.15, 0.05, 0.05)), as.character)\n\n#Variable 6: PM2.5 (in micrograms per cubic meter). The concentrations were computed following a log-normal distribution `rlnorm()`, characteristic of PM2.5 data.\nwff_data$PM2.5 &lt;- round(rlnorm(n_wffs, meanlog = log(30), sdlog = 0.5), 2)\n\n#Variable 7: Medication (If the participants take any medication for blood pressure). 0= No medication, 1=Medication. This variable depends on Age, with higher age, the highest the probability of taking blood pressure medication, using 40 years old as a cutoff point.\nwff_data$Medication[wff_data$Age &lt;= 40 ] &lt;- map_int(sample(0:1, sum(wff_data$Age &lt;= 40) , replace = T, prob = c(0.95, 0.05)), as.integer)\nwff_data$Medication[wff_data$Age &gt; 40 ] &lt;- map_int(sample(0:1, sum(wff_data$Age &gt; 40) , replace = T, prob = c(0.6, 0.4)), as.integer)\n\n#Variable 8: Smoking. 0=Doesn't smoke, 1=Smokes. Specifying that there are more non-smokers than smokers.\nwff_data$Smoking &lt;- map_int(sample(0:1, n_wffs, replace = T, prob = c(0.85, 0.15)), as.integer)\n\n#Variable 9: Forced Vital capacity (FVC) measured by spirometry. The mean value of FVC is dependent on Gender.\nwff_data$FVC[wff_data$Gender == \"Male\"] &lt;- round(rnorm(sum(wff_data$Gender == \"Male\"), mean = 5.3, sd = 1), 1)\nwff_data$FVC[wff_data$Gender == \"Female\"] &lt;- round(rnorm(sum(wff_data$Gender == \"Female\"), mean = 3.5, sd = 0.5), 1)\n\n#Variable 10: Polycyclic Aromatic Hydrocarbons (PAHs) from urine samples.(There are multiple PAHs, but in this case I assume that 3-Hydroxybenzo(a)pyrene was measured). The PAH level depends on the PM2.5 concentration, with higher PM2.5 exposure, the higher the level of urine PAH.\nwff_data$PAH[wff_data$PM2.5 &lt;= 45] &lt;- round(rnorm(sum(wff_data$PM2.5 &lt;= 45), mean = 0.12, sd= 0.04), 3)\nwff_data$PAH[wff_data$PM2.5 &gt; 45] &lt;- round(rnorm(sum(wff_data$PM2.5 &gt; 45), mean = 0.31, sd= 0.04), 3)\n\nAnd here, checking that the data frame looks good\n\nhead(wff_data)\n\n  ID       Date Age Gender        Ethnicity PM2.5 Medication Smoking FVC   PAH\n1  1 2024-02-07  54 Female        Caucasian 14.44          0       0 3.9 0.118\n2  2 2024-01-22  48   Male        Caucasian 42.32          0       0 5.2 0.145\n3  3 2024-02-27  46   Male        Caucasian 85.73          0       0 3.5 0.270\n4  4 2024-01-21  42   Male        Caucasian 15.76          0       0 4.9 0.173\n5  5 2024-01-10  36   Male        Caucasian 44.48          0       0 5.4 0.120\n6  6 2024-02-18  47   Male African American 44.07          0       0 6.1 0.161\n\n\nI saved the data frame in a .rds file\n\nsaveRDS(wff_data, here(\"data-exercise\", \"data\", \"wff_data.Rds\"))\n\n\n\n\nFirst I explored the dataset using summary()\n\nsummary(wff_data)\n\n       ID             Date                 Age           Gender         \n Min.   : 1.00   Min.   :2024-01-10   Min.   :24.00   Length:60         \n 1st Qu.:15.75   1st Qu.:2024-01-20   1st Qu.:33.75   Class :character  \n Median :30.50   Median :2024-02-03   Median :43.00   Mode  :character  \n Mean   :30.50   Mean   :2024-02-03   Mean   :41.05                     \n 3rd Qu.:45.25   3rd Qu.:2024-02-17   3rd Qu.:48.50                     \n Max.   :60.00   Max.   :2024-03-01   Max.   :55.00                     \n  Ethnicity             PM2.5          Medication      Smoking      \n Length:60          Min.   : 14.44   Min.   :0.00   Min.   :0.0000  \n Class :character   1st Qu.: 23.50   1st Qu.:0.00   1st Qu.:0.0000  \n Mode  :character   Median : 28.88   Median :0.00   Median :0.0000  \n                    Mean   : 35.90   Mean   :0.25   Mean   :0.1333  \n                    3rd Qu.: 40.69   3rd Qu.:0.25   3rd Qu.:0.0000  \n                    Max.   :151.67   Max.   :1.00   Max.   :1.0000  \n      FVC             PAH        \n Min.   :2.800   Min.   :0.0210  \n 1st Qu.:4.075   1st Qu.:0.1027  \n Median :5.250   Median :0.1340  \n Mean   :5.073   Mean   :0.1519  \n 3rd Qu.:6.000   3rd Qu.:0.1640  \n Max.   :7.300   Max.   :0.3650  \n\n\nAnd now I created a summary table with some of the descriptive statistics of this dataset, separating by gender.\n\n#Just showing the summary statistics of age, ethnicity, PM2.5, FVC and PAHs, computing the mean and sd, median and IQR and the minimum and maximum values\nsumtable &lt;- wff_data %&gt;% select(Age, Gender, PM2.5, FVC, PAH) %&gt;% \n  tbl_summary(by= Gender, \n              type = all_continuous() ~ \"continuous2\",\n              statistic = all_continuous() ~ c(\"{mean} ({sd})\", \"{median} ({p25}, {p75})\", \"{min}, {max}\")) %&gt;% \n  bold_labels()\n\n#Print the generated table\nsumtable\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nFemale, N = 11\nMale, N = 49\n\n\n\n\nAge\n\n\n\n\n\n\n    Mean (SD)\n38 (12)\n42 (9)\n\n\n    Median (IQR)\n35 (27, 49)\n43 (36, 48)\n\n\n    Range\n24, 54\n26, 55\n\n\nPM2.5\n\n\n\n\n\n\n    Mean (SD)\n44 (41)\n34 (18)\n\n\n    Median (IQR)\n35 (18, 40)\n28 (24, 41)\n\n\n    Range\n14, 152\n16, 90\n\n\nFVC\n\n\n\n\n\n\n    Mean (SD)\n3.62 (0.49)\n5.40 (1.00)\n\n\n    Median (IQR)\n3.80 (3.30, 3.95)\n5.40 (4.90, 6.10)\n\n\n    Range\n2.80, 4.30\n3.30, 7.30\n\n\nPAH\n\n\n\n\n\n\n    Mean (SD)\n0.15 (0.06)\n0.15 (0.08)\n\n\n    Median (IQR)\n0.14 (0.12, 0.17)\n0.13 (0.10, 0.16)\n\n\n    Range\n0.07, 0.27\n0.02, 0.37\n\n\n\n\n\n\n\nFirst I want to plot a histogram to check if the PM2.5 data follow a normal distribution (it should not)\n\nggplot(wff_data, aes(x= PM2.5))+\n  geom_histogram()+\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe histogram confirms that the concentrations do not follow a normal distribution, since the data is right-skewed. This should follow a logarithmic distribution.\nHere I am showing a plot to explore the association between the level of urine PAHs and PM2.5 concentration from the wildland firefighter subjects. I decided to separate them by gender.\n\nggplot(wff_data, aes(x= PM2.5, y= PAH, color= Gender))+\n  geom_point(size= 4)+\n  labs(x= \"PM2.5 concentration (ug/m3)\", y= \"PAH level (ug/L)\")\n\n\n\n\n\n\n\n\nBased on the graphic, it seems that the level of PAHs is positively associated with PM2.5 concentration.\nNext, I am showing a graph that explores the association between FVC and PM2.5.\n\nggplot(wff_data, aes(x= PM2.5, y=FVC))+\n  geom_point(shape= 25, fill= \"steelblue1\", size= 3)+\n  labs(x= \"PM2.5 Concentration (ug/m3)\", y= \"FVC (L)\")\n\n\n\n\n\n\n\n\nBased on this graph, it is hard to tell if FVC depends on PM2.5 concentration. Given so, I decided to fit a model and check for associations. In this case, since PM2.5 follows a log-normal distribution, I decided to use a log generalized linear model from the quasipoisson family. I used PM2.5, Smoking and Medication as predictors for FVC as an outcome.\n\nlog_fit &lt;- glm(FVC ~ PM2.5 + Smoking + Medication, data = wff_data, family = quasipoisson(link = \"log\"))\n\nsummary(log_fit)\n\n\nCall:\nglm(formula = FVC ~ PM2.5 + Smoking + Medication, family = quasipoisson(link = \"log\"), \n    data = wff_data)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.6225347  0.0583858  27.790   &lt;2e-16 ***\nPM2.5       -0.0003841  0.0013235  -0.290    0.773    \nSmoking      0.0238213  0.0883566   0.270    0.788    \nMedication   0.0471521  0.0690668   0.683    0.498    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 0.2747653)\n\n    Null deviance: 15.940  on 59  degrees of freedom\nResidual deviance: 15.764  on 56  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\n\nBased on the log_fit model, it seems that there is no interaction of PM2.5 or any of the covariates to the FVC outcome. Which makes sense, since this outcome was mapped independently from the PM2.5 concentrations. However, in the real-life study I hope to find an association among these variables.\nAnd finally, a model to explore if PM2.5 is a predictor of PAH levels. Also assessing Smoking and Medication as confounders.\n\nlog_fit2 &lt;- glm(PAH ~ PM2.5 + Smoking + Medication, data = wff_data, family = quasipoisson(link = \"log\"))\n\nsummary(log_fit2)\n\n\nCall:\nglm(formula = PAH ~ PM2.5 + Smoking + Medication, family = quasipoisson(link = \"log\"), \n    data = wff_data)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.324968   0.100237 -23.195  &lt; 2e-16 ***\nPM2.5        0.009844   0.001714   5.743 3.96e-07 ***\nSmoking      0.217576   0.144650   1.504    0.138    \nMedication   0.081541   0.122211   0.667    0.507    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 0.02641762)\n\n    Null deviance: 2.3605  on 59  degrees of freedom\nResidual deviance: 1.5498  on 56  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nIn this case, we reject the null hypothesis that there is no association between PM2.5 and PAH levels (the outcome). Of course this was expected, since this was one of the requirements when creating the data set. Based on this model we didn’t find an association between smoking and the use of medication with the PAH outcome."
  },
  {
    "objectID": "data-exercise/data-exercise.html#option-2-synthetic-data",
    "href": "data-exercise/data-exercise.html#option-2-synthetic-data",
    "title": "Data Exercise",
    "section": "",
    "text": "For this exercise I chose to create a dataset somewhat similar to the expected data I plain to obtain from a future research project I will be working on. The dataset is about a hypothetical project that assesses the health effects of exposure to air pollutants in wildland firefighters.\nThese are the packages used for this exercise:\n\nlibrary(here)\n\nhere() starts at C:/Users/molli/OneDrive/Documentos/UGA/Spring 2024/MADA/erickmollinedo-MADA-portfolio\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(lubridate)\nlibrary(gtsummary)\n\n#BlackLivesMatter\n\n\n\n\nFirst, I set a seed for reproducibility and then I defined the number of observations for this dataset\n\nset.seed(123)\n\n#Defined 60 as the number of observations (n=60 wildland firefighters)\nn_wffs &lt;- 60\n\nIn this part I created the empty data frame wff_data with thirteen variables, which includes the ID, Date, three socio-demographic variables, PM2.5 as an exposure, two potential confounders and two outcomes.\n\nwff_data &lt;- data.frame(\n  ID = numeric(n_wffs),\n  Date = as_date(character(n_wffs)),\n  Age = numeric(n_wffs),\n  Gender = character(n_wffs),\n  Ethnicity = character(n_wffs),\n  PM2.5 = numeric(n_wffs),\n  Medication = integer(n_wffs),\n  Smoking = integer(n_wffs),\n  FVC = numeric(n_wffs),\n  PAH = numeric(n_wffs)\n)\n\nAnd here I fill each variable with their respective values\n\n#Variable 1: ID\nwff_data$ID &lt;- 1:n_wffs\n\n#Variable 2: Date\nwff_data$Date &lt;- as_date(sample(seq(from = as_date(\"2024-01-08\"), to = as_date(\"2024-03-01\"), by= \"days\"), n_wffs, replace = T))\n\n#Variable 3: Age. Specifying that the ranges should be from 22 to 55 years old\nwff_data$Age &lt;- sample(22:55, 60, replace = TRUE)\n\n#Variable 4: Gender. Specifying that there should be more males than females using the `prob =` statement\nwff_data$Gender &lt;- map_chr(sample(c(\"Male\", \"Female\"), n_wffs, replace = T, prob = c(0.84, 0.16)), as.character)\n\n#Variable 5: Ethnicity. Specifying the proportions for each ethnicity.\nwff_data$Ethnicity &lt;- map_chr(sample(c(\"Caucasian\", \"African American\", \"Hispanic/Latino\", \"American Indian\"),\n                                  n_wffs, replace = T, prob = c(0.75, 0.15, 0.05, 0.05)), as.character)\n\n#Variable 6: PM2.5 (in micrograms per cubic meter). The concentrations were computed following a log-normal distribution `rlnorm()`, characteristic of PM2.5 data.\nwff_data$PM2.5 &lt;- round(rlnorm(n_wffs, meanlog = log(30), sdlog = 0.5), 2)\n\n#Variable 7: Medication (If the participants take any medication for blood pressure). 0= No medication, 1=Medication. This variable depends on Age, with higher age, the highest the probability of taking blood pressure medication, using 40 years old as a cutoff point.\nwff_data$Medication[wff_data$Age &lt;= 40 ] &lt;- map_int(sample(0:1, sum(wff_data$Age &lt;= 40) , replace = T, prob = c(0.95, 0.05)), as.integer)\nwff_data$Medication[wff_data$Age &gt; 40 ] &lt;- map_int(sample(0:1, sum(wff_data$Age &gt; 40) , replace = T, prob = c(0.6, 0.4)), as.integer)\n\n#Variable 8: Smoking. 0=Doesn't smoke, 1=Smokes. Specifying that there are more non-smokers than smokers.\nwff_data$Smoking &lt;- map_int(sample(0:1, n_wffs, replace = T, prob = c(0.85, 0.15)), as.integer)\n\n#Variable 9: Forced Vital capacity (FVC) measured by spirometry. The mean value of FVC is dependent on Gender.\nwff_data$FVC[wff_data$Gender == \"Male\"] &lt;- round(rnorm(sum(wff_data$Gender == \"Male\"), mean = 5.3, sd = 1), 1)\nwff_data$FVC[wff_data$Gender == \"Female\"] &lt;- round(rnorm(sum(wff_data$Gender == \"Female\"), mean = 3.5, sd = 0.5), 1)\n\n#Variable 10: Polycyclic Aromatic Hydrocarbons (PAHs) from urine samples.(There are multiple PAHs, but in this case I assume that 3-Hydroxybenzo(a)pyrene was measured). The PAH level depends on the PM2.5 concentration, with higher PM2.5 exposure, the higher the level of urine PAH.\nwff_data$PAH[wff_data$PM2.5 &lt;= 45] &lt;- round(rnorm(sum(wff_data$PM2.5 &lt;= 45), mean = 0.12, sd= 0.04), 3)\nwff_data$PAH[wff_data$PM2.5 &gt; 45] &lt;- round(rnorm(sum(wff_data$PM2.5 &gt; 45), mean = 0.31, sd= 0.04), 3)\n\nAnd here, checking that the data frame looks good\n\nhead(wff_data)\n\n  ID       Date Age Gender        Ethnicity PM2.5 Medication Smoking FVC   PAH\n1  1 2024-02-07  54 Female        Caucasian 14.44          0       0 3.9 0.118\n2  2 2024-01-22  48   Male        Caucasian 42.32          0       0 5.2 0.145\n3  3 2024-02-27  46   Male        Caucasian 85.73          0       0 3.5 0.270\n4  4 2024-01-21  42   Male        Caucasian 15.76          0       0 4.9 0.173\n5  5 2024-01-10  36   Male        Caucasian 44.48          0       0 5.4 0.120\n6  6 2024-02-18  47   Male African American 44.07          0       0 6.1 0.161\n\n\nI saved the data frame in a .rds file\n\nsaveRDS(wff_data, here(\"data-exercise\", \"data\", \"wff_data.Rds\"))\n\n\n\n\nFirst I explored the dataset using summary()\n\nsummary(wff_data)\n\n       ID             Date                 Age           Gender         \n Min.   : 1.00   Min.   :2024-01-10   Min.   :24.00   Length:60         \n 1st Qu.:15.75   1st Qu.:2024-01-20   1st Qu.:33.75   Class :character  \n Median :30.50   Median :2024-02-03   Median :43.00   Mode  :character  \n Mean   :30.50   Mean   :2024-02-03   Mean   :41.05                     \n 3rd Qu.:45.25   3rd Qu.:2024-02-17   3rd Qu.:48.50                     \n Max.   :60.00   Max.   :2024-03-01   Max.   :55.00                     \n  Ethnicity             PM2.5          Medication      Smoking      \n Length:60          Min.   : 14.44   Min.   :0.00   Min.   :0.0000  \n Class :character   1st Qu.: 23.50   1st Qu.:0.00   1st Qu.:0.0000  \n Mode  :character   Median : 28.88   Median :0.00   Median :0.0000  \n                    Mean   : 35.90   Mean   :0.25   Mean   :0.1333  \n                    3rd Qu.: 40.69   3rd Qu.:0.25   3rd Qu.:0.0000  \n                    Max.   :151.67   Max.   :1.00   Max.   :1.0000  \n      FVC             PAH        \n Min.   :2.800   Min.   :0.0210  \n 1st Qu.:4.075   1st Qu.:0.1027  \n Median :5.250   Median :0.1340  \n Mean   :5.073   Mean   :0.1519  \n 3rd Qu.:6.000   3rd Qu.:0.1640  \n Max.   :7.300   Max.   :0.3650  \n\n\nAnd now I created a summary table with some of the descriptive statistics of this dataset, separating by gender.\n\n#Just showing the summary statistics of age, ethnicity, PM2.5, FVC and PAHs, computing the mean and sd, median and IQR and the minimum and maximum values\nsumtable &lt;- wff_data %&gt;% select(Age, Gender, PM2.5, FVC, PAH) %&gt;% \n  tbl_summary(by= Gender, \n              type = all_continuous() ~ \"continuous2\",\n              statistic = all_continuous() ~ c(\"{mean} ({sd})\", \"{median} ({p25}, {p75})\", \"{min}, {max}\")) %&gt;% \n  bold_labels()\n\n#Print the generated table\nsumtable\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nFemale, N = 11\nMale, N = 49\n\n\n\n\nAge\n\n\n\n\n\n\n    Mean (SD)\n38 (12)\n42 (9)\n\n\n    Median (IQR)\n35 (27, 49)\n43 (36, 48)\n\n\n    Range\n24, 54\n26, 55\n\n\nPM2.5\n\n\n\n\n\n\n    Mean (SD)\n44 (41)\n34 (18)\n\n\n    Median (IQR)\n35 (18, 40)\n28 (24, 41)\n\n\n    Range\n14, 152\n16, 90\n\n\nFVC\n\n\n\n\n\n\n    Mean (SD)\n3.62 (0.49)\n5.40 (1.00)\n\n\n    Median (IQR)\n3.80 (3.30, 3.95)\n5.40 (4.90, 6.10)\n\n\n    Range\n2.80, 4.30\n3.30, 7.30\n\n\nPAH\n\n\n\n\n\n\n    Mean (SD)\n0.15 (0.06)\n0.15 (0.08)\n\n\n    Median (IQR)\n0.14 (0.12, 0.17)\n0.13 (0.10, 0.16)\n\n\n    Range\n0.07, 0.27\n0.02, 0.37\n\n\n\n\n\n\n\nFirst I want to plot a histogram to check if the PM2.5 data follow a normal distribution (it should not)\n\nggplot(wff_data, aes(x= PM2.5))+\n  geom_histogram()+\n  theme_bw()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe histogram confirms that the concentrations do not follow a normal distribution, since the data is right-skewed. This should follow a logarithmic distribution.\nHere I am showing a plot to explore the association between the level of urine PAHs and PM2.5 concentration from the wildland firefighter subjects. I decided to separate them by gender.\n\nggplot(wff_data, aes(x= PM2.5, y= PAH, color= Gender))+\n  geom_point(size= 4)+\n  labs(x= \"PM2.5 concentration (ug/m3)\", y= \"PAH level (ug/L)\")\n\n\n\n\n\n\n\n\nBased on the graphic, it seems that the level of PAHs is positively associated with PM2.5 concentration.\nNext, I am showing a graph that explores the association between FVC and PM2.5.\n\nggplot(wff_data, aes(x= PM2.5, y=FVC))+\n  geom_point(shape= 25, fill= \"steelblue1\", size= 3)+\n  labs(x= \"PM2.5 Concentration (ug/m3)\", y= \"FVC (L)\")\n\n\n\n\n\n\n\n\nBased on this graph, it is hard to tell if FVC depends on PM2.5 concentration. Given so, I decided to fit a model and check for associations. In this case, since PM2.5 follows a log-normal distribution, I decided to use a log generalized linear model from the quasipoisson family. I used PM2.5, Smoking and Medication as predictors for FVC as an outcome.\n\nlog_fit &lt;- glm(FVC ~ PM2.5 + Smoking + Medication, data = wff_data, family = quasipoisson(link = \"log\"))\n\nsummary(log_fit)\n\n\nCall:\nglm(formula = FVC ~ PM2.5 + Smoking + Medication, family = quasipoisson(link = \"log\"), \n    data = wff_data)\n\nCoefficients:\n              Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.6225347  0.0583858  27.790   &lt;2e-16 ***\nPM2.5       -0.0003841  0.0013235  -0.290    0.773    \nSmoking      0.0238213  0.0883566   0.270    0.788    \nMedication   0.0471521  0.0690668   0.683    0.498    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 0.2747653)\n\n    Null deviance: 15.940  on 59  degrees of freedom\nResidual deviance: 15.764  on 56  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 4\n\n\nBased on the log_fit model, it seems that there is no interaction of PM2.5 or any of the covariates to the FVC outcome. Which makes sense, since this outcome was mapped independently from the PM2.5 concentrations. However, in the real-life study I hope to find an association among these variables.\nAnd finally, a model to explore if PM2.5 is a predictor of PAH levels. Also assessing Smoking and Medication as confounders.\n\nlog_fit2 &lt;- glm(PAH ~ PM2.5 + Smoking + Medication, data = wff_data, family = quasipoisson(link = \"log\"))\n\nsummary(log_fit2)\n\n\nCall:\nglm(formula = PAH ~ PM2.5 + Smoking + Medication, family = quasipoisson(link = \"log\"), \n    data = wff_data)\n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -2.324968   0.100237 -23.195  &lt; 2e-16 ***\nPM2.5        0.009844   0.001714   5.743 3.96e-07 ***\nSmoking      0.217576   0.144650   1.504    0.138    \nMedication   0.081541   0.122211   0.667    0.507    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for quasipoisson family taken to be 0.02641762)\n\n    Null deviance: 2.3605  on 59  degrees of freedom\nResidual deviance: 1.5498  on 56  degrees of freedom\nAIC: NA\n\nNumber of Fisher Scoring iterations: 5\n\n\nIn this case, we reject the null hypothesis that there is no association between PM2.5 and PAH levels (the outcome). Of course this was expected, since this was one of the requirements when creating the data set. Based on this model we didn’t find an association between smoking and the use of medication with the PAH outcome."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My website and data analysis portfolio",
    "section": "",
    "text": "Erick Mollinedo’s MADA portfolio\n\nHello and welcome to my portfolio!\nThis is a space where I will be posting content for this course and you will get to know more about me. It is still under wraps but it will keep upgrading as time passes.\n\nPlease use the menu bar to explore about me and my projects\nHave fun!\n\nFeel free to contact me via email: erick.mollinedo@uga.edu"
  },
  {
    "objectID": "starter-analysis-exercise/code/analysis-code/readme.html",
    "href": "starter-analysis-exercise/code/analysis-code/readme.html",
    "title": "Erick Mollinedo MADA portfolio",
    "section": "",
    "text": "This folder contains code to do some simple exploratory analysis and statistical analysis on the processed/cleaned data. The code produces a few tables and figures, which are saved in the results folder.\nIt’s the same code done 3 times:\n\nFirst, there is an R script that you can run which does all the computations.\nSecond, there is a Quarto file which contains exactly the same code as the R script.\nThird, my current favorite, is a Quarto file with an approach where the code is pulled in from the R script and run.\n\nThe last version has the advantage of having code in one place for easy writing/debugging, and then being able to pull the code into the Quarto file for a nice combination of text/commentary and code.\nEach way of doing this is a reasonable approach, pick whichever one you prefer or makes the most sense for your setup. Whichever approach you choose, add ample documentation/commentary so you and others can easily understand what’s going on and what is done."
  },
  {
    "objectID": "starter-analysis-exercise/code/eda-code/eda2.html",
    "href": "starter-analysis-exercise/code/eda-code/eda2.html",
    "title": "An example exploratory analysis script",
    "section": "",
    "text": "This Quarto file loads the cleaned data and does some exploring.\nI’m only showing it the way where the code is included in the file. As described in the processing_code materials, I currently prefer the approach of having R code in a separate file and pulling it in.\nBut I already had this written and haven’t yet re-done it that way. Feel free to redo and send a pull request on GitHub :)\nAgain, it is largely a matter of preference and what makes the most sense to decide if one wants to have code inside Quarto files, or as separate R files. And sometimes, an R script with enough comments is good enough and one doesn’t need a Quarto file.\nAlso note that while here I split cleaning and exploring, this is iterative. You saw that as part of the processing, we already had to explore the data somewhat to understand how to clean it. In general, as you explore, you’ll find things that need cleaning. As you clean, you can explore more. Therefore, at times it might make more sense to combine the cleaning and exploring code parts into a single R or Quarto file. Or split things in any other logical way.\nAs part of the exploratory analysis, you should produce plots or tables or other summary quantities for the most interesting/important quantities in your data. Depending on the total number of variables in your dataset, explore all or some of the others. Figures produced here might be histograms or density plots, correlation plots, etc. Tables might summarize your data.\nStart by exploring one variable at a time. Then continue by creating plots or tables of the outcome(s) of interest and the predictor/exposure/input variables you are most interested in. If your dataset is small, you can do that for all variables.\nPlots produced here can be scatterplots, boxplots, violinplots, etc. Tables can be simple 2x2 tables or larger ones.\n\nSetup\n\n#load needed packages. make sure they are installed.\nlibrary(here) #for data loading/saving\n\nWarning: package 'here' was built under R version 4.2.3\n\n\nhere() starts at C:/Users/malik/Documents/1. UGA Classes/15. Malika Spring 2024/MADASpring_24/erickmollinedo-MADA-portfolio\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(skimr)\n\nWarning: package 'skimr' was built under R version 4.2.3\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.3\n\n\nLoad the data.\n\n#Path to data. Note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata2.rds\")\n#load data\nmydata &lt;- readRDS(data_location)\n\n\n\nData exploration through tables\nShowing a bit of code to produce and save a summary table.\n\nsummary_df = skimr::skim(mydata)\nprint(summary_df)\n\n── Data Summary ────────────────────────\n                           Values\nName                       mydata\nNumber of rows             9     \nNumber of columns          5     \n_______________________          \nColumn type frequency:           \n  character                1     \n  factor                   1     \n  numeric                  3     \n________________________         \nGroup variables            None  \n\n── Variable type: character ────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate min max empty n_unique whitespace\n1 Occupation            0             1   5  11     0        5          0\n\n── Variable type: factor ───────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate ordered n_unique top_counts      \n1 Gender                0             1 FALSE          3 M: 4, F: 3, O: 2\n\n── Variable type: numeric ──────────────────────────────────────────────────────\n  skim_variable n_missing complete_rate  mean    sd  p0 p25 p50 p75 p100 hist \n1 Height                0             1 166.  16.0  133 156 166 178  183 ▂▁▃▃▇\n2 Weight                0             1  70.1 21.2   45  55  70  80  110 ▇▂▃▂▂\n3 Age                   0             1  33.9  5.93  25  30  34  37   45 ▅▅▇▂▂\n\n# save to file\nsummarytable_file = here(\"starter-analysis-exercise\",\"results\", \"tables-files\", \"summarytable.rds\")\nsaveRDS(summary_df, file = summarytable_file)\n\nWe are saving the results to the results/tables folder. Structure the folders inside results such that they make sense for your specific analysis. Provide enough documentation that someone can understand what you are doing and what goes where. readme.md files inside each folder are a good idea.\n\n\nData exploration through figures\nHistogram plots for the continuous outcomes.\nHeight first.\n\np1 &lt;- mydata %&gt;% ggplot(aes(x=Height)) + geom_histogram() \nplot(p1)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-distribution.png\")\nggsave(filename = figure_file, plot=p1) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow weights.\n\np2 &lt;- mydata %&gt;% ggplot(aes(x=Weight)) + geom_histogram() \nplot(p2)\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"weight-distribution.png\")\nggsave(filename = figure_file, plot=p2) \n\nSaving 7 x 5 in image\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\nNow height as function of weight.\n\np3 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight)) + geom_point() + geom_smooth(method='lm')\nplot(p3)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight.png\")\nggsave(filename = figure_file, plot=p3) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nOnce more height as function of weight, stratified by gender. Note that there is so little data, it’s a bit silly. But we’ll plot it anyway.\n\np4 &lt;- mydata %&gt;% ggplot(aes(x=Height, y=Weight, color = Gender)) + geom_point() + geom_smooth(method='lm')\nplot(p4)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\n\nWarning in max(ids, na.rm = TRUE): no non-missing arguments to max; returning\n-Inf\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"height-weight-stratified.png\")\nggsave(filename = figure_file, plot=p4) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning in qt((1 - level)/2, df): NaNs produced\n\nWarning in qt((1 - level)/2, df): no non-missing arguments to max; returning\n-Inf\n\n\nNow height as function of Occupation.\n\np5 &lt;- mydata %&gt;% ggplot(aes(x= Occupation, y=Height)) + \n  geom_boxplot(fill= \"coral1\") + \n  geom_smooth(method='lm')+\n  theme_bw()\nplot(p5)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"Occupation-height.png\")\nggsave(filename = figure_file, plot=p5) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\nNow Age as a function of Weight.\n\np6 &lt;- mydata %&gt;% ggplot(aes(x=Weight, y=Age)) + \n  geom_point(color= \"firebrick\") + \n  geom_smooth(method='lm')\nplot(p6)\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nfigure_file = here(\"starter-analysis-exercise\",\"results\",\"figures\",\"Weight-age.png\")\nggsave(filename = figure_file, plot=p6) \n\nSaving 7 x 5 in image\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\nNotes\nFor your own explorations, tables and figures can be “quick and dirty”. As long as you can see what’s going on, there is no need to polish them. That’s in contrast to figures you’ll produce for your final products (paper, report, presentation, website, etc.). Those should look as nice, polished and easy to understand as possible."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "href": "starter-analysis-exercise/code/processing-code/processingfile.html",
    "title": "An example cleaning script",
    "section": "",
    "text": "Processing script\nThis Quarto file contains a mix of code and explanatory text to illustrate a simple data processing/cleaning setup.\n\n\nSetup\nLoad needed packages. make sure they are installed.\n\nlibrary(readxl) #for loading Excel files\n\nWarning: package 'readxl' was built under R version 4.2.3\n\nlibrary(dplyr) #for data processing/cleaning\n\nWarning: package 'dplyr' was built under R version 4.2.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(tidyr) #for data processing/cleaning\n\nWarning: package 'tidyr' was built under R version 4.2.3\n\nlibrary(skimr) #for nice visualization of data \n\nWarning: package 'skimr' was built under R version 4.2.3\n\nlibrary(here) #to set paths\n\nWarning: package 'here' was built under R version 4.2.3\n\n\nhere() starts at C:/Users/malik/Documents/1. UGA Classes/15. Malika Spring 2024/MADASpring_24/erickmollinedo-MADA-portfolio\n\n\n\n\nData loading\nNote that for functions that come from specific packages (instead of base R), I often specify both package and function like so: package::function() that’s not required one could just call the function specifying the package makes it clearer where the function “lives”, but it adds typing. You can do it either way.\n\n# path to data\n# note the use of the here() package and not absolute paths\ndata_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"raw-data\",\"exampledata.xlsx\")\nrawdata &lt;- readxl::read_excel(data_location)\n\n\n\nCheck data\nFirst we can look at the codebook\n\ncodebook &lt;- readxl::read_excel(data_location, sheet =\"Codebook\")\nprint(codebook)\n\n# A tibble: 3 × 3\n  `Variable Name` `Variable Definition`                 `Allowed Values`      \n  &lt;chr&gt;           &lt;chr&gt;                                 &lt;chr&gt;                 \n1 Height          height in centimeters                 numeric value &gt;0 or NA\n2 Weight          weight in kilograms                   numeric value &gt;0 or NA\n3 Gender          identified gender (male/female/other) M/F/O/NA              \n\n\nSeveral ways of looking at the data\n\ndplyr::glimpse(rawdata)\n\nRows: 14\nColumns: 3\n$ Height &lt;chr&gt; \"180\", \"175\", \"sixty\", \"178\", \"192\", \"6\", \"156\", \"166\", \"155\", …\n$ Weight &lt;dbl&gt; 80, 70, 60, 76, 90, 55, 90, 110, 54, 7000, NA, 45, 55, 50\n$ Gender &lt;chr&gt; \"M\", \"O\", \"F\", \"F\", \"NA\", \"F\", \"O\", \"M\", \"N\", \"M\", \"F\", \"F\", \"M…\n\nsummary(rawdata)\n\n    Height              Weight          Gender         \n Length:14          Min.   :  45.0   Length:14         \n Class :character   1st Qu.:  55.0   Class :character  \n Mode  :character   Median :  70.0   Mode  :character  \n                    Mean   : 602.7                     \n                    3rd Qu.:  90.0                     \n                    Max.   :7000.0                     \n                    NA's   :1                          \n\nhead(rawdata)\n\n# A tibble: 6 × 3\n  Height Weight Gender\n  &lt;chr&gt;   &lt;dbl&gt; &lt;chr&gt; \n1 180        80 M     \n2 175        70 O     \n3 sixty      60 F     \n4 178        76 F     \n5 192        90 NA    \n6 6          55 F     \n\nskimr::skim(rawdata)\n\n\nData summary\n\n\nName\nrawdata\n\n\nNumber of rows\n14\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n2\n\n\nnumeric\n1\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nHeight\n0\n1\n1\n5\n0\n13\n0\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nWeight\n1\n0.93\n602.69\n1922.25\n45\n55\n70\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\n\n\nCleaning\nBy inspecting the data as done above, we find some problems that need addressing:\nFirst, there is an entry for height which says “sixty” instead of a number. Does that mean it should be a numeric 60? It somehow doesn’t make sense since the weight is 60kg, which can’t happen for a 60cm person (a baby). Since we don’t know how to fix this, we might decide to remove the person. This “sixty” entry also turned all Height entries into characters instead of numeric. That conversion to character also means that our summary function isn’t very meaningful. So let’s fix that first.\n\nd1 &lt;- rawdata %&gt;% dplyr::filter( Height != \"sixty\" ) %&gt;% \n                  dplyr::mutate(Height = as.numeric(Height))\nskimr::skim(d1)\n\n\nData summary\n\n\nName\nd1\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n151.62\n46.46\n6\n154.00\n165\n175\n192\n▁▁▁▂▇\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\nhist(d1$Height)\n\n\n\n\nNow we see that there is one person with a height of 6. That could be a typo, or someone mistakenly entered their height in feet. Since we unfortunately don’t know, we might need to remove this person, which we’ll do here.\n\nd2 &lt;- d1 %&gt;% dplyr::mutate( Height = replace(Height, Height==\"6\",round(6*30.48,0)) )\nskimr::skim(d2)\n\n\nData summary\n\n\nName\nd2\n\n\nNumber of rows\n13\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1.00\n165.23\n16.52\n133\n155.00\n166\n178\n192\n▂▇▆▆▃\n\n\nWeight\n1\n0.92\n647.92\n2000.48\n45\n54.75\n73\n90\n7000\n▇▁▁▁▁\n\n\n\n\n\nHeight values seem ok now.\nNow let’s look at the Weight variable. There is a person with weight of 7000, which is impossible, and one person with missing weight. To be able to analyze the data, we’ll remove those individuals as well.\n\nd3 &lt;- d2 %&gt;%  dplyr::filter(Weight != 7000) %&gt;% tidyr::drop_na()\nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nGender\n0\n1\n1\n2\n0\n5\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow checking the Gender variable. Gender should be a categorical/factor variable but is loaded as character. We can fix that with simple base R code to mix things up.\n\nd3$Gender &lt;- as.factor(d3$Gender)  \nskimr::skim(d3)\n\n\nData summary\n\n\nName\nd3\n\n\nNumber of rows\n11\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n5\nM: 4, F: 3, O: 2, N: 1\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n167.09\n16.81\n133\n155.5\n166\n179\n192\n▂▇▅▇▅\n\n\nWeight\n0\n1\n70.45\n20.65\n45\n54.5\n70\n85\n110\n▇▂▃▃▂\n\n\n\n\n\nNow we see that there is another NA, but it’s not NA from R, instead it was loaded as character and is now considered as a category. Well proceed here by removing that individual with that NA entry. Since this keeps an empty category for Gender, I’m also using droplevels() to get rid of it.\n\nd4 &lt;- d3 %&gt;% dplyr::filter( !(Gender %in% c(\"NA\",\"N\")) ) %&gt;% droplevels()\nskimr::skim(d4)\n\n\nData summary\n\n\nName\nd4\n\n\nNumber of rows\n9\n\n\nNumber of columns\n3\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\nfactor\n1\n\n\nnumeric\n2\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: factor\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\n\n\n\n\nGender\n0\n1\nFALSE\n3\nM: 4, F: 3, O: 2\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nHeight\n0\n1\n165.67\n15.98\n133\n156\n166\n178\n183\n▂▁▃▃▇\n\n\nWeight\n0\n1\n70.11\n21.25\n45\n55\n70\n80\n110\n▇▂▃▂▂\n\n\n\n\n\nAll done, data is clean now.\nLet’s assign at the end to some final variable, this makes it easier to add further cleaning steps above.\n\nprocesseddata &lt;- d4\n\n\n\nSave data\nFinally, we save the clean data as RDS file. I suggest you save your processed and cleaned data as RDS or RDA/Rdata files. This preserves coding like factors, characters, numeric, etc. If you save as CSV, that information would get lost. However, CSV is better for sharing with others since it’s plain text. If you do CSV, you might want to write down somewhere what each variable is.\nSee here for some suggestions on how to store your processed data: http://www.sthda.com/english/wiki/saving-data-into-r-data-format-rds-and-rdata\n\nsave_data_location &lt;- here::here(\"starter-analysis-exercise\",\"data\",\"processed-data\",\"processeddata.rds\")\nsaveRDS(processeddata, file = save_data_location)\n\nNote the use of the here package and here command to specify a path relative to the main project directory, that is the folder that contains the .Rproj file. Always use this approach instead of hard-coding file paths that only exist on your computer.\n\n\nNotes\nRemoving anyone observation with “faulty” or missing data is one approach. It’s often not the best. based on your question and your analysis approach, you might want to do cleaning differently (e.g. keep observations with some missing information)."
  },
  {
    "objectID": "starter-analysis-exercise/code/processing-code/readme.html",
    "href": "starter-analysis-exercise/code/processing-code/readme.html",
    "title": "Erick Mollinedo MADA portfolio",
    "section": "",
    "text": "This folder contains code for processing data.\nCurrently, there is just a single Quarto file to illustrate how the processing can look like.\nInstead of a Quarto file that contains code, it is also possible to use R scripts or a combination of R scripts and Quarto code. Those approaches are illustrated in the full dataanalysis-template repository."
  },
  {
    "objectID": "starter-analysis-exercise/data/raw-data/readme.html",
    "href": "starter-analysis-exercise/data/raw-data/readme.html",
    "title": "Erick Mollinedo MADA portfolio",
    "section": "",
    "text": "This folder contains a simple made-up data-set in an Excel file.\nIt contains the variables Height, Weight and Gender of a few imaginary individuals.\nThe dataset purposefully contains some faulty entries that need to be cleaned.\nGenerally, any dataset should contain some meta-data explaining what each variable in the dataset is. (This is often called a Codebook.) For this simple example, the codebook is given as a second sheet in the Excel file.\nThis raw data-set should generally not be edited by hand. It should instead be loaded and processed/cleaned using code."
  },
  {
    "objectID": "starter-analysis-exercise/products/readme.html",
    "href": "starter-analysis-exercise/products/readme.html",
    "title": "Erick Mollinedo MADA portfolio",
    "section": "",
    "text": "The folders inside this folder should contain all the products of your project.\nFor a classical academic project, this will be a peer-reviewed manuscript, and should be placed into a manuscript folder.\nFor our case, since we’ll want to put it on the website, we call it a report.\nOften you need a library of references in bibtex format, as well as a CSL style file that determines reference formatting. Since those files might be used by several of the products, I’m placing them in the main products folder. Feel free to re-organize."
  },
  {
    "objectID": "starter-analysis-exercise/results/figures/readme.html",
    "href": "starter-analysis-exercise/results/figures/readme.html",
    "title": "Erick Mollinedo MADA portfolio",
    "section": "",
    "text": "Folder for all figures.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "starter-analysis-exercise/results/tables-files/readme.html",
    "href": "starter-analysis-exercise/results/tables-files/readme.html",
    "title": "Erick Mollinedo MADA portfolio",
    "section": "",
    "text": "Folder for all tables (generally stored as Rds files) and other files.\nYou can create further sub-folders if that makes sense."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#mavoglurant-modeling-exercise-week-8",
    "href": "fitting-exercise/fitting-exercise.html#mavoglurant-modeling-exercise-week-8",
    "title": "Fitting exercise",
    "section": "",
    "text": "These are the packages I used for this exercise\n\nlibrary(here)\nlibrary(readr)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gtsummary)\nlibrary(GGally)\n\nLoading the dataset, assigned it to the mavoglurant dataframe.\n\nmavoglurant &lt;- read_csv(here(\"fitting-exercise\", \"Mavoglurant_A2121_nmpk.csv\"))\n\nRows: 2678 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (17): ID, CMT, EVID, EVI2, MDV, DV, LNDV, AMT, TIME, DOSE, OCC, RATE, AG...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\nFirst, I created a plot showing the concentration of Mavoglurant DV over TIME, by DOSE. In the first attempt, the dose was plotted as a numeric variable so I mutated DOSE to be a categorical variable.\n\n#Make `DOSE` a categorical variable using as.factor().\nmavoglurant &lt;- mavoglurant %&gt;%\n  mutate(DOSE = as.factor(DOSE))\n\n#Create the plot of concentration by time, categorized by dose using ggplot().\nggplot(mavoglurant, aes(x = TIME, y = DV, group= ID)) +\n  geom_line() + #Do a line plot\n  facet_wrap(~ DOSE) + #Group by DOSE\n  labs(x = \"Time\", y = \"Mavoglurant concentration\", color = \"Dose\")\n\n\n\n\n\n\n\n\nNow, keeping just one of the observations for individuals that have two OCC observations.\n\nmavoglurant &lt;- mavoglurant %&gt;% filter(OCC == 1)\n\nNow, removing observations where TIME is equal to 0 and create a new dataframe mavoglurant_sum where it summarizes the concentrations from DV by each subject. Then, I created the mavoglurant_zero dataframe that contains only the observations where TIME is equal to 0. An finally I joined both new dataframes into the mavoglurant_new df.\n\n# Exclude observations where 'TIME' = 0 and then compute the sum of 'DV' for each subject or 'ID', to create the `mavoglurant_sum` dataframe.\nmavoglurant_sum &lt;- mavoglurant %&gt;%\n  filter(TIME != 0) %&gt;% #Remove observations where time= 0\n  group_by(ID) %&gt;% #Group by subject\n  summarize(Y = sum(DV)) #The sum variable is called `Y`\n\n#Create a dataframe with observations where TIME= 0.\nmavoglurant_zero &lt;- mavoglurant %&gt;% \n  filter(TIME == 0) %&gt;% \n  group_by(ID)\n\n#Join the previous dataframes using left_join()\nmavoglurant_new &lt;- inner_join(mavoglurant_sum, mavoglurant_zero, by = \"ID\")\n\nFinally, I filtered out unnecessary variables for this exercise and RACE, and SEX were converted to factor type variables.\n\n#Mutate SEX and RACE to factory type variables and then only keep Y, DOSE, AGE, SEX, RACE, WT and HT.\nmavoglurant_new &lt;- mavoglurant_new %&gt;% \n  mutate(RACE = as.factor(RACE), SEX = as.factor(SEX)) %&gt;% \n  select(c(Y, DOSE, AGE, SEX, RACE, WT, HT))\n\n#Check the structure of the new dataframe\nstr(mavoglurant_new)\n\ntibble [120 × 7] (S3: tbl_df/tbl/data.frame)\n $ Y   : num [1:120] 2691 2639 2150 1789 3126 ...\n $ DOSE: Factor w/ 3 levels \"25\",\"37.5\",\"50\": 1 1 1 1 1 1 1 1 1 1 ...\n $ AGE : num [1:120] 42 24 31 46 41 27 23 20 23 28 ...\n $ SEX : Factor w/ 2 levels \"1\",\"2\": 1 1 1 2 2 1 1 1 1 1 ...\n $ RACE: Factor w/ 4 levels \"1\",\"2\",\"7\",\"88\": 2 2 1 1 2 2 1 4 2 1 ...\n $ WT  : num [1:120] 94.3 80.4 71.8 77.4 64.3 ...\n $ HT  : num [1:120] 1.77 1.76 1.81 1.65 1.56 ...\n\n\n\n\n\nThe following plots and tables summarize the data observed from the mavoglurant_new dataframe.\nFirst, a Boxplot that shows the dependent variable (Y) across the three different doses.\n\n#Using ggplot() to create a boxplot of the predicted variable Y and the DOSE\nggplot(mavoglurant_new, aes(x= DOSE, y= Y))+\n  geom_boxplot(fill= \"aquamarine3\")+\n  theme_classic()+\n  labs(x= \"Dose\", y= \"Mavoglurant concentration\")\n\n\n\n\n\n\n\n\nBased on the previous plot, it can be observed that at higher dose, the concentration of mavoglurant (predicted variable) increases. It is also seen that the range of concentrations is higher at the higher dose (50).\nNow some plots that show the distribution of the dependent variable (Y) and the numeric independent variables AGE, WT and HT.\n\n#Histogram of the dependent variable (Y)\nggplot(mavoglurant_new, aes(x= Y))+\n  geom_histogram(fill= \"aquamarine3\", color= \"red\")+\n  labs(x= \"Mavoglurant concentration\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n#Histogram of AGE\nggplot(mavoglurant_new, aes(x= AGE))+\n  geom_histogram(fill= \"darkgoldenrod1\", color= \"red\")+\n  labs(x= \"Age\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n#Histogram of WT\nggplot(mavoglurant_new, aes(x= WT))+\n  geom_histogram(fill= \"darkgoldenrod1\", color= \"red\")+\n  labs(x= \"Weight\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n#Histogram of HT\nggplot(mavoglurant_new, aes(x= HT))+\n  geom_histogram(fill= \"darkgoldenrod1\", color= \"red\")+\n  labs(x= \"Height\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nIn the previous plots in can be seen that the dependent (Y) variable and the Weight, follow a normal distribution. Height is observed that is skewed to the right, so this variable could not be following a normal distribution. On the other hand, it is observed that Age follows a bi-modal distribution. This is providing an insight about maybe first applying a regression model to this dataset.\nThe following table summarizes the previous variables, categorized by SEX (1 or 2). Here, it is shown the mean (sd), median (IQR) and the range.\n\n#Creating a summary table using the tbl_summary() function from `gtsummary`\nsumtable &lt;- mavoglurant_new %&gt;% select(Y, AGE, HT, WT, SEX) %&gt;% \n  tbl_summary(by= SEX, \n              type = all_continuous() ~ \"continuous2\",\n              statistic = all_continuous() ~ c(\"{mean} ({sd})\", \"{median} ({p25}, {p75})\", \"{min}, {max}\")) %&gt;% \n  bold_labels()\n\n#Visualize the table\nsumtable\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\n1, N = 104\n2, N = 16\n\n\n\n\nY\n\n\n\n\n\n\n    Mean (SD)\n2,478 (959)\n2,236 (983)\n\n\n    Median (IQR)\n2,398 (1,727, 3,072)\n2,060 (1,491, 2,698)\n\n\n    Range\n826, 5,607\n1,044, 4,835\n\n\nAGE\n\n\n\n\n\n\n    Mean (SD)\n32 (9)\n41 (7)\n\n\n    Median (IQR)\n30 (25, 39)\n42 (38, 45)\n\n\n    Range\n18, 49\n28, 50\n\n\nHT\n\n\n\n\n\n\n    Mean (SD)\n1.78 (0.07)\n1.63 (0.06)\n\n\n    Median (IQR)\n1.78 (1.73, 1.82)\n1.63 (1.58, 1.66)\n\n\n    Range\n1.59, 1.93\n1.52, 1.75\n\n\nWT\n\n\n\n\n\n\n    Mean (SD)\n84 (12)\n73 (11)\n\n\n    Median (IQR)\n83 (75, 92)\n70 (64, 81)\n\n\n    Range\n57, 115\n58, 90\n\n\n\n\n\n\n\nAnd here, showing barplots for the categorical variables SEX and RACE.\n\n#Creating a bar plot that shows the counts for each race category by sex.\nggplot(mavoglurant_new, aes(x= RACE, fill= SEX))+\n  geom_bar(position = \"dodge\")+\n  theme_classic()+\n  labs(x= \"Race\")\n\n\n\n\n\n\n\n\nIt is observed on the previous plot that there are more subjects of sex 1, than 2 for the 1, 2 and 88 race categories. Meanwhile for the race category 7, it seems that there is the same amount of subjects by sex category. It is a shame that the correct labels for these categories are not known for sure.\nAnd finally, exploring correlations between all the variables, visualizing by a plot:\n\n#Creating a correlation plot using the ggpairs() function from the GGally package.\nggpairs(mavoglurant_new, columns = c(1, 3, 6, 7), progress = F)\n\n\n\n\n\n\n\n\nBased on this plot it is observed that the highest correlation is between the variables Height and Weight (0.6), and the linear plots in the middle confirm the distribution of each one of the variables.\n\n\n\n\n\nFirst, I fitted a linear model using the continuous outcome (Y) and DOSE as the predictor.\n\n# Define the model specification for linear regression\nlinear_model &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;% #Specify the linear model to fit the model\n  set_mode(\"regression\") #Setting the mode as a regression model\n\n# Define the formula\nformula1 &lt;- Y ~ DOSE\n\n# Fit the model\nlm_simple &lt;- linear_model %&gt;%\n  fit(formula1, data = mavoglurant_new) #Calling the formula and the dataframe to compute the linear model\n\n# Output the model summary\nsummary(lm_simple$fit)\n\n\nCall:\nstats::lm(formula = Y ~ DOSE, data = data)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1290.1  -445.6   -90.9   352.2  2367.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1782.67      87.85  20.292  &lt; 2e-16 ***\nDOSE37.5      681.24     213.69   3.188  0.00184 ** \nDOSE50       1456.20     130.43  11.165  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 674.8 on 117 degrees of freedom\nMultiple R-squared:  0.5159,    Adjusted R-squared:  0.5076 \nF-statistic: 62.33 on 2 and 117 DF,  p-value: &lt; 2.2e-16\n\n\nBased on the model it can be inferred that the outcome increases by around 681.24 units with the dose 37.5 and increases by 1456.20 with the dose 50, all compared with the dose 25. It is also observed that the differences are statistically significant, given the p-values are less than 0.001.\nNow, fitting a linear model using the continuous outcome (Y) and using the rest of the variables as predictors.\n\n#The model specification has already been set in the previous code chunk, so there is no need to set it again.\n\n# Define the formula\nformula2 &lt;- Y ~ AGE + WT + HT + DOSE + SEX + RACE\n\n# Fit the model\nlm_multi &lt;- linear_model %&gt;%\n  fit(formula2, data = mavoglurant_new)\n\n# Output the model summary\nsummary(lm_multi$fit)\n\n\nCall:\nstats::lm(formula = Y ~ AGE + WT + HT + DOSE + SEX + RACE, data = data)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1496.97  -362.81   -71.26   285.84  2421.48 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 4890.923   1822.710   2.683 0.008415 ** \nAGE            3.521      7.895   0.446 0.656517    \nWT           -23.281      6.440  -3.615 0.000454 ***\nHT          -741.050   1108.100  -0.669 0.505051    \nDOSE37.5     663.683    200.448   3.311 0.001258 ** \nDOSE50      1499.048    122.462  12.241  &lt; 2e-16 ***\nSEX2        -360.048    217.775  -1.653 0.101121    \nRACE2        148.883    129.821   1.147 0.253936    \nRACE7       -420.950    451.163  -0.933 0.352846    \nRACE88       -65.300    246.961  -0.264 0.791954    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 616.6 on 110 degrees of freedom\nMultiple R-squared:   0.62, Adjusted R-squared:  0.5889 \nF-statistic: 19.94 on 9 and 110 DF,  p-value: &lt; 2.2e-16\n\n\nFor the interpretation of this model I will focus only on the statistically significant predictors (p-value &lt; 0.001). Besides dose 37.5 with an increase of the outcome by a factor of ~664 and dose 50 with an increase by a factor of ~1500, Weight is also another variable associated with a decrease of the outcome by a factor of ~23.\nIn summary, it can be observed that the coefficients slightly changed between both models, however the second model seems a better fit. To evaluate which model is best, I computed the root mean square error (RMSE) and R-squared as metrics. First for the linear model using one predictor, and then using multiple predictors.\n\n#ONE VARIABLE AS PREDICTOR\n#Create a prediction from the dataframe\nlmsimple_pred &lt;- predict(lm_simple, new_data = mavoglurant_new %&gt;% select(-Y))\n\n#Match predicted with observed\nlmsimple_pred &lt;- bind_cols(lmsimple_pred, mavoglurant_new %&gt;% select(Y))\n\n#Estimate the metrics\nlmsimple_metrics &lt;- metric_set(rmse, rsq)\nlmsimple_metrics(lmsimple_pred, truth = Y, estimate = .pred)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     666.   \n2 rsq     standard       0.516\n\n#MULTIPLE VARIABLES AS PREDICTORS\n#Create a prediction from the dataframe\nlmmulti_pred &lt;- predict(lm_multi, new_data = mavoglurant_new %&gt;% select(-Y))\n\n#Match predicted with observed\nlmmulti_pred &lt;- bind_cols(lmmulti_pred, mavoglurant_new %&gt;% select(Y))\n\n#Estimate the metrics\nlmmulti_metrics &lt;- metric_set(rmse, rsq)\nlmmulti_metrics(lmmulti_pred, truth = Y, estimate = .pred)\n\n# A tibble: 2 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     590.   \n2 rsq     standard       0.620\n\n\nWe can observe that the RMSE is lower (590.3) in the model that inputs all the variables as predictors compared to the linear model that uses Dose as a predictor (RMSE= 666.3). We also observe that the R2 is slightly higher in the second model (0.62) compared to the first model (0.52). In this case we can conclude that the second model (linear model with multiple predictors) is a better fit to this dataset.\n\n\n\nNow, I fitted a logistic model to the outcome SEX, and using DOSE as a predictor. I also evaluated the Accuracy and ROC-AUC of this model in the following steps.\n\n# Define the model specification\nlogistic_spec &lt;- logistic_reg() %&gt;%  #Defining as logistic\n  set_engine(\"glm\") %&gt;% #...From the GLM family\n  set_mode(\"classification\") #Classification, since it involves categorical variables\n\n# Create the recipe\nrecipe &lt;- recipe(SEX ~ DOSE, data = mavoglurant_new) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes())\n\n# Split the data into training and testing sets\nset.seed(123) #For reproducibility\ndata_split &lt;- initial_split(mavoglurant_new, prop = 0.75)\ntrain_data &lt;- training(data_split) #Create a training data to apply the model\ntest_data &lt;- testing(data_split) #Create a test data to apply the model evaluation\n\n# Fit the model\nlogistic_fit &lt;- workflow() %&gt;%\n  add_recipe(recipe) %&gt;%\n  add_model(logistic_spec) %&gt;%\n  fit(data = train_data)\n\n# Make predictions on the test set to determine the ROC-AUC of the model\npredictions &lt;- predict(logistic_fit, test_data, type = \"prob\")\n\n#Make predictions on the test set to determine the Accuracy of the model\npredictions2 &lt;- logistic_fit %&gt;% predict(new_data = test_data)\n\n# Bind the predictions to the testing set\nresults &lt;- bind_cols(test_data, predictions) #ROC-AUC\nresults2 &lt;- bind_cols(test_data, predictions2) #Accuracy\n\n# Calculate ROC-AUC\nroc_auc &lt;- roc_auc(results, truth = SEX, .pred_1)\n\n# Calculate Accuracy\naccuracy &lt;- accuracy(results2, truth = SEX, estimate = .pred_class)\n\n# Output the model and the metrics\nlog1 &lt;- glm(formula = SEX ~ DOSE, family = binomial(link = \"logit\"), \n    data = train_data)\nsummary(log1)\n\n\nCall:\nglm(formula = SEX ~ DOSE, family = binomial(link = \"logit\"), \n    data = train_data)\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  -1.3581     0.3737  -3.634 0.000279 ***\nDOSE37.5      0.2595     0.8980   0.289 0.772583    \nDOSE50       -1.0986     0.7082  -1.551 0.120851    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 77.801  on 89  degrees of freedom\nResidual deviance: 74.572  on 87  degrees of freedom\nAIC: 80.572\n\nNumber of Fisher Scoring iterations: 5\n\nlist(Accuracy = accuracy, ROC_AUC = roc_auc)\n\n$Accuracy\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.933\n\n$ROC_AUC\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.393\n\n\nAnd finally, fitting a logistic model to the outcome SEX, using all of the variables as predictors. I also computed the ROC-AUC and Accuracy of this model.\n\n# The model has been defined before 'logistic_spec', so there is no need to define it again.\n\n# Create the recipe of this model\nrecipe2 &lt;- recipe(SEX ~ Y + AGE + WT + HT + DOSE + RACE, data = mavoglurant_new) %&gt;% \n  step_dummy(all_nominal(), -all_outcomes()) %&gt;%\n  step_zv(all_predictors()) %&gt;%\n  step_normalize(all_predictors())\n\n# Split the data into training and testing sets\nset.seed(123) #For reproducibility\ndata_split2 &lt;- initial_split(mavoglurant_new, prop = 0.75)\ntrain_data2 &lt;- training(data_split2) #Create a training data to apply the model\ntest_data2 &lt;- testing(data_split2) #Create a test data to apply the model evaluation\n\n# Fit the model\nlogistic_fit2 &lt;- workflow() %&gt;%\n  add_recipe(recipe2) %&gt;%\n  add_model(logistic_spec) %&gt;%\n  fit(data = train_data)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\n# Make predictions on the test set to determine the ROC-AUC of the model\npredictions_auc &lt;- predict(logistic_fit2, test_data2, type = \"prob\")\n\n#Make predictions on the test set to determine the Accuracy of the model\npredictions_acc &lt;- logistic_fit %&gt;% predict(new_data = test_data2)\n\n# Bind the predictions to the testing set\nresults_auc2 &lt;- bind_cols(test_data2, predictions_auc) #ROC-AUC\nresults_acc2 &lt;- bind_cols(test_data2, predictions_acc) #Accuracy\n\n# Calculate ROC-AUC\nroc_auc2 &lt;- roc_auc(results_auc2, truth = SEX, .pred_1)\n\n# Calculate Accuracy\naccuracy2 &lt;- accuracy(results_acc2, truth = SEX, estimate = .pred_class)\n\n# Output the metrics using list()\nlog2 &lt;- glm(formula = SEX ~ Y + AGE + WT + HT + DOSE + RACE, family = binomial(link = \"logit\"), \n    data = train_data2)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nsummary(log2)\n\n\nCall:\nglm(formula = SEX ~ Y + AGE + WT + HT + DOSE + RACE, family = binomial(link = \"logit\"), \n    data = train_data2)\n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)  \n(Intercept) 119.467600  47.800737   2.499   0.0124 *\nY            -0.002105   0.002031  -1.036   0.3000  \nAGE           0.319506   0.174943   1.826   0.0678 .\nWT           -0.190608   0.130000  -1.466   0.1426  \nHT          -64.948813  26.352756  -2.465   0.0137 *\nDOSE37.5     -7.347424   8.122569  -0.905   0.3657  \nDOSE50       -3.665713   5.095677  -0.719   0.4719  \nRACE2        -6.722388   4.881126  -1.377   0.1684  \nRACE7        -3.259653  17.820091  -0.183   0.8549  \nRACE88       -5.593682  12.167640  -0.460   0.6457  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 77.801  on 89  degrees of freedom\nResidual deviance: 12.013  on 80  degrees of freedom\nAIC: 32.013\n\nNumber of Fisher Scoring iterations: 10\n\nlist(Accuracy = accuracy2, ROC_AUC = roc_auc2)\n\n$Accuracy\n# A tibble: 1 × 3\n  .metric  .estimator .estimate\n  &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;\n1 accuracy binary         0.933\n\n$ROC_AUC\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.964\n\n\nBased on the previous logistic models, it is observed that appears there is no association between the dose of mavoglurant and sex. However, when observing the second logistic model, it appears there is a statistically significant association between height and sex (p-value &lt; 0.05). While looking at the accuracy from both models, we can see that both have the same accuracy (93%), however, the ROC-AUC value is pretty low for the model that uses only Dose as a predictor (0.39), meanwhile, the model that uses dose and all the other variables as predictors has a better value (0.96), which reflects better sensitivity and specificity."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#mavoglurant-modeling-exercise-continuation-week-10",
    "href": "fitting-exercise/fitting-exercise.html#mavoglurant-modeling-exercise-continuation-week-10",
    "title": "Fitting exercise",
    "section": "Mavoglurant modeling Exercise continuation (Week 10)",
    "text": "Mavoglurant modeling Exercise continuation (Week 10)\nFirst, naming the seed for reproducibility\n\nrngseed = 1234\nset.seed(rngseed)\n\nRemove the RACE variable from the mavoglurant_new dataframe.\n\n#Select all other variables, except `RACE`\nmavoglurant_new &lt;- mavoglurant_new %&gt;% select(-RACE)\n\nSplit the data randomly to a 75% train and 25% test data.\n\n#Let the data split be 75-25% or 3/4\ndata_split3 &lt;- initial_split(mavoglurant_new, prop = 0.75)\n\n#Create the train and test data frames\ntrain_data3 &lt;- training(data_split3)\ntest_data3 &lt;- testing(data_split3)\n\n\nModel performance assessment 1\nNow, fitting a linear regression model that predicts the outcome Y based on DOSE alone. Then, making predictions to compare against the observed values and then computing the RMSE to evaluate.\n\n#Define the model specification for linear regression\nlm_dose &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;% #Specify the linear model to fit the model\n  set_mode(\"regression\") %&gt;% #Setting the mode as a regression model\n  fit(Y ~ DOSE, data = train_data3)\n\n#Tidy the results\ntidy(lm_dose)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)    1873.      109.     17.2  1.07e-29\n2 DOSE37.5        651.      275.      2.36 2.03e- 2\n3 DOSE50         1336.      158.      8.45 5.97e-13\n\n#Create a prediction from the dataframe for the `DOSE` model\nlmdose_pred &lt;- predict(lm_dose, new_data = train_data3)\n\n#Match predicted with observed values\nlmdose_pred &lt;- bind_cols(lmdose_pred, train_data3)\n\n#Estimate the metrics for the `DOSE` model\nlmdose_metric &lt;- metric_set(rmse) #Set the function to estimate RMSE\nlmdose_metric(lmdose_pred, truth = Y, estimate = .pred) #Compute the RMSE\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        703.\n\n\nComputing another linear regression model that predicts Y using the other variables as predictors. I also computed the predicted vs observed values to compute the RMSE of this model.\n\n#Define the model specification for linear regression\nlm_all &lt;- linear_reg() %&gt;%\n  set_engine(\"lm\") %&gt;% #Specify the linear model to fit the model\n  set_mode(\"regression\") %&gt;% #Setting the mode as a regression model\n  fit(Y ~ ., data = train_data3)\n\n#Tidy the results\ntidy(lm_all)\n\n# A tibble: 7 × 5\n  term         estimate std.error statistic  p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)  5764.      2178.      2.65   9.72e- 3\n2 DOSE37.5      640.       255.      2.51   1.39e- 2\n3 DOSE50       1384.       147.      9.44   8.65e-15\n4 AGE            -0.119      9.66   -0.0123 9.90e- 1\n5 SEX2         -571.       287.     -1.99   5.00e- 2\n6 WT            -22.8        7.72   -2.95   4.13e- 3\n7 HT          -1117.      1368.     -0.817  4.17e- 1\n\n#Create a prediction from the dataframe of the model with all the other predictors\nlmall_pred &lt;- predict(lm_all, new_data = train_data3)\n\n#Match predicted with observed\nlmall_pred &lt;- bind_cols(lmall_pred, train_data3)\n\n#Estimate the metrics for the model with all the variables as predictors\nlmall_metric &lt;- metric_set(rmse) #Set the function\nlmall_metric(lmall_pred, truth = Y, estimate = .pred) #Compute the RMSE\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        627.\n\n\nNow I created a null model and computed the RMSE.\n\n#Run the null model\nlm_null &lt;- null_model(mode = \"regression\") %&gt;% \n    set_engine(\"parsnip\") %&gt;%\n    fit(Y ~ 1, data = train_data3)\n\n#Compute the RMSE and other estimates\nnull_metric &lt;- lm_null %&gt;% \n  predict(train_data3) %&gt;% \n  bind_cols(train_data3) %&gt;% \n  metrics(truth = Y, estimate = .pred)\n\nWarning: A correlation computation is required, but `estimate` is constant and has 0\nstandard deviation, resulting in a divide by 0 error. `NA` will be returned.\n\n#Print the RMSE (Note: This includes also the R-squared and MAE but I am only interested in the RMSE)\nnull_metric\n\n# A tibble: 3 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        948.\n2 rsq     standard         NA \n3 mae     standard        765.\n\n\nIn summary, according to the RMSE parameters, the model that includes all the variables as predictors performed better (RMSE= 627), compared to the model using only DOSE as a predictor (RMSE= 702) and the null model (RMSE= 948) as a reference.\n\n\nModel performance assessment 2\nNow, evaluating both models using a 10-fold cross-validation. First, by the model that predicts Y only using DOSE.\n\n#Set the seed for reproducibility\nset.seed(rngseed)\n\n#Set the cross-validation folds as 10\nfolds &lt;- vfold_cv(train_data3, v= 10)\n\n#Set the model specification, for linear regression\nlinear_mod &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n#Set the workflow\nlinear_wf &lt;- workflow() %&gt;% add_model(linear_mod) %&gt;% add_formula(Y ~ DOSE)\n\n#Do the resamples\ndose_resample &lt;- fit_resamples(linear_wf, resamples = folds)\n\n#Extract the metric\ncollect_metrics(dose_resample)\n\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   697.       10 68.1    Preprocessor1_Model1\n2 rsq     standard     0.500    10  0.0605 Preprocessor1_Model1\n\n\nNow, doing the cross-validation for the model that predicts Y using all the other variables as predictors.\n\n#Set the seed for reproducibility\nset.seed(rngseed)\n\n#Set the workflow\nlinear_all_wf &lt;- workflow() %&gt;% add_model(linear_mod) %&gt;% add_formula(Y ~ .)\n\n#Do the resamples\nall_resample &lt;- fit_resamples(linear_all_wf, resamples = folds)\n\n#Extract the metric\ncollect_metrics(all_resample)\n\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   653.       10 63.6    Preprocessor1_Model1\n2 rsq     standard     0.561    10  0.0717 Preprocessor1_Model1\n\n\nBased on the previous results, it can be observed that when fitting the model and conducting a 10-fold cross-validation the RMSEs changed but still, the model that uses all predictors have a better metric (RMSE= 653, SE= 63.6) than the model that uses only DOSE as a predictor (RMSE= 696, SE= 68).\nAnd now, computing the same CV analysis, but setting a different seed.\n\n#Set the seed for reproducibility\nset.seed(2302)\n\n#Set the cross-validation folds as 10\nfolds2 &lt;- vfold_cv(train_data3, v= 10)\n\n#Set the model specification, for linear regression\nlinear_mod2 &lt;- linear_reg() %&gt;% set_engine(\"lm\")\n\n#ONE PREDICTOR LINEAR MODEL\n#Set the workflow\nlinear_wf2 &lt;- workflow() %&gt;% add_model(linear_mod2) %&gt;% add_formula(Y ~ DOSE)\n\n#Do the resamples\ndose_resample2 &lt;- fit_resamples(linear_wf2, resamples = folds2)\n\n#Extract the metric\ncollect_metrics(dose_resample2)\n\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   706.       10 53.3    Preprocessor1_Model1\n2 rsq     standard     0.451    10  0.0690 Preprocessor1_Model1\n\n#ALL PREDICTORS LINEAR MODEL\n#Set the workflow\nlinear_all_wf2 &lt;- workflow() %&gt;% add_model(linear_mod2) %&gt;% add_formula(Y ~ .)\n\n#Do the resamples\nall_resample2 &lt;- fit_resamples(linear_all_wf2, resamples = folds2)\n\n#Extract the metric\ncollect_metrics(all_resample2)\n\n# A tibble: 2 × 6\n  .metric .estimator    mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   660.       10 57.5    Preprocessor1_Model1\n2 rsq     standard     0.516    10  0.0613 Preprocessor1_Model1\n\n\nIn this case, when changing the seed, the metrics changed for both models but they are similar in proportion. For the linear model with a single predictor the RMSE= 705.9 and for the model with all predictors the RMSE= 660. However, the standar error is higher for the model with all the predictors (SE= 57.51), compared to the model with a single predictor (SE= 53.33). Still, it seems that the linear model that uses all the predictors is better than the model that uses only DOSE as a single predictor."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#this-section-is-added-by-malika-dhakhwa.",
    "href": "fitting-exercise/fitting-exercise.html#this-section-is-added-by-malika-dhakhwa.",
    "title": "Fitting exercise",
    "section": "This section is added by Malika Dhakhwa.",
    "text": "This section is added by Malika Dhakhwa.\nI conducted a visual inspection of performance of the models by plotting the predicted values from all three models against the observed values of the training data.\nFirst, I extracted the predicted values of the Null model.For the other two models, this step has already been completed in earlier phases.\n\n# Recovering predicted values of the null model\nnull_pred &lt;- predict(lm_null, new_data = train_data3)\n\nI created a combined data frame of the predictors in long format for all three models and created the plot.\n\n#Creating the object for the outcome variable Y in the training data\nobserved_values &lt;- train_data3$Y\n\n#Creating separate data frames for each set of predictions with model labels\n\ndf_dose &lt;- data.frame(Observed = observed_values, Predicted = lmdose_pred$.pred, Model=\"Dose Model\")\ndf_all &lt;- data.frame(Observed= observed_values, Predicted = lmall_pred$.pred, Model=\"Full Model\")\ndf_null&lt;- data.frame(Observed = observed_values, Predicted = null_pred$.pred, Model=\"Null Model\")\n\n#Combining all the predicted values to a single data frame by rows to create a long format data\ncombined_df &lt;- rbind(df_dose, df_all, df_null)\n\n#Plotting of predicted vs observed data\nggplot(combined_df, aes(x=Observed, y=Predicted, color=Model))+\n  geom_point()+ \n  scale_color_manual(values = c(\"Null Model\"= \"lightblue\", \"Dose Model\"=\"red\", \"Full Model\"=\"green\")) + \n  geom_abline(intercept = 0, slope = 1, linetype = \"solid\", color=\"black\") + #45 degree line\n  xlim(0, 5000) + #X-axis limits\n  ylim(0, 5000) + #y-axis limits\n  labs(x= \"Observed Values\", y =\"Predicted Values\", title = \"Predicted vs. Observed Values\")+\n  theme_minimal() # Use a minimal theme\n\n\n\n\n\n\n\n\nThe predictions from Null model follows a horizontal line indicating that they assume a single value which is the mean of the outcome variable. The variable Dose in the observed data has only three distinct values, leading the Dose model’s predictions to appear as three horizontal lines in the plot, marked by red dots. One of these predicted values is close to that of the Null model, creating almost overlapping with the predictions from the Null model. The predicted values from the model incorporating all predictors are more dispersed indicating that this model is better than the other two models. However, the scatter still exhibits some pattern. To further investigate the existence of any patterns, I plotted the predicted values against the residuals for this comprehensive model.\nFirst, I computed the residuals and subsequently generated the plot.\n\n#creating an object for the full model from the combined data adding a column of Residuals  \nall_vars_residuals &lt;- combined_df %&gt;%\n  filter(Model == \"Full Model\")%&gt;%\n  mutate(Residuals = Predicted - Observed)\n\n# plotting Predicted vs. Residuals for the full model\n\nggplot(all_vars_residuals, aes(x = Predicted, y = Residuals)) +\n  geom_point(color = \"darkblue\") + # Plot the residuals\n  geom_hline(yintercept = 0, linetype = \"solid\", color = \"darkred\") + # Add a horizontal line at 0\n  labs(x = \"Predicted Values\", y = \"Residuals\", title = \"Residuals vs. Predicted Values (Full Model)\") +\n  ylim(-2500, 2500) +  #y-axis limits\n  theme_minimal() # Use a minimal theme\n\n\n\n\n\n\n\n\nThe residuals are not randomly scattered around the zero line. We can see that there is a discernible pattern suggesting that the model may not be capturing some of the data features.\n\nModel Uncertainty Assessment with Bootstrap\nI assessed the variability of the predicted values by employing bootstrap resampling, drawing 100 samples from the training data. For each sample, I refitted the comprehensive model that includes all the predictors. The goal is to measure the uncertainty present in the model’s predictions.\n\n#Setting the seed same as at the beginning of the exercise for reproducibility\nset.seed(rngseed)\n\n#Creating 100 bootstrap samples from the training data\nboot_samples &lt;-bootstraps(train_data3, times = 100)\n\n#Following codes help check any bootstrap sample if needed  \ndat_sample &lt;- rsample::analysis(boot_samples$splits[[57]]) #Replace [[57]] by [[relevant sample no.from 1 to 100]] to view that sample\n#dat_sample\n\nI fitted the model to each Bootstrap sample and collected the predictions into a list using the map function.I converted the list to a matrix to simplify data manipulation.\n\n# Setting the function to fit model and make predictions\nfit_and_predict &lt;- function(boot) {\n  # Fit the model to the bootstrap sample\n  lm_all_bs &lt;- linear_reg() %&gt;%\n    set_engine(\"lm\") %&gt;%\n    set_mode(\"regression\") %&gt;%\n    fit(Y ~ ., data = analysis(boot))\n  \n  # Make predictions on the original training data\n  predictions &lt;- predict(lm_all_bs, new_data = train_data3)$.pred\n  \n  return(predictions)\n}\n\n# Applying the function to each bootstrap sample and collecting predictions\npredictions_list &lt;- map(boot_samples$splits, fit_and_predict)\n\n# Converting the list of predictions to a matrix\npredictions_matrix &lt;- do.call(cbind, predictions_list)\n\nI computed the means, medians and 89% confidence intervals of the predictions for each of the samples.\n\n# Calculating the mean prediction for each sample\nmean_predictions &lt;- rowMeans(predictions_matrix)\n\n#Computing median and 89% Confidence Interval\npreds &lt;-predictions_matrix %&gt;% \n  apply(1, quantile, c(0.055, 0.5, 0.945))%&gt;%  \n  t()\n\n#mean_predictions\npreds\n\n          5.5%      50%    94.5%\n [1,] 3095.536 3345.767 3553.229\n [2,] 1688.124 1962.397 2185.772\n [3,] 2420.566 2717.821 2937.470\n [4,] 1798.039 2094.438 2386.281\n [5,] 2660.761 2936.070 3141.177\n [6,] 1066.095 1301.015 1490.740\n [7,] 2176.857 2433.866 2661.169\n [8,] 1699.532 1942.421 2292.867\n [9,] 1290.691 1540.256 1879.126\n[10,] 2271.149 2516.569 2752.715\n[11,] 1295.460 1559.158 1886.120\n[12,] 1730.178 1914.622 2143.851\n[13,] 2002.008 2437.799 2755.797\n[14,] 2935.489 3295.735 3623.611\n[15,] 1757.794 2000.467 2240.672\n[16,] 2043.776 2254.439 2480.689\n[17,] 3168.574 3432.252 3730.375\n[18,] 2615.269 2960.192 3268.438\n[19,] 1966.970 2350.748 2850.072\n[20,] 3071.362 3278.368 3507.790\n[21,] 3667.221 3987.499 4259.103\n[22,] 3003.822 3238.672 3510.319\n[23,] 1001.757 1288.117 1555.024\n[24,] 2248.555 2742.403 3177.462\n[25,] 2954.560 3205.178 3395.522\n[26,] 2793.860 3113.973 3311.245\n[27,] 2073.601 2506.080 2823.173\n[28,] 1328.110 1510.954 1685.221\n[29,] 1588.768 1812.558 2025.079\n[30,] 1636.373 2017.330 2345.360\n[31,] 2587.672 2831.790 3028.960\n[32,] 3377.058 3568.144 3832.479\n[33,] 1797.321 2022.020 2210.438\n[34,] 1151.598 1319.471 1562.546\n[35,] 1873.889 2107.277 2333.929\n[36,] 1560.185 1723.784 1970.624\n[37,] 2894.356 3220.719 3457.213\n[38,] 3346.501 3715.141 4046.741\n[39,] 1951.650 2176.815 2413.828\n[40,] 1410.545 1779.102 2067.531\n[41,] 1806.414 1970.386 2137.388\n[42,] 2042.688 2229.645 2407.115\n[43,] 1654.071 1863.188 2089.427\n[44,] 3708.077 3923.919 4247.251\n[45,] 3092.981 3342.049 3601.919\n[46,] 3074.294 3458.830 3791.452\n[47,] 1830.768 2038.590 2260.294\n[48,] 1996.170 2208.096 2431.201\n[49,] 3244.506 3475.193 3738.398\n[50,] 3235.159 3480.633 3749.818\n[51,] 2235.910 2429.083 2631.497\n[52,] 2613.417 2897.492 3137.076\n[53,] 1874.792 2126.913 2362.522\n[54,] 3354.259 3567.665 3834.856\n[55,] 2083.492 2282.726 2489.073\n[56,] 1540.146 1703.919 1869.360\n[57,] 2593.562 2878.157 3121.766\n[58,] 1708.720 2040.540 2396.428\n[59,] 3041.401 3319.502 3537.824\n[60,] 1795.475 1980.278 2171.579\n[61,] 2903.656 3118.823 3325.790\n[62,] 1638.444 1922.457 2249.845\n[63,] 1133.853 1375.503 1585.288\n[64,] 3212.848 3390.520 3597.378\n[65,] 1458.885 1698.531 1954.253\n[66,] 3006.204 3285.482 3523.127\n[67,] 3006.686 3377.229 3771.809\n[68,] 1271.515 1462.235 1704.392\n[69,] 2100.180 2476.924 2799.641\n[70,] 2002.306 2169.424 2371.753\n[71,] 2413.541 2834.638 3283.439\n[72,] 2517.515 2867.246 3105.709\n[73,] 2054.845 2225.255 2379.857\n[74,] 2089.605 2394.062 2678.434\n[75,] 1550.165 1735.336 1926.220\n[76,] 1492.762 1705.287 1912.585\n[77,] 2845.864 3073.650 3261.779\n[78,] 2876.595 3168.327 3630.069\n[79,] 1161.988 1391.100 1649.330\n[80,] 2596.782 2876.622 3220.815\n[81,] 2107.255 2321.774 2507.333\n[82,] 2749.645 3066.775 3318.273\n[83,] 1399.377 1625.991 1847.334\n[84,] 3229.100 3610.564 3922.780\n[85,] 2644.674 2959.644 3240.935\n[86,] 1563.498 1718.005 1856.712\n[87,] 1602.279 1776.502 1981.108\n[88,] 3026.372 3246.728 3442.484\n[89,] 3214.368 3499.709 3812.543\n[90,] 3127.580 3325.632 3504.080\n\n\nFinally, I generated an error bar plot that illustrates a comparison between the observed values and the point estimates obtained from the original predictions on the training data. This plot also exhibits the median and the variability within the predictions, as indicated by upper and lower bounds of the predictions from the bootstrap samples. ChatGPT assisted me with creating this plot.\n\n#Converting the preds matrix into a data frame\npreds_df &lt;- as.data.frame(preds)\nnames(preds_df) &lt;- c(\"LowerBound\", \"Median\", \"UpperBound\" )\n\n#Adding observed values and original predictions of the training data \npreds_df&lt;- mutate(preds_df,\n                  Observed= train_data3$Y)\n\nggplot(preds_df, aes(x = Observed)) +\n  geom_point(aes(y = lmall_pred$.pred, color = \"Original Predictions\"), size=2) +\n  geom_point(aes(y = Median, color = \"Median Predictions\"), size=2) +\n  geom_errorbar(aes(ymin = LowerBound, ymax = UpperBound, y = Median, color = \"Confidence Intervals\"), width = .2) +\n  geom_abline(intercept = 0, slope = 1,  linetype=\"solid\") +\n  labs(x = \"Observed Values\", y = \"Predictions\", title = \"Predictions vs. Observed Values\") +\n  scale_color_manual(name = \"Legend\", \n                     values = c(\"Original Predictions\" = \"black\", \n                                \"Median Predictions\" = \"red\",\n                                \"Confidence Intervals\" = \"darkgreen\",\n                                \"Reference Line\" = \"lightblue\")) +\n  theme_minimal() +\n  coord_fixed()+\n  theme(legend.position = \"top\", # Keeps the legend at the top of the plot\n        )\n\n\n\n\n\n\n\n\nWe can see that the original predictions (means) and medians are closely aligned.The predictions generally seem to follow the line at lower values and with shorter Confidence Intervals. This suggests that the model has reasonable predictive accuracy at lower values. However, at higher values of the observed data, some predictions are away from the line with wider confidence intervals indicating higher uncertainty of the predicted values."
  },
  {
    "objectID": "fitting-exercise/fitting-exercise.html#part-3",
    "href": "fitting-exercise/fitting-exercise.html#part-3",
    "title": "Fitting exercise",
    "section": "Part 3",
    "text": "Part 3\nNow doing a final evaluation of the model with all the predictors. In this case I will use the fitted model to make predictions on the test data test_data3.\n\n#Create a prediction using the test data, for the model with all the predictors\nlmall_predtest &lt;- predict(lm_all, new_data = test_data3)\n\n#Match predicted with observed\nlmall_predtest &lt;- bind_cols(lmall_predtest, test_data3)\n\n#Estimate the metrics for the model with all the variables as predictors\nlmall_metrictest &lt;- metric_set(rmse) #Set the function\nlmall_metrictest(lmall_predtest, truth = Y, estimate = .pred) #Compute the RMSE\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        518.\n\n\nAnd then, plotting the observed vs predicted values of this model\n\n#EDIT THIS PART\n#Plotting of predicted vs observed data\nggplot(lmall_predtest, aes(x= Y, y= .pred))+\n  geom_point(aes(y = .pred), color = \"orangered2\", size= 3)+\n  geom_point(aes(y = Y), color = \"steelblue2\", size= 3)+\n  geom_abline(intercept = 0, slope = 1, linetype = \"dashed\", color = \"gray20\")+ #Draw the 45 degree line\n  xlim(0, 5000)+ #Set the x limits\n  ylim(0, 5000)+ #Set the y limits\n  labs(x= \"Observed values\", y= \"Predicted values\")+\n  theme_classic()\n\n\n\n\n\n\n\n\n\nOverall Model Assessment\nIn general, the model with all the variables as predictors (Model 2) seems like the better fit compared to the model where it was used only Dose as a predictor. A good starting point was comparing the RMSE metric between both models and including a null model as reference. In this case, the null model performed the least, which was expected, and Model 2 perfomed better. The next part, which was to conduct an assessment using cross-validation was another way to assess for the validity of the model. Even though in this case we only assessed Model 2 with CV, it was observed that the uncertainties were relatively small when conducting bootstrap. Finally, the next part was to fit the model with a data set split for testing (test data), and it was observed that the predicted values follow a similar patters as observed from the observed values.\nIn this case, we chose the most appropriate model in a manner that it doesn’t overfit the data. However, I believe a most accurate model could have been estimated if we had collected other variables. There might be stronger predictors that also, could be generalized to a larger population. I also believe that other metrics such as MAE or R2 could be added to the model assessment so they help in the model selection process."
  }
]