---
title: "Fitting exercise"
author: Erick E. Mollinedo
date: '`r format(Sys.Date(), "%B %d, %Y")`'
format: html
editor: visual
---

## Mavoglurant modeling Exercise (Week 8)

These are the packages I used for this exercise

```{r message=FALSE, warning=FALSE}
library(here)
library(readr)
library(tidyverse)
library(tidymodels)
library(gtsummary)
library(GGally)
```

Loading the dataset, assigned it to the `mavoglurant` dataframe.

```{r}
mavoglurant <- read_csv(here("fitting-exercise", "Mavoglurant_A2121_nmpk.csv"))
```

### Data Cleaning

First, I created a plot showing the concentration of Mavoglurant `DV` over `TIME`, by `DOSE`. In the first attempt, the dose was plotted as a numeric variable so I mutated `DOSE` to be a categorical variable.

```{r}
#Make `DOSE` a categorical variable using as.factor().
mavoglurant <- mavoglurant %>%
  mutate(DOSE = as.factor(DOSE))

#Create the plot of concentration by time, categorized by dose using ggplot().
ggplot(mavoglurant, aes(x = TIME, y = DV, group= ID)) +
  geom_line() + #Do a line plot
  facet_wrap(~ DOSE) + #Group by DOSE
  labs(x = "Time", y = "Mavoglurant concentration", color = "Dose")
```

Now, keeping just one of the observations for individuals that have two `OCC` observations.

```{r}
mavoglurant <- mavoglurant %>% filter(OCC == 1)
```

Now, removing observations where `TIME` is equal to 0 and create a new dataframe `mavoglurant_sum` where it summarizes the concentrations from `DV` by each subject. Then, I created the `mavoglurant_zero` dataframe that contains only the observations where `TIME` is equal to 0. An finally I joined both new dataframes into the `mavoglurant_new` df.

```{r}
# Exclude observations where 'TIME' = 0 and then compute the sum of 'DV' for each subject or 'ID', to create the `mavoglurant_sum` dataframe.
mavoglurant_sum <- mavoglurant %>%
  filter(TIME != 0) %>% #Remove observations where time= 0
  group_by(ID) %>% #Group by subject
  summarize(Y = sum(DV)) #The sum variable is called `Y`

#Create a dataframe with observations where TIME= 0.
mavoglurant_zero <- mavoglurant %>% 
  filter(TIME == 0) %>% 
  group_by(ID)

#Join the previous dataframes using left_join()
mavoglurant_new <- inner_join(mavoglurant_sum, mavoglurant_zero, by = "ID")
```

Finally, I filtered out unnecessary variables for this exercise and `RACE`, and `SEX` were converted to factor type variables.

```{r}
#Mutate SEX and RACE to factory type variables and then only keep Y, DOSE, AGE, SEX, RACE, WT and HT.
mavoglurant_new <- mavoglurant_new %>% 
  mutate(RACE = as.factor(RACE), SEX = as.factor(SEX)) %>% 
  select(c(Y, DOSE, AGE, SEX, RACE, WT, HT))

#Check the structure of the new dataframe
str(mavoglurant_new)
```

### Exploratory Data Analysis

The following plots and tables summarize the data observed from the `mavoglurant_new` dataframe.

First, a Boxplot that shows the dependent variable (Y) across the three different doses.

```{r}
#Using ggplot() to create a boxplot of the predicted variable Y and the DOSE
ggplot(mavoglurant_new, aes(x= DOSE, y= Y))+
  geom_boxplot(fill= "aquamarine3")+
  theme_classic()+
  labs(x= "Dose", y= "Mavoglurant concentration")
```

Based on the previous plot, it can be observed that at higher dose, the concentration of mavoglurant (predicted variable) increases. It is also seen that the range of concentrations is higher at the higher dose (50).

Now some plots that show the distribution of the dependent variable (Y) and the numeric independent variables `AGE`, `WT` and `HT`.

```{r}
#Histogram of the dependent variable (Y)
ggplot(mavoglurant_new, aes(x= Y))+
  geom_histogram(fill= "aquamarine3", color= "red")+
  labs(x= "Mavoglurant concentration")

#Histogram of AGE
ggplot(mavoglurant_new, aes(x= AGE))+
  geom_histogram(fill= "darkgoldenrod1", color= "red")+
  labs(x= "Age")

#Histogram of WT
ggplot(mavoglurant_new, aes(x= WT))+
  geom_histogram(fill= "darkgoldenrod1", color= "red")+
  labs(x= "Weight")

#Histogram of HT
ggplot(mavoglurant_new, aes(x= HT))+
  geom_histogram(fill= "darkgoldenrod1", color= "red")+
  labs(x= "Height")
```

In the previous plots in can be seen that the dependent (Y) variable and the Weight, follow a normal distribution. Height is observed that is skewed to the right, so this variable could not be following a normal distribution. On the other hand, it is observed that Age follows a bi-modal distribution. This is providing an insight about maybe first applying a regression model to this dataset.

The following table summarizes the previous variables, categorized by SEX (1 or 2). Here, it is shown the mean (sd), median (IQR) and the range.

```{r}
#Creating a summary table using the tbl_summary() function from `gtsummary`
sumtable <- mavoglurant_new %>% select(Y, AGE, HT, WT, SEX) %>% 
  tbl_summary(by= SEX, 
              type = all_continuous() ~ "continuous2",
              statistic = all_continuous() ~ c("{mean} ({sd})", "{median} ({p25}, {p75})", "{min}, {max}")) %>% 
  bold_labels()

#Visualize the table
sumtable
```

And here, showing barplots for the categorical variables `SEX` and `RACE`.

```{r}
#Creating a bar plot that shows the counts for each race category by sex.
ggplot(mavoglurant_new, aes(x= RACE, fill= SEX))+
  geom_bar(position = "dodge")+
  theme_classic()+
  labs(x= "Race")
```

It is observed on the previous plot that there are more subjects of sex `1`, than `2` for the 1, 2 and 88 race categories. Meanwhile for the race category `7`, it seems that there is the same amount of subjects by sex category. It is a shame that the correct labels for these categories are not known for sure.

And finally, exploring correlations between all the variables, visualizing by a plot:

```{r}
#Creating a correlation plot using the ggpairs() function from the GGally package.
ggpairs(mavoglurant_new, columns = c(1, 3, 6, 7), progress = F)
```

Based on this plot it is observed that the highest correlation is between the variables Height and Weight (0.6), and the linear plots in the middle confirm the distribution of each one of the variables.

### Model Fitting

#### Linear Regression Models

First, I fitted a linear model using the continuous outcome (Y) and `DOSE` as the predictor.

```{r}
# Define the model specification for linear regression
linear_model <- linear_reg() %>%
  set_engine("lm") %>% #Specify the linear model to fit the model
  set_mode("regression") #Setting the mode as a regression model

# Define the formula
formula1 <- Y ~ DOSE

# Fit the model
lm_simple <- linear_model %>%
  fit(formula1, data = mavoglurant_new) #Calling the formula and the dataframe to compute the linear model

# Output the model summary
summary(lm_simple$fit)
```

Based on the model it can be inferred that the outcome increases by around 681.24 units with the dose 37.5 and increases by 1456.20 with the dose 50, all compared with the dose 25. It is also observed that the differences are statistically significant, given the p-values are less than 0.001.

Now, fitting a linear model using the continuous outcome (Y) and using the rest of the variables as predictors.

```{r}
#The model specification has already been set in the previous code chunk, so there is no need to set it again.

# Define the formula
formula2 <- Y ~ AGE + WT + HT + DOSE + SEX + RACE

# Fit the model
lm_multi <- linear_model %>%
  fit(formula2, data = mavoglurant_new)

# Output the model summary
summary(lm_multi$fit)
```

For the interpretation of this model I will focus only on the statistically significant predictors (p-value \< 0.001). Besides dose 37.5 with an increase of the outcome by a factor of \~664 and dose 50 with an increase by a factor of \~1500, Weight is also another variable associated with a decrease of the outcome by a factor of \~23.

In summary, it can be observed that the coefficients slightly changed between both models, however the second model seems a better fit. To evaluate which model is best, I computed the root mean square error (RMSE) and R-squared as metrics. First for the linear model using one predictor, and then using multiple predictors.

```{r}
#ONE VARIABLE AS PREDICTOR
#Create a prediction from the dataframe
lmsimple_pred <- predict(lm_simple, new_data = mavoglurant_new %>% select(-Y))

#Match predicted with observed
lmsimple_pred <- bind_cols(lmsimple_pred, mavoglurant_new %>% select(Y))

#Estimate the metrics
lmsimple_metrics <- metric_set(rmse, rsq)
lmsimple_metrics(lmsimple_pred, truth = Y, estimate = .pred)

#MULTIPLE VARIABLES AS PREDICTORS
#Create a prediction from the dataframe
lmmulti_pred <- predict(lm_multi, new_data = mavoglurant_new %>% select(-Y))

#Match predicted with observed
lmmulti_pred <- bind_cols(lmmulti_pred, mavoglurant_new %>% select(Y))

#Estimate the metrics
lmmulti_metrics <- metric_set(rmse, rsq)
lmmulti_metrics(lmmulti_pred, truth = Y, estimate = .pred)
```

We can observe that the RMSE is lower (590.3) in the model that inputs all the variables as predictors compared to the linear model that uses Dose as a predictor (RMSE= 666.3). We also observe that the R^2^ is slightly higher in the second model (0.62) compared to the first model (0.52). In this case we can conclude that the second model (linear model with multiple predictors) is a better fit to this dataset.

#### Logistic Models

Now, I fitted a logistic model to the outcome `SEX`, and using `DOSE` as a predictor. I also evaluated the Accuracy and ROC-AUC of this model in the following steps.

```{r}
# Define the model specification
logistic_spec <- logistic_reg() %>%  #Defining as logistic
  set_engine("glm") %>% #...From the GLM family
  set_mode("classification") #Classification, since it involves categorical variables

# Create the recipe
recipe <- recipe(SEX ~ DOSE, data = mavoglurant_new) %>% 
  step_dummy(all_nominal(), -all_outcomes())

# Split the data into training and testing sets
set.seed(123) #For reproducibility
data_split <- initial_split(mavoglurant_new, prop = 0.75)
train_data <- training(data_split) #Create a training data to apply the model
test_data <- testing(data_split) #Create a test data to apply the model evaluation

# Fit the model
logistic_fit <- workflow() %>%
  add_recipe(recipe) %>%
  add_model(logistic_spec) %>%
  fit(data = train_data)

# Make predictions on the test set to determine the ROC-AUC of the model
predictions <- predict(logistic_fit, test_data, type = "prob")

#Make predictions on the test set to determine the Accuracy of the model
predictions2 <- logistic_fit %>% predict(new_data = test_data)

# Bind the predictions to the testing set
results <- bind_cols(test_data, predictions) #ROC-AUC
results2 <- bind_cols(test_data, predictions2) #Accuracy

# Calculate ROC-AUC
roc_auc <- roc_auc(results, truth = SEX, .pred_1)

# Calculate Accuracy
accuracy <- accuracy(results2, truth = SEX, estimate = .pred_class)

# Output the model and the metrics
log1 <- glm(formula = SEX ~ DOSE, family = binomial(link = "logit"), 
    data = train_data)
summary(log1)
list(Accuracy = accuracy, ROC_AUC = roc_auc)
```

And finally, fitting a logistic model to the outcome `SEX`, using all of the variables as predictors. I also computed the ROC-AUC and Accuracy of this model.

```{r}
# The model has been defined before 'logistic_spec', so there is no need to define it again.

# Create the recipe of this model
recipe2 <- recipe(SEX ~ Y + AGE + WT + HT + DOSE + RACE, data = mavoglurant_new) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors())

# Split the data into training and testing sets
set.seed(123) #For reproducibility
data_split2 <- initial_split(mavoglurant_new, prop = 0.75)
train_data2 <- training(data_split2) #Create a training data to apply the model
test_data2 <- testing(data_split2) #Create a test data to apply the model evaluation

# Fit the model
logistic_fit2 <- workflow() %>%
  add_recipe(recipe2) %>%
  add_model(logistic_spec) %>%
  fit(data = train_data)

# Make predictions on the test set to determine the ROC-AUC of the model
predictions_auc <- predict(logistic_fit2, test_data2, type = "prob")

#Make predictions on the test set to determine the Accuracy of the model
predictions_acc <- logistic_fit %>% predict(new_data = test_data2)

# Bind the predictions to the testing set
results_auc2 <- bind_cols(test_data2, predictions_auc) #ROC-AUC
results_acc2 <- bind_cols(test_data2, predictions_acc) #Accuracy

# Calculate ROC-AUC
roc_auc2 <- roc_auc(results_auc2, truth = SEX, .pred_1)

# Calculate Accuracy
accuracy2 <- accuracy(results_acc2, truth = SEX, estimate = .pred_class)

# Output the metrics using list()
log2 <- glm(formula = SEX ~ Y + AGE + WT + HT + DOSE + RACE, family = binomial(link = "logit"), 
    data = train_data2)
summary(log2)
list(Accuracy = accuracy2, ROC_AUC = roc_auc2)
```

Based on the previous logistic models, it is observed that appears there is no association between the dose of mavoglurant and sex. However, when observing the second logistic model, it appears there is a statistically significant association between height and sex (p-value \< 0.05). While looking at the accuracy from both models, we can see that both have the same accuracy (93%), however, the ROC-AUC value is pretty low for the model that uses only Dose as a predictor (0.39), meanwhile, the model that uses dose and all the other variables as predictors has a better value (0.96), which reflects better sensitivity and specificity.

## Mavoglurant modeling Exercise continuation (Week 10)

First, naming the seed for reproducibility

```{r}
rngseed = 1234
set.seed(rngseed)
```

Remove the `RACE` variable from the `mavoglurant_new` dataframe.

```{r}
#Select all other variables, except `RACE`
mavoglurant_new <- mavoglurant_new %>% select(-RACE)
```

Split the data randomly to a 75% train and 25% test data.

```{r}
#Let the data split be 75-25% or 3/4
data_split3 <- initial_split(mavoglurant_new, prop = 0.75)

#Create the train and test data frames
train_data3 <- training(data_split3)
test_data3 <- testing(data_split3)
```

### Model performance assessment 1

Now, fitting a linear regression model that predicts the outcome `Y` based on `DOSE` alone. Then, making predictions to compare against the observed values and then computing the RMSE to evaluate.

```{r}
#Define the model specification for linear regression
lm_dose <- linear_reg() %>%
  set_engine("lm") %>% #Specify the linear model to fit the model
  set_mode("regression") %>% #Setting the mode as a regression model
  fit(Y ~ DOSE, data = train_data3)

#Tidy the results
tidy(lm_dose)

#Create a prediction from the dataframe for the `DOSE` model
lmdose_pred <- predict(lm_dose, new_data = train_data3)

#Match predicted with observed values
lmdose_pred <- bind_cols(lmdose_pred, train_data3)

#Estimate the metrics for the `DOSE` model
lmdose_metric <- metric_set(rmse) #Set the function to estimate RMSE
lmdose_metric(lmdose_pred, truth = Y, estimate = .pred) #Compute the RMSE
```

Computing another linear regression model that predicts `Y` using the other variables as predictors. I also computed the predicted vs observed values to compute the RMSE of this model.

```{r}
#Define the model specification for linear regression
lm_all <- linear_reg() %>%
  set_engine("lm") %>% #Specify the linear model to fit the model
  set_mode("regression") %>% #Setting the mode as a regression model
  fit(Y ~ ., data = train_data3)

#Tidy the results
tidy(lm_all)

#Create a prediction from the dataframe of the model with all the other predictors
lmall_pred <- predict(lm_all, new_data = train_data3)

#Match predicted with observed
lmall_pred <- bind_cols(lmall_pred, train_data3)

#Estimate the metrics for the model with all the variables as predictors
lmall_metric <- metric_set(rmse) #Set the function
lmall_metric(lmall_pred, truth = Y, estimate = .pred) #Compute the RMSE
```

Now I created a null model and computed the RMSE.

```{r}
#Run the null model
lm_null <- null_model(mode = "regression") %>% 
    set_engine("parsnip") %>%
    fit(Y ~ 1, data = train_data3)

#Compute the RMSE and other estimates
null_metric <- lm_null %>% 
  predict(train_data3) %>% 
  bind_cols(train_data3) %>% 
  metrics(truth = Y, estimate = .pred)

#Print the RMSE (Note: This includes also the R-squared and MAE but I am only interested in the RMSE)
null_metric
```

In summary, according to the RMSE parameters, the model that includes all the variables as predictors performed better (RMSE= 627), compared to the model using only `DOSE` as a predictor (RMSE= 702) and the null model (RMSE= 948) as a reference.

### Model performance assessment 2

Now, evaluating both models using a 10-fold cross-validation. First, by the model that predicts `Y` only using `DOSE`.

```{r}
#Set the seed for reproducibility
set.seed(rngseed)

#Set the cross-validation folds as 10
folds <- vfold_cv(train_data3, v= 10)

#Set the model specification, for linear regression
linear_mod <- linear_reg() %>% set_engine("lm")

#Set the workflow
linear_wf <- workflow() %>% add_model(linear_mod) %>% add_formula(Y ~ DOSE)

#Do the resamples
dose_resample <- fit_resamples(linear_wf, resamples = folds)

#Extract the metric
collect_metrics(dose_resample)
```

Now, doing the cross-validation for the model that predicts `Y` using all the other variables as predictors.

```{r}
#Set the seed for reproducibility
set.seed(rngseed)

#Set the workflow
linear_all_wf <- workflow() %>% add_model(linear_mod) %>% add_formula(Y ~ .)

#Do the resamples
all_resample <- fit_resamples(linear_all_wf, resamples = folds)

#Extract the metric
collect_metrics(all_resample)
```

Based on the previous results, it can be observed that when fitting the model and conducting a 10-fold cross-validation the RMSEs changed but still, the model that uses all predictors have a better metric (RMSE= 653, SE= 63.6) than the model that uses only `DOSE` as a predictor (RMSE= 696, SE= 68).

And now, computing the same CV analysis, but setting a different seed.

```{r}
#Set the seed for reproducibility
set.seed(2302)

#Set the cross-validation folds as 10
folds2 <- vfold_cv(train_data3, v= 10)

#Set the model specification, for linear regression
linear_mod2 <- linear_reg() %>% set_engine("lm")

#ONE PREDICTOR LINEAR MODEL
#Set the workflow
linear_wf2 <- workflow() %>% add_model(linear_mod2) %>% add_formula(Y ~ DOSE)

#Do the resamples
dose_resample2 <- fit_resamples(linear_wf2, resamples = folds2)

#Extract the metric
collect_metrics(dose_resample2)

#ALL PREDICTORS LINEAR MODEL
#Set the workflow
linear_all_wf2 <- workflow() %>% add_model(linear_mod2) %>% add_formula(Y ~ .)

#Do the resamples
all_resample2 <- fit_resamples(linear_all_wf2, resamples = folds2)

#Extract the metric
collect_metrics(all_resample2)
```

In this case, when changing the seed, the metrics changed for both models but they are similar in proportion. For the linear model with a single predictor the RMSE= 705.9 and for the model with all predictors the RMSE= 660. However, the standar error is higher for the model with all the predictors (SE= 57.51), compared to the model with a single predictor (SE= 53.33). Still, it seems that the linear model that uses all the predictors is better than the model that uses only `DOSE` as a single predictor.

## This section is added by Malika Dhakhwa.

I conducted a visual inspection of performance of the models by plotting the predicted values from all three models against the observed values of the training data.

First, I extracted the predicted values of the Null model.For the other two models, this step has already been completed in earlier phases.

```{r}
# Recovering predicted values of the null model
null_pred <- predict(lm_null, new_data = train_data3)
```

I created a combined data frame of the predictors in long format for all three models and created the plot.

```{r}
#| warning: false

#Creating the object for the outcome variable Y in the training data
observed_values <- train_data3$Y

#Creating separate data frames for each set of predictions with model labels

df_dose <- data.frame(Observed = observed_values, Predicted = lmdose_pred$.pred, Model="Dose Model")
df_all <- data.frame(Observed= observed_values, Predicted = lmall_pred$.pred, Model="Full Model")
df_null<- data.frame(Observed = observed_values, Predicted = null_pred$.pred, Model="Null Model")

#Combining all the predicted values to a single data frame by rows to create a long format data
combined_df <- rbind(df_dose, df_all, df_null)

#Plotting of predicted vs observed data
ggplot(combined_df, aes(x=Observed, y=Predicted, color=Model))+
  geom_point()+ 
  scale_color_manual(values = c("Null Model"= "lightblue", "Dose Model"="red", "Full Model"="green")) + 
  geom_abline(intercept = 0, slope = 1, linetype = "solid", color="black") + #45 degree line
  xlim(0, 5000) + #X-axis limits
  ylim(0, 5000) + #y-axis limits
  labs(x= "Observed Values", y ="Predicted Values", title = "Predicted vs. Observed Values")+
  theme_minimal() # Use a minimal theme
```

The predictions from Null model follows a horizontal line indicating that they assume a single value which is the mean of the outcome variable. The variable Dose in the observed data has only three distinct values, leading the Dose model's predictions to appear as three horizontal lines in the plot, marked by red dots. One of these predicted values is close to that of the Null model, creating almost overlapping with the predictions from the Null model. The predicted values from the model incorporating all predictors are more dispersed indicating that this model is better than the other two models. However, the scatter still exhibits some pattern. To further investigate the existence of any patterns, I plotted the predicted values against the residuals for this comprehensive model.

First, I computed the residuals and subsequently generated the plot.

```{r}
#creating an object for the full model from the combined data adding a column of Residuals  
all_vars_residuals <- combined_df %>%
  filter(Model == "Full Model")%>%
  mutate(Residuals = Predicted - Observed)

# plotting Predicted vs. Residuals for the full model

ggplot(all_vars_residuals, aes(x = Predicted, y = Residuals)) +
  geom_point(color = "darkblue") + # Plot the residuals
  geom_hline(yintercept = 0, linetype = "solid", color = "darkred") + # Add a horizontal line at 0
  labs(x = "Predicted Values", y = "Residuals", title = "Residuals vs. Predicted Values (Full Model)") +
  ylim(-2500, 2500) +  #y-axis limits
  theme_minimal() # Use a minimal theme
```

The residuals are not randomly scattered around the zero line. We can see that there is a discernible pattern suggesting that the model may not be capturing some of the data features.

### Model Uncertainty Assessment with Bootstrap

I assessed the variability of the predicted values by employing bootstrap resampling, drawing 100 samples from the training data. For each sample, I refitted the comprehensive model that includes all the predictors. The goal is to measure the uncertainty present in the model's predictions.

```{r}
#Setting the seed same as at the beginning of the exercise for reproducibility
set.seed(rngseed)

#Creating 100 bootstrap samples from the training data
boot_samples <-bootstraps(train_data3, times = 100)

#Following codes help check any bootstrap sample if needed  
dat_sample <- rsample::analysis(boot_samples$splits[[57]]) #Replace [[57]] by [[relevant sample no.from 1 to 100]] to view that sample
#dat_sample
```

I fitted the model to each Bootstrap sample and collected the predictions into a list using the map function.I converted the list to a matrix to simplify data manipulation.

```{r}
# Setting the function to fit model and make predictions
fit_and_predict <- function(boot) {
  # Fit the model to the bootstrap sample
  lm_all_bs <- linear_reg() %>%
    set_engine("lm") %>%
    set_mode("regression") %>%
    fit(Y ~ ., data = analysis(boot))
  
  # Make predictions on the original training data
  predictions <- predict(lm_all_bs, new_data = train_data3)$.pred
  
  return(predictions)
}

# Applying the function to each bootstrap sample and collecting predictions
predictions_list <- map(boot_samples$splits, fit_and_predict)

# Converting the list of predictions to a matrix
predictions_matrix <- do.call(cbind, predictions_list)

```

I computed the means, medians and 89% confidence intervals of the predictions for each of the samples.

```{r}
# Calculating the mean prediction for each sample
mean_predictions <- rowMeans(predictions_matrix)

#Computing median and 89% Confidence Interval
preds <-predictions_matrix %>% 
  apply(1, quantile, c(0.055, 0.5, 0.945))%>%  
  t()

#mean_predictions
preds
```

Finally, I generated an error bar plot that illustrates a comparison between the observed values and the point estimates obtained from the original predictions on the training data. This plot also exhibits the median and the variability within the predictions, as indicated by upper and lower bounds of the predictions from the bootstrap samples. ChatGPT assisted me with creating this plot.

```{r}
#Converting the preds matrix into a data frame
preds_df <- as.data.frame(preds)
names(preds_df) <- c("LowerBound", "Median", "UpperBound" )

#Adding observed values and original predictions of the training data 
preds_df<- mutate(preds_df,
                  Observed= train_data3$Y)

ggplot(preds_df, aes(x = Observed)) +
  geom_point(aes(y = lmall_pred$.pred, color = "Original Predictions"), size=2) +
  geom_point(aes(y = Median, color = "Median Predictions"), size=2) +
  geom_errorbar(aes(ymin = LowerBound, ymax = UpperBound, y = Median, color = "Confidence Intervals"), width = .2) +
  geom_abline(intercept = 0, slope = 1,  linetype="solid") +
  labs(x = "Observed Values", y = "Predictions", title = "Predictions vs. Observed Values") +
  scale_color_manual(name = "Legend", 
                     values = c("Original Predictions" = "black", 
                                "Median Predictions" = "red",
                                "Confidence Intervals" = "darkgreen",
                                "Reference Line" = "lightblue")) +
  theme_minimal() +
  coord_fixed()+
  theme(legend.position = "top", # Keeps the legend at the top of the plot
        )

```

We can see that the original predictions (means) and medians are closely aligned.The predictions generally seem to follow the line at lower values and with shorter Confidence Intervals. This suggests that the model has reasonable predictive accuracy at lower values. However, at higher values of the observed data, some predictions are away from the line with wider confidence intervals indicating higher uncertainty of the predicted values.

## Part 3

Now doing a final evaluation of the model with all the predictors. In this case I will use the fitted model to make predictions on the test data `test_data3`.

```{r}
#Create a prediction using the test data, for the model with all the predictors
lmall_predtest <- predict(lm_all, new_data = test_data3)

#Match predicted with observed
lmall_predtest <- bind_cols(lmall_predtest, test_data3)

#Estimate the metrics for the model with all the variables as predictors
lmall_metrictest <- metric_set(rmse) #Set the function
lmall_metrictest(lmall_predtest, truth = Y, estimate = .pred) #Compute the RMSE
```

And then, plotting the observed vs predicted values of this model

```{r}
#EDIT THIS PART
#Plotting of predicted vs observed data
ggplot(lmall_predtest, aes(x= Y, y= .pred))+
  geom_point(aes(y = .pred), color = "orangered2", size= 3)+
  geom_point(aes(y = Y), color = "steelblue2", size= 3)+
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gray20")+ #Draw the 45 degree line
  xlim(0, 5000)+ #Set the x limits
  ylim(0, 5000)+ #Set the y limits
  labs(x= "Observed values", y= "Predicted values")+
  theme_classic()
```

#### Overall Model Assessment

In general, the model with all the variables as predictors (Model 2) seems like the better fit compared to the model where it was used only Dose as a predictor. A good starting point was comparing the RMSE metric between both models and including a null model as reference. In this case, the null model performed the least, which was expected, and Model 2 perfomed better. The next part, which was to conduct an assessment using cross-validation was another way to assess for the validity of the model. Even though in this case we only assessed Model 2 with CV, it was observed that the uncertainties were relatively small when conducting bootstrap. Finally, the next part was to fit the model with a data set split for testing (test data), and it was observed that the predicted values follow a similar patters as observed from the observed values.

In this case, we chose the most appropriate model in a manner that it doesn't overfit the data. However, I believe a most accurate model could have been estimated if we had collected other variables. There might be stronger predictors that also, could be generalized to a larger population. I also believe that other metrics such as MAE or R^2^ could be added to the model assessment so they help in the model selection process.
